[["统计描述与基础统计分析方法.html", "Medical Statistics wtih R Chapter 1 统计描述与基础统计分析方法 1.1 描述性统计分析 1.2 线性相关与秩相关分析 1.3 组间差异t检验 1.4 组间差异秩和检验", " Medical Statistics wtih R Wade Wang 2021-11-19 Chapter 1 统计描述与基础统计分析方法 1.1 描述性统计分析 1.1.1 单组数据汇总统计量 ➢ 均数、标准差、中位数、分位数计算 ➢ 统计描述过程中的缺失值处理 ➢ ISwR包中的 juul 数据集 library(ISwR) attach(juul) mean(igf1) ## [1] NA mean(igf1,na.rm=T) ## [1] 340.168 sum(!is.na(igf1)) ## [1] 1018 summary(igf1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 25.0 202.2 313.5 340.2 462.8 915.0 321 summary(juul) ## age menarche sex igf1 ## Min. : 0.170 Min. :1.000 Min. :1.000 Min. : 25.0 ## 1st Qu.: 9.053 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:202.2 ## Median :12.560 Median :1.000 Median :2.000 Median :313.5 ## Mean :15.095 Mean :1.476 Mean :1.534 Mean :340.2 ## 3rd Qu.:16.855 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:462.8 ## Max. :83.000 Max. :2.000 Max. :2.000 Max. :915.0 ## NA&#39;s :5 NA&#39;s :635 NA&#39;s :5 NA&#39;s :321 ## tanner testvol ## Min. :1.00 Min. : 1.000 ## 1st Qu.:1.00 1st Qu.: 1.000 ## Median :2.00 Median : 3.000 ## Mean :2.64 Mean : 7.896 ## 3rd Qu.:5.00 3rd Qu.:15.000 ## Max. :5.00 Max. :30.000 ## NA&#39;s :240 NA&#39;s :859 detach(juul) #将某些数值因子化，避免对描述性变量进行计算 library(ISwR) attach(juul) juul$sex &lt;- factor(juul$sex,labels=c(&quot;M&quot;,&quot;F&quot;)) juul$menarche &lt;- factor(juul$menarche,labels=c(&quot;No&quot;,&quot;Yes&quot;)) juul$tanner &lt;- factor(juul$tanner, labels=c(&quot;I&quot;,&quot;II&quot;,&quot;III&quot;,&quot;IV&quot;,&quot;V&quot;)) summary(juul) ## age menarche sex igf1 tanner ## Min. : 0.170 No :369 M :621 Min. : 25.0 I :515 ## 1st Qu.: 9.053 Yes :335 F :713 1st Qu.:202.2 II :103 ## Median :12.560 NA&#39;s:635 NA&#39;s: 5 Median :313.5 III : 72 ## Mean :15.095 Mean :340.2 IV : 81 ## 3rd Qu.:16.855 3rd Qu.:462.8 V :328 ## Max. :83.000 Max. :915.0 NA&#39;s:240 ## NA&#39;s :5 NA&#39;s :321 ## testvol ## Min. : 1.000 ## 1st Qu.: 1.000 ## Median : 3.000 ## Mean : 7.896 ## 3rd Qu.:15.000 ## Max. :30.000 ## NA&#39;s :859 detach(juul) #以下代码与上述5行代码等价 juul &lt;- transform(juul, sex=factor(sex,labels=c(&quot;M&quot;,&quot;F&quot;)), menarche=factor(menarche,labels=c(&quot;No&quot;,&quot;Yes&quot;)), tanner=factor(tanner,labels=c(&quot;I&quot;,&quot;II&quot;,&quot;III&quot;,&quot;IV&quot;,&quot;V&quot;))) attach(juul) summary(juul) ## age menarche sex igf1 tanner ## Min. : 0.170 No :369 M :621 Min. : 25.0 I :515 ## 1st Qu.: 9.053 Yes :335 F :713 1st Qu.:202.2 II :103 ## Median :12.560 NA&#39;s:635 NA&#39;s: 5 Median :313.5 III : 72 ## Mean :15.095 Mean :340.2 IV : 81 ## 3rd Qu.:16.855 3rd Qu.:462.8 V :328 ## Max. :83.000 Max. :915.0 NA&#39;s:240 ## NA&#39;s :5 NA&#39;s :321 ## testvol ## Min. : 1.000 ## 1st Qu.: 1.000 ## Median : 3.000 ## Mean : 7.896 ## 3rd Qu.:15.000 ## Max. :30.000 ## NA&#39;s :859 detach(juul) 1.1.2 数据分布类型的图形描述 library(ISwR) #条形图 x &lt;- rnorm(50) hist(x) #QQ图 qqnorm(x) #箱式图 par(mfrow=c(1,2)) boxplot(IgM) boxplot(log(IgM)) par(mfrow=c(1,1)) 1.2 线性相关与秩相关分析 1.2.1 三种相关系数 ➢ Pearson相关系数 ➢ Spearman秩相关 ➢ Kendall相关系数 ➢ cor()函数可以计算这三种相关系数，而cov()函数可用来计算协方差。两个函数的参数有很多，其中与相关系数的计算有关的参数可以简化为：cor(x, use= , method= ) 1.2.2 相关系数的显著性检验 ➢ 可以使用cor.test()函数对单个的Pearson、Spearman和Kendall相关系数进行 检验。简化后的使用格式为：cor.test(x, y, alternative = , method = ) ➢ 其中的x和y为要检验相关性的变量，alternative则用来指定进行双侧检验或单侧 检验（取值为\"two.side\"、\"less\"或\"greater\"），而method用以指定要计算的 相关类型（\"pearson\"、\"kendall\" 或\"spearman\" ）。当研究的假设为总体的相 关系数小于0 时， 请使用alternative=\"less\" 。在研究的假设为总体的相关系数 大于0 时， 应使用alternative=\"greater\"。在默认情况下，假设为 alternative=\"two.side\"（总体相关系数不等于0）。 1.3 组间差异t检验 在研究中最常见的行为就是对两个组进行比较。接受某种新药治疗的患者是否较使用某种现有药物的患者表现出了更大程度的改善？这里我们将关注结果变 为连续型的组间比较，并假设其服从正态分布。 1.3.1 单样本t检验 【案例】输入一组变量，对它进行统计分析 daily.intake &lt;- c(5260,5470,5640,6180,6390,6515, 6805,7515,7515,8230,8770) mean(daily.intake) ## [1] 6753.636 sd(daily.intake) ## [1] 1142.123 quantile(daily.intake) ## 0% 25% 50% 75% 100% ## 5260 5910 6515 7515 8770 进行单样本t检验及秩和检验 t.test(daily.intake,mu=7725) ## ## One Sample t-test ## ## data: daily.intake ## t = -2.8208, df = 10, p-value = 0.01814 ## alternative hypothesis: true mean is not equal to 7725 ## 95 percent confidence interval: ## 5986.348 7520.925 ## sample estimates: ## mean of x ## 6753.636 wilcox.test(daily.intake, mu=7725) ## Warning in wilcox.test.default(daily.intake, mu = 7725): cannot compute exact p- ## value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: daily.intake ## V = 8, p-value = 0.0293 ## alternative hypothesis: true location is not equal to 7725 实际上，在现实研究过程中，我们并不在意单样本的t检验是否有意义。 1.3.2 两样本t检验 一个针对两组的独立样本t检验可以用于检验两个总体的均值相等的假设。这里假设两组数据是独立的，并且是从正态总体中抽得。检验的调用格式为： t.test(y~x , data)。 可选参数data的取值为一个包含了这些变量的矩阵或数据框。与其他多数统计软件不同的是，这里的t检验默认假定方差不相等，并使用Welsh的修正自由度你可以添加一个参数 var.equl=TRUE以假定方差相等，并使用合并方差估计。默认的备择假设是双侧的（即均值不相等，但大小的方向不确定） 你可以添加一个参数 alternative=\"less\"或alternative=\"greater\"来进行有方向的检验 代码清单 library(ISwR) attach(energy) energy ## expend stature ## 1 9.21 obese ## 2 7.53 lean ## 3 7.48 lean ## 4 8.08 lean ## 5 8.09 lean ## 6 10.15 lean ## 7 8.40 lean ## 8 10.88 lean ## 9 6.13 lean ## 10 7.90 lean ## 11 11.51 obese ## 12 12.79 obese ## 13 7.05 lean ## 14 11.85 obese ## 15 9.97 obese ## 16 7.48 lean ## 17 8.79 obese ## 18 9.69 obese ## 19 9.68 obese ## 20 7.58 lean ## 21 9.19 obese ## 22 8.11 lean # 两样本t检验 t.test(expend~stature) ## ## Welch Two Sample t-test ## ## data: expend by stature ## t = -3.8555, df = 15.919, p-value = 0.001411 ## alternative hypothesis: true difference in means between group lean and group obese is not equal to 0 ## 95 percent confidence interval: ## -3.459167 -1.004081 ## sample estimates: ## mean in group lean mean in group obese ## 8.066154 10.297778 #假定方差齐 t.test(expend~stature, var.equal=T) ## ## Two Sample t-test ## ## data: expend by stature ## t = -3.9456, df = 20, p-value = 0.000799 ## alternative hypothesis: true difference in means between group lean and group obese is not equal to 0 ## 95 percent confidence interval: ## -3.411451 -1.051796 ## sample estimates: ## mean in group lean mean in group obese ## 8.066154 10.297778 #方差齐性检验 var.test(expend~stature) ## ## F test to compare two variances ## ## data: expend by stature ## F = 0.78445, num df = 12, denom df = 8, p-value = 0.6797 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.1867876 2.7547991 ## sample estimates: ## ratio of variances ## 0.784446 #符号秩和检验 wilcox.test(expend~stature) ## Warning in wilcox.test.default(x = c(7.53, 7.48, 8.08, 8.09, 10.15, 8.4, : ## cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: expend by stature ## W = 12, p-value = 0.002122 ## alternative hypothesis: true location shift is not equal to 0 1.4 组间差异秩和检验 如果数据无法满足 检验或ANOVA的参数假设，可以转而使用非参数方法。举例来说，若结果变量在木质上就严重偏倚或呈现有序关系，那么你可能会希望使用本节中的方法。 1.4.1 单样本Wilcoxon符号秩检验 wilcox.test(x, mu= ) x为检验样本;mu为总体均数 daily.intake &lt;- c(5260,5470,5640,6180,6390,6515, 6805,7515,7515,8230,8770) mean(daily.intake) ## [1] 6753.636 sd(daily.intake) ## [1] 1142.123 quantile(daily.intake) ## 0% 25% 50% 75% 100% ## 5260 5910 6515 7515 8770 t.test(daily.intake,mu=7725) ## ## One Sample t-test ## ## data: daily.intake ## t = -2.8208, df = 10, p-value = 0.01814 ## alternative hypothesis: true mean is not equal to 7725 ## 95 percent confidence interval: ## 5986.348 7520.925 ## sample estimates: ## mean of x ## 6753.636 wilcox.test(daily.intake, mu=7725) ## Warning in wilcox.test.default(daily.intake, mu = 7725): cannot compute exact p- ## value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: daily.intake ## V = 8, p-value = 0.0293 ## alternative hypothesis: true location is not equal to 7725 1.4.2 两组的比较 若两组数据独立，可以使用Wilcoxon秩和检验（更广为人知的名字是Mann-Whitney 检验）来评估观测是否是从相同的概率分布中抽得的（即，在一个总体中获得更高得分的概率是否比另一个总体要大）。调用格式为 wilcox.test(x~y,data) “x” ——结果变量 “y” ——分组变量 library(ISwR) attach(energy) ## The following objects are masked from energy (pos = 3): ## ## expend, stature energy ## expend stature ## 1 9.21 obese ## 2 7.53 lean ## 3 7.48 lean ## 4 8.08 lean ## 5 8.09 lean ## 6 10.15 lean ## 7 8.40 lean ## 8 10.88 lean ## 9 6.13 lean ## 10 7.90 lean ## 11 11.51 obese ## 12 12.79 obese ## 13 7.05 lean ## 14 11.85 obese ## 15 9.97 obese ## 16 7.48 lean ## 17 8.79 obese ## 18 9.69 obese ## 19 9.68 obese ## 20 7.58 lean ## 21 9.19 obese ## 22 8.11 lean t.test(expend~stature) ## ## Welch Two Sample t-test ## ## data: expend by stature ## t = -3.8555, df = 15.919, p-value = 0.001411 ## alternative hypothesis: true difference in means between group lean and group obese is not equal to 0 ## 95 percent confidence interval: ## -3.459167 -1.004081 ## sample estimates: ## mean in group lean mean in group obese ## 8.066154 10.297778 t.test(expend~stature, var.equal=T) ## ## Two Sample t-test ## ## data: expend by stature ## t = -3.9456, df = 20, p-value = 0.000799 ## alternative hypothesis: true difference in means between group lean and group obese is not equal to 0 ## 95 percent confidence interval: ## -3.411451 -1.051796 ## sample estimates: ## mean in group lean mean in group obese ## 8.066154 10.297778 var.test(expend~stature) ## ## F test to compare two variances ## ## data: expend by stature ## F = 0.78445, num df = 12, denom df = 8, p-value = 0.6797 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.1867876 2.7547991 ## sample estimates: ## ratio of variances ## 0.784446 wilcox.test(expend~stature) ## Warning in wilcox.test.default(x = c(7.53, 7.48, 8.08, 8.09, 10.15, 8.4, : ## cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: expend by stature ## W = 12, p-value = 0.002122 ## alternative hypothesis: true location shift is not equal to 0 1.4.3 配对Wilcoxon符号秩检验 wilcox.test(x, y, paired=T) x，y 为配对的两组变量 paired=T 意为该变量为配对变量，否则系统将认为这两组变量无关，按照两独立样本t检验执行 attach(intake) intake ## pre post ## 1 5260 3910 ## 2 5470 4220 ## 3 5640 3885 ## 4 6180 5160 ## 5 6390 5645 ## 6 6515 4680 ## 7 6805 5265 ## 8 7515 5975 ## 9 7515 6790 ## 10 8230 6900 ## 11 8770 7335 post - pre ## [1] -1350 -1250 -1755 -1020 -745 -1835 -1540 -1540 -725 -1330 -1435 t.test(pre, post, paired=T) ## ## Paired t-test ## ## data: pre and post ## t = 11.941, df = 10, p-value = 3.059e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1074.072 1566.838 ## sample estimates: ## mean of the differences ## 1320.455 # t.test(pre, post) #WRONG! wilcox.test(pre, post, paired=T) ## Warning in wilcox.test.default(pre, post, paired = T): cannot compute exact p- ## value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: pre and post ## V = 66, p-value = 0.00384 ## alternative hypothesis: true location shift is not equal to 0 "],["方差分析与秩和kruskal-wallis检验.html", "Chapter 2 方差分析与秩和Kruskal-Wallis检验 2.1 方差分析概述 2.2 单因素方差分析 2.3 双因素方差分析 2.4 重复测量的方差分析 2.5 注意", " Chapter 2 方差分析与秩和Kruskal-Wallis检验 2.1 方差分析概述 2.1.1 方差分析基本思想 \\[ 总变异=组间变异＋组内变异 \\] 2.1.2 方差分析用途 1.检验两个或多个样本均数间的差异有无统计学意义 两个样本均数的比较可以采用t检验或者F检验 两个以上样本均数的比较只能用F检验 2.回归方程的线性假设检验 3.检验两个或多个因素间有无交互作用 2.1.3 方差分应用条件 1.各个样本是相互独立的随机样本 2.各个样本来自正态总体 3.各个处理组的方差相等，即方差齐 4.不满足方差分析应用条件的处理 进行变量变换，以达到方差齐或正态的要求 采用非参数法（秩和检验） 使用近似F检验 2.1.4 方差分析的R语言实现 ➢ ANOVA和回归方法都是独立发展，但从函数形式上，它们都是广义线性模型的特例。回归中的lm()函数也能分析ANOVA模型。两个函数的结果是等同的。 ➢ aov()函数的语法为： aov(formula, data=dataframe) 表9-4列举了表达式中可以使用的特殊符号。表9-4中的y是因变量，字母A、B、C代表因子。 表9-5列举了一些常见的研究设计表达式。在表9-5中，小写字母表示定量变量， 大写字母表示组别因子，Subject是对被试者独有的标识变量 2.1.5 表达式中各项的顺序 表达式中效应的顺序在两种情况下会造成影响： (a)因子不止一个，并且是非平衡设计； (b)存在协变量。出现任意一种情况时，等式右边的变量都与其他每个变量相关。此时，我们无法清晰地划分它们对因变量的影响。例如，对于双因素方差分析，若不同处理方式中的观测数不同，那么模型y ~ A*B与模型y ~ B*A的结果不同 样本大小越不平衡，效应项的顺序对结果的影响越大。一般来说，越基础性的效应越需要放在表达式前面。具体来讲，首先是协变量，然后是主效应，接着是双因素的交互项，再接着是三因素的交互项，以此类推。对于主效应，越基础性的变量越应放在表达式前面，因此性别要放在处理方式之前。有一个基本的准则：若研究设计不是正交的（也就是说，因子和/或协变量相关），一定要谨慎设置效应的顺序。 顺序很重要！ 含因子A、B和因变量y的双因素不平衡因子设计，有三种效应：A和B的主效应，A 和B的交互效应。假设你正使用如下表达式对数据进行建模：Y ~ A + B + A:B 有三种类型的方法可以分解等式右边各效应对y所解释的方差。 ➢ 类型Ⅰ（序贯型）效应根据表达式中先出现的效应做调整。A不做调整，B根据A调整，A:B交互项根据A和B调整。 ➢ 类型Ⅱ（分层型）效应根据同水平或低水平的效应做调整。A根据B调整，B依据A调整，A:B交互项同时根据A和B调整。 ➢ 类型Ⅲ（边界型）每个效应根据模型其他各效应做相应调整。A根据B和A:B做调整，A:B交互项根据A和B调整。 R默认调用类型I方法，其他软件（比如SAS和SPSS）默认调用类型Ⅲ方法。 ➢请注意car包中的Anova()函数（不要与标准anova()函数混淆）提供了使用类型Ⅱ或类型Ⅲ方法的选项，而aov()函数使用的是类型I方法。若想使结果与其他软件（如SAS和SPSS）提供的结果保持一致，可以使用Anova()函数，细节可参考 help(Anova,package=\"car\")。 2.2 单因素方差分析 ➢ 单因素方差分析用于比较分类因子定义的两个或多个组别中的因变量均值。 以multcomp包中的 cholesterol 数据集为例，50个患者均接受降低胆固醇药物治疗（trt）五种疗法中的一种疗法。其中三种治疗条件使用药物相同，分别是20mg一天一次（1time）、10mg一天两次（2times）和5mg一天四次（4times）。剩下的两种方式（drugD和drugE）代表候选药物。哪种药物疗法降低胆固醇（响应变量）最多呢？分析过程见如下代码。 library(multcomp) attach(cholesterol) table(trt) #① ## trt ## 1time 2times 4times drugD drugE ## 10 10 10 10 10 aggregate(response, by=list(trt), FUN=mean) #② ## Group.1 x ## 1 1time 5.78197 ## 2 2times 9.22497 ## 3 4times 12.37478 ## 4 drugD 15.36117 ## 5 drugE 20.94752 aggregate(response, by=list(trt), FUN=sd) #③ ## Group.1 x ## 1 1time 2.878113 ## 2 2times 3.483054 ## 3 4times 2.923119 ## 4 drugD 3.454636 ## 5 drugE 3.345003 fit &lt;- aov(response ~ trt) #④ summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trt 4 1351.4 337.8 32.43 9.82e-13 *** ## Residuals 45 468.8 10.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 结果解读 从输出结果可以看到，每 10个患者接受其中一个药物疗法① 均值显示drugE降低胆固醇最多，而1time 降低胆固醇最少② 各组的标准差相对恒定，在2.88~3.48间浮动③ ANOVA对治疗方式 (trt)的 检验非常显著 (p&lt;0.0001),说明五种疗法的效果不同④ gplot包中的plotmeans()可以用来绘制带有置信区间的组均值图形 如图所示，图形展示了带有95%的置信区间的各疗法均值，可以清楚看到它们之间的差异。 library(gplots) plotmeans(response ~ trt, xlab=&quot;Treatment&quot;, ylab=&quot;Response&quot;, main=&quot;Mean Plot\\nwith 95% CI&quot;) detach(cholesterol) 2.2.1 多重比较 ➢ 虽然ANOVA对各疗法的F检验表明五种药物疗法效果不同，但是并没有告诉你哪 种疗法与其他疗法不同。多重比较可以解决这个问题。例如，TukeyHSD()函数 提供了对各组均值差异的成对检验（见如下代码）。 par(ask=TRUE) opar &lt;- par(no.readonly=TRUE) # save original parameters TukeyHSD(fit) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = response ~ trt) ## ## $trt ## diff lwr upr p adj ## 2times-1time 3.44300 -0.6582817 7.544282 0.1380949 ## 4times-1time 6.59281 2.4915283 10.694092 0.0003542 ## drugD-1time 9.57920 5.4779183 13.680482 0.0000003 ## drugE-1time 15.16555 11.0642683 19.266832 0.0000000 ## 4times-2times 3.14981 -0.9514717 7.251092 0.2050382 ## drugD-2times 6.13620 2.0349183 10.237482 0.0009611 ## drugE-2times 11.72255 7.6212683 15.823832 0.0000000 ## drugD-4times 2.98639 -1.1148917 7.087672 0.2512446 ## drugE-4times 8.57274 4.4714583 12.674022 0.0000037 ## drugE-drugD 5.58635 1.4850683 9.687632 0.0030633 par(las=2) par(mar=c(5,8,4,2)) plot(TukeyHSD(fit)) par(opar) # Multiple comparisons the multcomp package library(multcomp) par(mar=c(5,4,6,2)) tuk &lt;- glht(fit, linfct=mcp(trt=&quot;Tukey&quot;)) plot(cld(tuk, level=.05),col=&quot;lightgrey&quot;) par(opar) 结果解读 可以看到，1time和2times 的均值差异不显著 (p=0.138), 1time和4times间的差异非常显著(p&lt;0.001)。 成对比较图形如图所示。第一个par语句用来旋转轴标签，第二个用来增大左边界的面积，可使标签摆放更美观。 图形中置信区间包含0的疗法说明差异不显著(p&gt;0.5)。 2.2.2 评估方差分析的假设条件 ➢ 单因素方差分析中，我们假设因变量服从正态分布，各组方差相等。可使用Q-Q 图来检验正态性假设： library(car) qqPlot(lm(response ~ trt, data=cholesterol), simulate=TRUE, main=&quot;Q-Q Plot&quot;, labels=FALSE) ## [1] 19 38 ➢ R提供了一些可用来做方差齐性检验的函数。例如，可以通过如下代码来做 bartlett检验： bartlett.test(response ~ trt, data=cholesterol) ## ## Bartlett test of homogeneity of variances ## ## data: response by trt ## Bartlett&#39;s K-squared = 0.57975, df = 4, p-value = 0.9653 ➢ 方差齐性分析对离群点非常敏感。可利用car包中的outlierTest()函数来检测离群点： library(car) outlierTest(fit) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 19 2.251149 0.029422 NA 2.3 双因素方差分析 ➢ 双因素方差分析中，受试者被分配到两因子交叉类别组中。以基础安装包的 ToothGrowth 数据集为例，随机分配60只豚鼠，分别采用两种喂食方法（橙汁或维生素C），各喂食方法中抗坏血酸含量有三种水平（0.5mg、1mg或2mg），每种处理方式组合都被分配10只豚鼠。牙齿长度为因变量，代码如下 attach(ToothGrowth) table(supp,dose) ## dose ## supp 0.5 1 2 ## OJ 10 10 10 ## VC 10 10 10 aggregate(len, by=list(supp,dose), FUN=mean) ## Group.1 Group.2 x ## 1 OJ 0.5 13.23 ## 2 VC 0.5 7.98 ## 3 OJ 1.0 22.70 ## 4 VC 1.0 16.77 ## 5 OJ 2.0 26.06 ## 6 VC 2.0 26.14 aggregate(len, by=list(supp,dose), FUN=sd) ## Group.1 Group.2 x ## 1 OJ 0.5 4.459709 ## 2 VC 0.5 2.746634 ## 3 OJ 1.0 3.910953 ## 4 VC 1.0 2.515309 ## 5 OJ 2.0 2.655058 ## 6 VC 2.0 4.797731 dose &lt;- factor(dose) fit &lt;- aov(len ~ supp*dose) # fit1&lt;- aov(len~supp+dose+supp:dose) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 205.4 205.4 15.572 0.000231 *** ## dose 2 2426.4 1213.2 92.000 &lt; 2e-16 *** ## supp:dose 2 108.3 54.2 4.107 0.021860 * ## Residuals 54 712.1 13.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 table语句的预处理表明该设计是均衡设计（各设计单元中样本大小都相同）， aggregate语句处理可获得各单元的均值和标准差。dose变量被转换为因子变量，这样aov()函数就会将它当做一个分组变量，而不是一个数值型协变量。用summary()函数得到方差分析表，可以看到主效应（supp和dose）和交互效应都非常显著。 有多种方式对结果进行可视化处理。此处可用interaction.plot()函数来展示双因素方差分析的交互效应。 interaction.plot(dose, supp, len, type=&quot;b&quot;, col=c(&quot;red&quot;,&quot;blue&quot;), pch=c(16, 18), main = &quot;Interaction between Dose and Supplement Type&quot;) 还可以用gplots包中的plotmeans()函数来展示交互效应，代码如下： library(gplots) plotmeans(len ~ interaction(supp, dose, sep=&quot; &quot;), connect=list(c(1, 3, 5),c(2, 4, 6)), col=c(&quot;red&quot;,&quot;darkgreen&quot;), main = &quot;Interaction Plot with 95% CIs&quot;, xlab=&quot;Treatment and Dose Combination&quot;) 图形展示了均值、误差棒（95%的置信区间）和样本大小。 最后，你还能用HH包中的函数来可视化结果，图形对任意顺序的因子设计的主效应和交互效应都会进行展示 library(HH) ## 载入需要的程辑包：lattice ## 载入需要的程辑包：grid ## 载入需要的程辑包：latticeExtra ## 载入需要的程辑包：gridExtra ## ## 载入程辑包：&#39;HH&#39; ## The following objects are masked from &#39;package:car&#39;: ## ## logit, vif ## The following object is masked from &#39;package:gplots&#39;: ## ## residplot interaction2wt(len~supp*dose) 2.4 重复测量的方差分析 2.4.1 概述 一般来说，研究设计中考虑以下问题时应采用重复测量设计： ➢ 研究主要目的之一是考察某指标在不同时间的变化情况。 ➢ 研究个体间变异很大，应用普通研究设计的方差分析时，方差分析表中的误差项值将很大，即计算F值时的分母很大，对反应变量有作用的因素常难以识别。 ➢ 有的研究中研究对象很难征募到足够多的数量，此时可考虑对所征募到的对象在不同条件下的反应进行测量。 2.4.2 原理 ➢ 基本思想：仍然应用方差分析的基本思想，将反应变量的变异分解成以下四个部分：研究对象内的变异（即测量时间点或测量条件下的效应）、研究对象间的变异（即处理因素效应）、上述两者的交互作用、随机误差变异。 ➢ 因素： 受试者内因素—-用于区分重复测量次数的变量 受试者间因素—-在重复测量时保持恒定的因素 ➢ 分析目的：一是分析受试者间因素的作用；二是考察随着测量次数的增加，测量指标是如何发生变化的，以及分组因素的作用是否会随时间发生，即是否和时间存在交互作用 2.4.3 应用条件 ➢ 反应变量之间存在相关关系。 ➢ 反应变量的均数向量服从多元正态分布。 ➢ 对于自变量的各取值水平组合而言，反应变量的方差、协方差矩阵相等。 2.4.4 案例1 因变量是二氧化碳吸收量（uptake），单位为ml/L，自变量是植物类型Type（魁北克VS.密西西比）和七种水平（95~1000 umol/m^2 sec）的二氧化碳浓度（conc）。另外，Type是组间因子，conc是组内因子。Type已经被存储为一个因子变量，但你还需要先将conc转换为因子变量。 CO2$conc &lt;- factor(CO2$conc) w1b1 &lt;- subset(CO2, Treatment==&#39;chilled&#39;) fit &lt;- aov(uptake ~ (conc*Type) + Error(Plant/(conc)), w1b1) summary(fit) ## ## Error: Plant ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Type 1 2667.2 2667.2 60.41 0.00148 ** ## Residuals 4 176.6 44.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Plant:conc ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## conc 6 1472.4 245.40 52.52 1.26e-12 *** ## conc:Type 6 428.8 71.47 15.30 3.75e-07 *** ## Residuals 24 112.1 4.67 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 方差分析表表明在0.01 的水平下，主效应类型和浓度以及交叉效应类型 浓度都非常显著 par(las=2) par(mar=c(10,4,4,2)) with(w1b1, interaction.plot(conc,Type,uptake, type=&quot;b&quot;, col=c(&quot;red&quot;,&quot;blue&quot;), pch=c(16,18), main=&quot;Interaction Plot for Plant Type and Concentration&quot;)) boxplot(uptake ~ Type*conc, data=w1b1, col=(c(&quot;gold&quot;,&quot;green&quot;)), main=&quot;Chilled Quebec and Mississippi Plants&quot;, ylab=&quot;Carbon dioxide uptake rate (umol/m^2 sec)&quot;) par(opar) 通过interaction.plot()函数展示了交互效应,使用boxplot()函数对相同的数据画图。从以上任意一幅图都可以看出，魁北克省的植物比密西西比州的植物二氧化碳吸收率高，而且随着co浓度的升高，差异越来越明显。 2.5 注意 通常处理的数据集是宽格式(wide format)；即列是变量，行是观测值, litter数据框就是一个很好的例子。不过在处理重复测量设计时，需要有长格式 (long format) 数据才能拟合模型。在长格式中，因变量的每次测量都要放到 它独有的行中，CO2数据集即该种形式 幸运的是，reshape 包可方便地将数据转换为相应的格式。 "],["列联表资料的处理.html", "Chapter 3 列联表资料的处理 3.1 R×C表分类 3.2 列变量为有序变量的行均分检验（Cochran-Mantel-Haenszel） 3.3 行列变量均为有序变量检验方法 3.4 分层行列表分析 Mantel-Haenszel 检验", " Chapter 3 列联表资料的处理 3.1 R×C表分类 双向无序R X C 表， 表中两个分类变量皆为无序分类变量 ①若研究目的为多个样本率(或构成比) 比较，可用行×列表资料的 \\(\\chi ^2\\) 检验 ②若研究目的为分析两个分类变量之间有无关联性以及关系的密切程度，可以用行×列表资料的\\(\\chi^2\\)检验，以及Pearson 列联系数分析 单向有序R X C 表有两种形式 ①RXC 表中的分组变量是有序的，而结局变量是无序的，此种单向有序RXC 表资料可以用行×列表资料的\\(\\chi^2\\)检验进行分析 ②RXC 表中的分组变量是无序的，而结局变量是有序的，此种单向有序RXC表资料直用秩和检验分析 双向有序属性相同R X C表，表中的两个分类变量皆为有序且属性相同。实际上是2 X 2配对设计的扩展，此时宜用一致性检验(或称Kappa 检验)。 双向有序属性不同R X C 表，表中两个分类变量皆为有序且属性不同。对于该类资料，需要分析两个有序分类变量间是否存在线性变化趋势， 可用有序分组资料线性趋势检验。 3.2 列变量为有序变量的行均分检验（Cochran-Mantel-Haenszel） 3.2.1 行均分检验Cochran-Mantel-Haenszel # 行均分检验 Cochran-Mantel-Haenszel Example13_7 &lt;- read.table (&quot;data/example13_7.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example13_7) mytable &lt;- xtabs(~a + b) library(gmodels) ## Registered S3 method overwritten by &#39;gdata&#39;: ## method from ## reorder.factor gplots CrossTable(a, b) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 270 ## ## ## | b ## a | 1 | 2 | 3 | 4 | Row Total | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 5 | 31 | 49 | 15 | 100 | ## | 10.212 | 5.260 | 17.841 | 7.782 | | ## | 0.050 | 0.310 | 0.490 | 0.150 | 0.370 | ## | 0.098 | 0.246 | 0.671 | 0.750 | | ## | 0.019 | 0.115 | 0.181 | 0.056 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 22 | 50 | 9 | 4 | 85 | ## | 2.201 | 2.692 | 8.506 | 0.837 | | ## | 0.259 | 0.588 | 0.106 | 0.047 | 0.315 | ## | 0.431 | 0.397 | 0.123 | 0.200 | | ## | 0.081 | 0.185 | 0.033 | 0.015 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 24 | 45 | 15 | 1 | 85 | ## | 3.931 | 0.717 | 2.772 | 4.455 | | ## | 0.282 | 0.529 | 0.176 | 0.012 | 0.315 | ## | 0.471 | 0.357 | 0.205 | 0.050 | | ## | 0.089 | 0.167 | 0.056 | 0.004 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 51 | 126 | 73 | 20 | 270 | ## | 0.189 | 0.467 | 0.270 | 0.074 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## ## library(vcdExtra) ## 载入需要的程辑包：vcd ## ## 载入程辑包：&#39;vcd&#39; ## The following object is masked from &#39;package:HH&#39;: ## ## odds ## The following object is masked from &#39;package:latticeExtra&#39;: ## ## rootogram ## 载入需要的程辑包：gnm ## ## 载入程辑包：&#39;gnm&#39; ## The following object is masked from &#39;package:lattice&#39;: ## ## barley ## ## 载入程辑包：&#39;vcdExtra&#39; ## The following object is masked from &#39;package:carData&#39;: ## ## Burt CMHtest(mytable) ## Cochran-Mantel-Haenszel Statistics for a by b ## ## AltHypothesis Chisq Df Prob ## cor Nonzero correlation 46.297 1 1.0160e-11 ## rmeans Row mean scores differ 58.678 2 1.8125e-13 ## cmeans Col mean scores differ 48.838 3 1.4122e-10 ## general General association 66.958 6 1.7167e-12 detach (Example13_7) 结果解读 1 ●阅读结果时，阅读第二行的结果 AltHypothesis Chisq Df Prob rmeans Row mean scores differ 58.678 2 1.8125e-13 此处卡方值是58.678，P＜0.0001 3.2.2 Ordinal logistic回归模型 # 用Ordinal logistic回归模型 Example13_7 &lt;- read.table (&quot;data/example13_7.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example13_7) # 以下三行为设置哑变量 Example13_7$x1 &lt;- ifelse (a==1, 1, 0) Example13_7$x2 &lt;- ifelse (a==2, 1, 0) Example13_7$x3 &lt;- ifelse (a==3, 1, 0) library(rms) ## 载入需要的程辑包：Hmisc ## 载入需要的程辑包：Formula ## 载入需要的程辑包：ggplot2 ## ## 载入程辑包：&#39;ggplot2&#39; ## The following object is masked from &#39;package:latticeExtra&#39;: ## ## layer ## ## 载入程辑包：&#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## 载入需要的程辑包：SparseM ## ## 载入程辑包：&#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve ## ## 载入程辑包：&#39;rms&#39; ## The following object is masked from &#39;package:HH&#39;: ## ## vif ## The following objects are masked from &#39;package:car&#39;: ## ## Predict, vif # 拟合等级回归模型，此处实际上是以C(x3=1)为参照 fit1 &lt;- lrm(b~ x1 + x2 , data=Example13_7, model=FALSE, x=FALSE, y=FALSE) fit1 ## Logistic Regression Model ## ## lrm(formula = b ~ x1 + x2, data = Example13_7, model = FALSE, ## x = FALSE, y = FALSE) ## ## ## Frequencies of Responses ## ## 1 2 3 4 ## 51 126 73 20 ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 270 LR chi2 66.98 R2 0.241 C 0.686 ## max |deriv| 3e-07 d.f. 2 g 0.976 Dxy 0.372 ## Pr(&gt; chi2) &lt;0.0001 gr 2.654 gamma 0.536 ## gp 0.213 tau-a 0.249 ## Brier 0.174 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## y&gt;=2 0.9782 0.2248 4.35 &lt;0.0001 ## y&gt;=3 -1.5521 0.2432 -6.38 &lt;0.0001 ## y&gt;=4 -3.7483 0.3375 -11.11 &lt;0.0001 ## x1 2.0850 0.3088 6.75 &lt;0.0001 ## x2 0.0028 0.2955 0.01 0.9926 ## # 将fit1的回归系数单列出来 coefficients(fit1) ## y&gt;=2 y&gt;=3 y&gt;=4 x1 x2 ## 0.978150920 -1.552142851 -3.748324155 2.084976695 0.002753265 # 将回归系数取对数，得到OR值 exp(coefficients(fit1)) ## y&gt;=2 y&gt;=3 y&gt;=4 x1 x2 ## 2.65953400 0.21179365 0.02355719 8.04440400 1.00275706 detach (Example13_7) 说明： 哑变量设置后，a列实际上由X1、X2、X3三列表示，a列中1变为(1,0,0)，2变为(0.1.0),3变为(0,0,1) 结果判读 2 (1) y=2，y=3, y=4实际上是以y=1作为参照 主要解读x1 &amp; x2等自变量的结果 计算OR值，对于累积比数因变量模型，平行性假设决定了每个自变量的OR值对于前g-1 个模型是相同的。例如，自变量xl 的OR=8.044 ，表示使用A 药物治愈的可能性是C药物的8.044 倍;也表示使用A 药物显效或治愈的可能性是C药的8.044倍;同时也表示使用A 药物至少好转的可能性是C药的8.044 倍。 3.3 行列变量均为有序变量检验方法 双向有序属性不同R X C表 例13-8 为了研究晶状体混浊程度是否与年龄相关，将资料整理为表13 -6 的形式，试编写R程序，分析年龄与晶状体混浊程度的相关关系。 3.3.1 sperman秩相关 Example13_8 &lt;- read.table (&quot;data/example13_8.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example13_8) cor(Example13_8, method=&quot;spearman&quot;) #计算秩相关系数 ## a b ## a 1.0000000 0.2533556 ## b 0.2533556 1.0000000 cor.test(a, b, method=&quot;spearman&quot;) # 检验秩相关系数有无意义 ## Warning in cor.test.default(a, b, method = &quot;spearman&quot;): Cannot compute exact p- ## value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: a and b ## S = 135583954, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.2533556 detach (Example13_8) 结果解读 查看相关系数以及P值 3.3.2 线性趋势检验 # 双向有序属性不同 线性趋势检验 Example13_8 &lt;- read.table (&quot;data/example13_8.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example13_8) library(gmodels) CrossTable(a, b) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 1029 ## ## ## | b ## a | 1 | 2 | 3 | Row Total | ## -------------|-----------|-----------|-----------|-----------| ## 1 | 215 | 131 | 148 | 494 | ## | 21.863 | 0.797 | 11.700 | | ## | 0.435 | 0.265 | 0.300 | 0.480 | ## | 0.660 | 0.444 | 0.363 | | ## | 0.209 | 0.127 | 0.144 | | ## -------------|-----------|-----------|-----------|-----------| ## 2 | 67 | 101 | 128 | 296 | ## | 7.646 | 3.070 | 0.964 | | ## | 0.226 | 0.341 | 0.432 | 0.288 | ## | 0.206 | 0.342 | 0.314 | | ## | 0.065 | 0.098 | 0.124 | | ## -------------|-----------|-----------|-----------|-----------| ## 3 | 44 | 63 | 132 | 239 | ## | 13.287 | 0.444 | 14.631 | | ## | 0.184 | 0.264 | 0.552 | 0.232 | ## | 0.135 | 0.214 | 0.324 | | ## | 0.043 | 0.061 | 0.128 | | ## -------------|-----------|-----------|-----------|-----------| ## Column Total | 326 | 295 | 408 | 1029 | ## | 0.317 | 0.287 | 0.397 | | ## -------------|-----------|-----------|-----------|-----------| ## ## mytable &lt;- xtabs(~a + b) chisq.test(mytable) ## ## Pearson&#39;s Chi-squared test ## ## data: mytable ## X-squared = 74.402, df = 4, p-value = 2.667e-15 fit &lt;- lm(a~b) summary(fit) ## ## Call: ## lm(formula = a ~ b) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.97398 -0.73298 0.02602 0.50802 1.50802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.25098 0.06501 19.242 &lt; 2e-16 *** ## b 0.24100 0.02898 8.315 2.88e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7817 on 1027 degrees of freedom ## Multiple R-squared: 0.06308, Adjusted R-squared: 0.06217 ## F-statistic: 69.15 on 1 and 1027 DF, p-value: 2.876e-16 coefficients(fit) ## (Intercept) b ## 1.2509772 0.2410021 confint(fit) ## 2.5 % 97.5 % ## (Intercept) 1.1234015 1.3785529 ## b 0.1841306 0.2978736 detach (Example13_8) 双向有序属性相同R X C表——一致性检验 例13-11 某学校学生的文化课成绩和体育课成绩整理如表13-7 所示，试对学生文化课和体育课成绩进行一致性检验。 3.3.3 一致性检验 # 一致性检验 Example13_10 &lt;- read.table (&quot;data/example13_10.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example13_10) library(gmodels) CrossTable(a, b) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 395 ## ## ## | b ## a | 1 | 2 | 3 | 4 | Row Total | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 4 | 7 | 8 | 3 | 22 | ## | 4.883 | 0.252 | 1.573 | 0.153 | | ## | 0.182 | 0.318 | 0.364 | 0.136 | 0.056 | ## | 0.160 | 0.067 | 0.036 | 0.070 | | ## | 0.010 | 0.018 | 0.020 | 0.008 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 5 | 56 | 74 | 15 | 150 | ## | 2.127 | 6.899 | 1.348 | 0.108 | | ## | 0.033 | 0.373 | 0.493 | 0.100 | 0.380 | ## | 0.200 | 0.538 | 0.332 | 0.349 | | ## | 0.013 | 0.142 | 0.187 | 0.038 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 10 | 32 | 128 | 17 | 187 | ## | 0.285 | 6.033 | 4.765 | 0.554 | | ## | 0.053 | 0.171 | 0.684 | 0.091 | 0.473 | ## | 0.400 | 0.308 | 0.574 | 0.395 | | ## | 0.025 | 0.081 | 0.324 | 0.043 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 4 | 6 | 9 | 13 | 8 | 36 | ## | 6.078 | 0.024 | 2.639 | 4.250 | | ## | 0.167 | 0.250 | 0.361 | 0.222 | 0.091 | ## | 0.240 | 0.087 | 0.058 | 0.186 | | ## | 0.015 | 0.023 | 0.033 | 0.020 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 25 | 104 | 223 | 43 | 395 | ## | 0.063 | 0.263 | 0.565 | 0.109 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## ## mytable &lt;- xtabs(~a + b) mcnemar.test(mytable) ## ## McNemar&#39;s Chi-squared test ## ## data: mytable ## McNemar&#39;s chi-squared = 20.23, df = 6, p-value = 0.00252 library(fmsb) ## ## 载入程辑包：&#39;fmsb&#39; ## The following object is masked from &#39;package:vcd&#39;: ## ## oddsratio Kappa.test(mytable, conf.level=0.95) ## $Result ## ## Estimate Cohen&#39;s kappa statistics and test the null hypothesis that ## the extent of agreement is same as random (kappa=0) ## ## data: mytable ## Z = 4.7276, p-value = 1.136e-06 ## 95 percent confidence interval: ## 0.1068853 0.2661199 ## sample estimates: ## [1] 0.1865026 ## ## ## $Judgement ## [1] &quot;Slight agreement&quot; detach (Example13_10) 3.4 分层行列表分析 Mantel-Haenszel 检验 例13-12 为研究心肌梗塞与近期使用避孕药之间的关系，在5所医院中采用病例-对照研究方法调查了234名心肌梗塞病人与1742 名对照者使用口服避孕药状况，资料见表13-8 。请在排除了研究医院影响后，分析使用口服避孕药与否对是否患心肌梗塞病的影响情况。 knitr::include_graphics(&quot;fig/5d.png&quot;) 3.4.1 Mantel-Haenszel 检验 # Mantel-Haenszel 检验 Example13_12 &lt;- read.table (&quot;data/example13_12.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example13_12) mytable &lt;- xtabs(~drug + case + hos) #分层输出各家医院四格表频数 mytable ## , , hos = 1 ## ## case ## drug 1 2 ## 1 4 62 ## 2 2 224 ## ## , , hos = 2 ## ## case ## drug 1 2 ## 1 9 33 ## 2 12 390 ## ## , , hos = 3 ## ## case ## drug 1 2 ## 1 4 26 ## 2 33 330 ## ## , , hos = 4 ## ## case ## drug 1 2 ## 1 6 9 ## 2 65 362 ## ## , , hos = 5 ## ## case ## drug 1 2 ## 1 6 5 ## 2 93 301 prop.table(mytable,3) #分层输出各家医院四格表百分数 ## , , hos = 1 ## ## case ## drug 1 2 ## 1 0.013698630 0.212328767 ## 2 0.006849315 0.767123288 ## ## , , hos = 2 ## ## case ## drug 1 2 ## 1 0.020270270 0.074324324 ## 2 0.027027027 0.878378378 ## ## , , hos = 3 ## ## case ## drug 1 2 ## 1 0.010178117 0.066157761 ## 2 0.083969466 0.839694656 ## ## , , hos = 4 ## ## case ## drug 1 2 ## 1 0.013574661 0.020361991 ## 2 0.147058824 0.819004525 ## ## , , hos = 5 ## ## case ## drug 1 2 ## 1 0.014814815 0.012345679 ## 2 0.229629630 0.743209877 addmargins(mytable) #分层输出各家医院四格表边际频数 ## , , hos = 1 ## ## case ## drug 1 2 Sum ## 1 4 62 66 ## 2 2 224 226 ## Sum 6 286 292 ## ## , , hos = 2 ## ## case ## drug 1 2 Sum ## 1 9 33 42 ## 2 12 390 402 ## Sum 21 423 444 ## ## , , hos = 3 ## ## case ## drug 1 2 Sum ## 1 4 26 30 ## 2 33 330 363 ## Sum 37 356 393 ## ## , , hos = 4 ## ## case ## drug 1 2 Sum ## 1 6 9 15 ## 2 65 362 427 ## Sum 71 371 442 ## ## , , hos = 5 ## ## case ## drug 1 2 Sum ## 1 6 5 11 ## 2 93 301 394 ## Sum 99 306 405 ## ## , , hos = Sum ## ## case ## drug 1 2 Sum ## 1 29 135 164 ## 2 205 1607 1812 ## Sum 234 1742 1976 mantelhaen.test(mytable) ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: mytable ## Mantel-Haenszel X-squared = 32.793, df = 1, p-value = 1.025e-08 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.426983 6.493688 ## sample estimates: ## common odds ratio ## 3.969895 detach (Example13_12) "],["秩和检验处理列联表资料.html", "Chapter 4 秩和检验处理列联表资料 4.1 等级分组资料的非参数检验 4.2 多个组的等级指标的非参数检验", " Chapter 4 秩和检验处理列联表资料 4.1 等级分组资料的非参数检验 例14- 13 用某药治疗不同病情( A 型和B 型)的老年慢性支气管炎病人， 疗效见表14-10 ，试比较该药对两种病情的疗效。 #代码 example14_15 &lt;- read.table (&quot;data/example14_15.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(example14_15) mytable&lt;- xtabs(~x+g) mytable wilcox.test(x ~ g) detach(example14_15) #结果 g x 1 2 1 65 42 2 18 6 3 30 23 4 13 11 Wilcoxon rank sum test with continuity correction data: x by g W = 4954.5, p-value = 0.5883 alternative hypothesis: true location shift is not equal to 0 4.2 多个组的等级指标的非参数检验 例14-14 4 种疾病患者痰液内嗜酸性粒细胞的检查结果见表14-11。试分析4 种疾病患者痰液内嗜酸性粒细胞有无差别。 # 多个组的等级指标的非参数检验 example14_16 &lt;- read.table (&quot;data/example14_16.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(example14_16) mytable&lt;- xtabs(~x + g) mytable kruskal.test(x~ g) # 设置两两比较 library(nparcomp) nparcomp(x ~ g, data=example14_16, alternative = &quot;two.sided&quot;) detach(example14_16) g x 1 2 3 4 1 0 3 5 3 2 2 5 7 5 3 9 5 3 3 4 6 2 2 0 Kruskal-Wallis rank sum test data: x by g Kruskal-Wallis chi-squared = 15.506, df = 3, p-value = 0.001432 #------Nonparametric Multiple Comparisons for relative contrast effects-----# "],["简单线性回归.html", "Chapter 5 简单线性回归 5.1 回归分析对资料的要求 5.2 直线回归方程 5.3 回归系数b的统计学意义 5.4 回归分析的步骤 5.5 Y的离均差平方和的划分 5.6 三种平方和的意义 5.7 应用回归分析应注意的问题 5.8 简单线性回归的R语言实现 5.9 案例 1 5.10 案例 2 5.11 案例 3", " Chapter 5 简单线性回归 5.1 回归分析对资料的要求 ➢ 回归分析涉及到两个变量，X与Y，其中X称自变量，Y 为因变量或反应变量。 Y — 必须是呈正态分布的随机变量 X可以是非随机变量: 年龄、药物浓度或剂量：Ⅰ型回归 X也可以是随机变量: 身高、体重、血清胆固醇的含量，血红蛋白的含量：Ⅱ型回归 5.2 直线回归方程 由X推算Y的直线回归方程一般表达式为 \\(\\widehat{Y}=a+bX\\) a称为截距，b为回归系数，即直线的斜率 5.2.1 直线回归方程的建立 \\[ b=\\cfrac{\\sum(X-\\overline{X})(Y-\\overline{Y})}{\\sum(X-\\overline{X})^2}=\\cfrac{lxy}{lxx} \\] \\[ a= \\bar Y - b \\bar X \\] 式中\\(\\overline {X}\\)、\\(\\overline {Y}\\)分别是X、Y的均数；\\(l_{xx}\\)为X的离均差平方和；\\(l_{xy}\\)为X与Y的离均差积和，按下式计算 \\(l_{xy}=\\sum(X-\\overline{X})(Y-\\overline{Y})=\\sum XY-\\cfrac{(\\sum X)(\\sum Y)}{n}\\) 5.3 回归系数b的统计学意义 ➢ b&gt;0时,Y随X增大而增大; ➢ b&lt;0时,Y随X的增大而减少； ➢ b=0时,X与Y无直线关系。 ➢ b的统计学意义是：X每增（减）一个单位，Y平均改变 b个单位。 5.4 回归分析的步骤 ➢ 用原始数据绘制散点图； ➢ 求a和b (如果呈直线关系) ➢ 对回归系数b作假设检验（方法：a. F检验 b. t检验 c. 用r检验来代替）。 ➢ 如果x与y存在直线关系（ b假设检验的结果P&lt;0.05），列出回归方程。否则，不列回归方程。 5.5 Y的离均差平方和的划分 ➢ 第一段：\\((Y-\\widehat{Y})\\)表示P点与回归直线的纵向距离, 即实测值Y与估计值\\(\\widehat{Y}\\)之差, 称剩余或残差。 ➢ 第二段：\\((Y-\\widehat{Y})\\) 即估计值与均数之差，它与回归系数的大小有关。|b|值越大， \\((Y-\\widehat{Y})\\) 的差值也越大，反之越小。当b=0时，则 \\((Y-\\widehat{Y})=0\\) 。则 \\((Y-\\widehat{Y})=(Y-\\overline{Y})\\) 也就是回归直线并不能使残差减小。 ➢ 第三段： \\(\\overline{Y}\\) ，是应变量Y的均数 \\(\\sum (Y-\\widehat{Y})^2 = \\sum(\\widehat{Y}-\\overline{Y})^2+\\sum(Y-\\widehat{Y})^2\\) 5.5.1 三种平方和的关系 \\(\\sum(Y-\\overline{Y})^2\\) 为总平方和，用\\(SS_总\\)表示 \\(\\sum(Y-\\widehat{Y})^2\\) 为回归平方和，用\\(SS_回\\)表示 \\(\\sum(\\widehat{Y}-\\overline{Y})^2\\)为剩余平方和，用\\(SS_剩\\)表示 \\(SS_总=SS_回+SS_剩\\) 5.6 三种平方和的意义 （1）SS总，为Y值的离均差平方和，说明未考虑X与Y的回归关系时Y总的变异。 （2）SS回，它反映在Y的变异中由于X与Y的直线关系而使Y变异减少的部分，也是在总平方和中可以用X解析的部分。SS回越大，说明回归效果越好。 （3）SS剩，反映X对Y的线性影响之外其它因素对Y的变异的作用，也是在总平方和中无法用X解析的部分。SS剩越小，说明回归方程的估计误差越小。 5.6.1 三种平方和的自由度及其关系 \\(v_总=n-1，v_回=1，v_剩=n-2\\) \\(v_总=v_回+v_剩\\) 5.7 应用回归分析应注意的问题 1.作回归分析要有实际意义 2.进行直线回归分析前，应绘制散点图 ☆作用：① 看散点图是否呈直线趋势；②有无异常点、高杠杆点和强影响点 3注意建立线性回归模型的基本条件：线性、独立性、正态性、方差齐性 4.直线回归方程的适用范围以求回归方程时的X的实测值范围为限；若无充分理由证明超过该范围还是直线，应避免外延。 5.两变量有线性关系，不一定是因果关系，也不一定表面两变量间确有内在联系。 5.8 简单线性回归的R语言实现 在R中，拟合线性模型最基本的函数就是lm()，格式为：myfit &lt;- lm(formula, data) ◎其中，formula指要拟合的模型形式，data是一个数据框，包含了用于拟合模型的数据。结果对象（本例中是myfit）存储在一个列表中，包含了所拟合模型的大量信息。表达式（formula）形式如下：Y ~ X1 + X2 + ... + Xk ◎~左边为响应变量，右边为各个预测变量，预测变量之间用+符号分隔。表8-2中的符号可以用不同方式修改这一表达式 ◎当回归模型包含一个因变量和一个自变量时，我们称为简单线性回归。当只有一个预测变量，但同时包含变量的幂（比如，\\(X\\)、\\(X^2\\)、\\(X^3\\)）时，我们称为多项式回归。当有不止一个预测变量时，则称为多元线性回归。 5.9 案例 1 ➢ 让我们通过一个回归示例来熟悉表8-3中的函数。基础安装中的数据集women提供了15个年龄在30~39岁间女性的身高和体重信息，我们想通过身高来预测体重，获得一个等式可以帮助我们分辨出那些过重或过轻的个体。代码如下： fit &lt;- lm(weight ~ height, data=women) summary(fit) ## ## Call: ## lm(formula = weight ~ height, data = women) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7333 -1.1333 -0.3833 0.7417 3.1167 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -87.51667 5.93694 -14.74 1.71e-09 *** ## height 3.45000 0.09114 37.85 1.09e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.525 on 13 degrees of freedom ## Multiple R-squared: 0.991, Adjusted R-squared: 0.9903 ## F-statistic: 1433 on 1 and 13 DF, p-value: 1.091e-14 coefficients(fit) #列出拟合模型的模型参数（截距项和斜率） ## (Intercept) height ## -87.51667 3.45000 women$weight ## [1] 115 117 120 123 126 129 132 135 139 142 146 150 154 159 164 fitted(fit) #列出拟合模型的预测值 ## 1 2 3 4 5 6 7 8 ## 112.5833 116.0333 119.4833 122.9333 126.3833 129.8333 133.2833 136.7333 ## 9 10 11 12 13 14 15 ## 140.1833 143.6333 147.0833 150.5333 153.9833 157.4333 160.8833 residuals(fit) #列出拟合模型的残差值 ## 1 2 3 4 5 6 ## 2.41666667 0.96666667 0.51666667 0.06666667 -0.38333333 -0.83333333 ## 7 8 9 10 11 12 ## -1.28333333 -1.73333333 -1.18333333 -1.63333333 -1.08333333 -0.53333333 ## 13 14 15 ## 0.01666667 1.56666667 3.11666667 plot(women$height,women$weight, main=&quot;Women Age 30-39&quot;, xlab=&quot;Height (in inches)&quot;, ylab=&quot;Weight (in pounds)&quot;) # add the line of best fit abline(fit) 结果解读 ◎通过输出结果，可以得到预测等式：\\(\\widehat{Weight}= -87.52+3.45×Height\\) ◎由于简单回归只有一个预测变量，此处F检验等同于身高回归系数的t检验 ◎Intercept：截距 ◎height：斜率（斜率在结果中的名称根据数据的自变量名称改变） ◎Adjusted R-squared: 决定系数（模型好、结果准确） 5.10 案例 2 library(foreign) library(stats) Example9_4 &lt;- read.table (&quot;data/example9_4.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example9_4) plot(Example9_4) fit &lt;-lm(y~x) anova(fit) summary (fit) confint(fit) y #输出观测值 fitted (fit) #输出预测值 residuals (fit) #输出残差 detach (Example9_4) 5.11 案例 3 例9-5 大多数公司最终会询问关于花费在广告上的费用对公司产品销售额的影响程度。由于广告需要一定的时间才能达到它的效应，同时它的效应也不是永久持续的，它的影响也许仅仅延续开头的一段时期。假设公司相信销售额与当月以及前两个月内所花的广告费有较密切的关系，假设它们之间存在线性关系，现在有某公司15 月内有关广告花费X 与销售额Y 的数据，如表9-7 所示。 Example9_5 &lt;- read.table (&quot;data/example9_5.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example9_5) Example9_5 #非必须步骤，此处是为了查看数据的录入形式(注1) ## SALES ADV ADVLAG1 ADVLAG2 ## 1 2945 280 NA NA ## 2 4295 400 280 NA ## 3 5645 450 400 280 ## 4 6995 590 450 400 ## 5 8345 650 590 450 ## 6 9695 750 650 590 ## 7 11045 890 750 650 ## 8 12395 1000 890 750 ## 9 13745 1050 1000 890 ## 10 15095 1200 1050 1000 ## 11 16445 1250 1200 1050 ## 12 17795 1350 1250 1200 ## 13 19145 1460 1350 1250 ## 14 20495 1500 1460 1350 ## 15 21845 1650 1500 1460 fit &lt;- lm(SALES~ADV + ADVLAG1 + ADVLAG2) anova(fit) ## Analysis of Variance Table ## ## Response: SALES ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ADV 1 330167453 330167453 9661.0106 5.924e-15 *** ## ADVLAG1 1 966283 966283 28.2744 0.0004827 *** ## ADVLAG2 1 253686 253686 7.4231 0.0234332 * ## Residuals 9 307577 34175 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary (fit) #生成一个拟合模型的方差分析表，或者比较两个或更多拟合模型的方差分析表 ## ## Call: ## lm(formula = SALES ~ ADV + ADVLAG1 + ADVLAG2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -258.52 -127.81 24.02 142.31 208.43 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 522.131 372.394 1.402 0.19442 ## ADV 3.681 1.779 2.070 0.06842 . ## ADVLAG1 4.966 1.466 3.387 0.00804 ** ## ADVLAG2 5.200 1.908 2.725 0.02343 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 184.9 on 9 degrees of freedom ## (因为不存在，2个观察量被删除了) ## Multiple R-squared: 0.9991, Adjusted R-squared: 0.9988 ## F-statistic: 3232 on 3 and 9 DF, p-value: 5.822e-14 SALES #输出观测值 ## [1] 2945 4295 5645 6995 8345 9695 11045 12395 13745 15095 16445 17795 ## [13] 19145 20495 21845 fitted (fit) #输出预测值 ## 3 4 5 6 7 8 9 10 ## 5620.983 7008.622 8184.699 9578.727 10902.686 12522.813 13981.057 15353.516 ## 11 12 13 14 15 ## 16542.436 17938.801 19100.320 20313.769 21636.570 residuals (fit) #输出残差 ## 3 4 5 6 7 8 9 ## 24.01694 -13.62206 160.30065 116.27277 142.31395 -127.81293 -236.05694 ## 10 11 12 13 14 15 ## -258.51569 -97.43621 -143.80111 44.67967 181.23083 208.43013 detach (Example9_5) 于商家认为当月的销售额与前两个月的广告投入有关，故将之前一个月和之前两个月分别列出，当做两个独立的变量处理。故而第一行和第二行数据有缺失值（因为没有之前的数据） "],["残差与回归值预测域与置信带.html", "Chapter 6 残差与回归值&amp;预测域与置信带", " Chapter 6 残差与回归值&amp;预测域与置信带 library(ISwR) attach(thuesen) fit&lt;- lm(short.velocity~blood.glucose) summary(fit) ## ## Call: ## lm(formula = short.velocity ~ blood.glucose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40141 -0.14760 -0.02202 0.03001 0.43490 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.09781 0.11748 9.345 6.26e-09 *** ## blood.glucose 0.02196 0.01045 2.101 0.0479 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2167 on 21 degrees of freedom ## (因为不存在，1个观察量被删除了) ## Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343 ## F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479 plot(blood.glucose,short.velocity) abline(fit) lm.velo &lt;- lm(short.velocity~blood.glucose) fitted(lm.velo) ## 1 2 3 4 5 6 7 8 ## 1.433841 1.335010 1.275711 1.526084 1.255945 1.214216 1.302066 1.341599 ## 9 10 11 12 13 14 15 17 ## 1.262534 1.365758 1.244964 1.212020 1.515103 1.429449 1.244964 1.190057 ## 18 19 20 21 22 23 24 ## 1.324029 1.372346 1.451411 1.389916 1.205431 1.291085 1.306459 plot(blood.glucose,short.velocity) # lines(blood.glucose,fitted(lm.velo)) # wrong 因有缺失值，所以此命令报错 lines(blood.glucose[!is.na(short.velocity)],fitted(lm.velo)) # cc &lt;- complete.cases(thuesen) # options(na.action=na.exclude) # lm.velo &lt;- lm(short.velocity~blood.glucose) # fitted(lm.velo) # lines(blood.glucose,fitted(lm.velo)) segments(blood.glucose,fitted(lm.velo), blood.glucose,short.velocity) #带有残差线段的图形 plot(fitted(lm.velo),resid(lm.velo)) # 残差与回归值的散点图 qqnorm(resid(lm.velo)) # Q-Q图 predict(lm.velo) ## 1 2 3 4 5 6 7 8 ## 1.433841 1.335010 1.275711 1.526084 1.255945 1.214216 1.302066 1.341599 ## 9 10 11 12 13 14 15 17 ## 1.262534 1.365758 1.244964 1.212020 1.515103 1.429449 1.244964 1.190057 ## 18 19 20 21 22 23 24 ## 1.324029 1.372346 1.451411 1.389916 1.205431 1.291085 1.306459 predict(lm.velo,int=&quot;c&quot;) # 置信区间confidence interval ## fit lwr upr ## 1 1.433841 1.291371 1.576312 ## 2 1.335010 1.240589 1.429431 ## 3 1.275711 1.169536 1.381887 ## 4 1.526084 1.306561 1.745607 ## 5 1.255945 1.139367 1.372523 ## 6 1.214216 1.069315 1.359118 ## 7 1.302066 1.205244 1.398889 ## 8 1.341599 1.246317 1.436881 ## 9 1.262534 1.149694 1.375374 ## 10 1.365758 1.263750 1.467765 ## 11 1.244964 1.121641 1.368287 ## 12 1.212020 1.065457 1.358583 ## 13 1.515103 1.305352 1.724854 ## 14 1.429449 1.290217 1.568681 ## 15 1.244964 1.121641 1.368287 ## 17 1.190057 1.026217 1.353898 ## 18 1.324029 1.230050 1.418008 ## 19 1.372346 1.267629 1.477064 ## 20 1.451411 1.295446 1.607377 ## 21 1.389916 1.276444 1.503389 ## 22 1.205431 1.053805 1.357057 ## 23 1.291085 1.191084 1.391086 ## 24 1.306459 1.210592 1.402326 predict(lm.velo,int=&quot;p&quot;) # 预测区间prediction interval.预测区间PI总是要比对应的置信区间CI大，这是因为在对单个响应与响应均值的预测中包括了更多的不确定性。 ## Warning in predict.lm(lm.velo, int = &quot;p&quot;): predictions on current data refer to _future_ responses ## fit lwr upr ## 1 1.433841 0.9612137 1.906469 ## 2 1.335010 0.8745815 1.795439 ## 3 1.275711 0.8127292 1.738693 ## 4 1.526084 1.0248161 2.027352 ## 5 1.255945 0.7904672 1.721423 ## 6 1.214216 0.7408499 1.687583 ## 7 1.302066 0.8411393 1.762993 ## 8 1.341599 0.8809929 1.802205 ## 9 1.262534 0.7979780 1.727090 ## 10 1.365758 0.9037136 1.827802 ## 11 1.244964 0.7777510 1.712177 ## 12 1.212020 0.7381424 1.685898 ## 13 1.515103 1.0180367 2.012169 ## 14 1.429449 0.9577873 1.901111 ## 15 1.244964 0.7777510 1.712177 ## 17 1.190057 0.7105546 1.669560 ## 18 1.324029 0.8636906 1.784367 ## 19 1.372346 0.9096964 1.834996 ## 20 1.451411 0.9745421 1.928281 ## 21 1.389916 0.9252067 1.854626 ## 22 1.205431 0.7299634 1.680899 ## 23 1.291085 0.8294798 1.752690 ## 24 1.306459 0.8457315 1.767186 # pred.frame &lt;- data.frame(thuesen[4:20,]) # blood.glucose 的值是随机排列的，我们不希望置信曲线上的线段杂乱无章地排列 pred.frame &lt;- data.frame(blood.glucose= 4:20) pp &lt;- predict(lm.velo, int=&quot;p&quot;, newdata=pred.frame) pc &lt;- predict(lm.velo, int=&quot;c&quot;, newdata=pred.frame) plot(blood.glucose,short.velocity, ylim=range(short.velocity, pp, na.rm=T)) pred.gluc &lt;- pred.frame$blood.glucose matlines(pred.gluc, pc, lty=c(1,2,2), col=&quot;black&quot;) matlines(pred.gluc, pp, lty=c(1,3,3), col=&quot;black&quot;) detach(thuesen) "],["多元线性回归.html", "Chapter 7 多元线性回归 7.1 多元线性回归模型 7.2 回归模型的应用条件 7.3 偏回归系数的估计 7.4 案例一 7.5 案例二 7.6 用线性回归来做ANOVA", " Chapter 7 多元线性回归 7.1 多元线性回归模型 ➢ 因变量y, 自变量为x1,x2,…,xm \\[ \\widehat{y}=a+b_1x_1+b_2x_2+…+b_mx_m \\] ➢ a为截距(intercept)，又称常数项(constant),表示各自变量均为0时y 的估计值 ➢ bi 称为偏回归系数(partial regression coefficient)，简称为回归系数 ➢ \\(\\hat{y}\\)称为 y 的估计值或预测值(predicted value) 7.2 回归模型的应用条件 \\[ y_i=\\widehat{y}+e_i=b_0+b_1x_{1i}+b_2x_{2i}+ \\cdots +b_mx_{mi}+e_i \\] \\(e_i\\) 称为残差: 自变量与因变量的关系是线性的(Linear )； \\(Cov(e_i,e_j)=0\\)，即独立性(Independence )； \\(e_i\\)~\\(N(0,\\sigma^2)\\)，即正态性(Normality)； \\(Var(e_i)=\\sigma^2\\)，即方差齐性(Equal variance)； 7.3 偏回归系数的估计 最小二乘法 (least square, LS) 基本思想：残差平方和(sum of squares for residuals)最小！ 7.3.1 回归方程的假设检验 未引进回归时的总变异(sum of squares about the mean of Y):\\(\\sum (Y-\\overline Y)^2\\) 引进回归以后的剩余变异(sum of squares about regression): \\(\\sum (Y-\\widehat Y)^2\\) 回归的贡献，回归平方和(sum of squares due to regression)：\\(\\sum (\\widehat Y-\\overline Y)^2\\) 7.4 案例一 例10-1 某学校20 名一年级女大学生体重(公斤)、胸围(厘米)、肩宽(厘米)及肺活量(升)实测值如表1 0-1 所示，试对影响女大学生肺活量的有关因素作多元回归分析 # 第三节多元线性回归案例1 Example10_1 &lt;- read.table (&quot;data/example10_1.csv&quot;, header=TRUE, sep=&quot;,&quot;) library(MASS) attach(Example10_1) fit1 &lt;- lm(y~ x1 + x2 + x3) fit2 &lt;- lm(y ~ 1)#拟合一个空模型 stepAIC(fit2,direction=&quot;both&quot;,scope=list(upper=fit1,lower=fit2)) ## Start: AIC=-26.8 ## y ~ 1 ## ## Df Sum of Sq RSS AIC ## + x3 1 2.4153 2.3227 -39.060 ## + x1 1 2.3089 2.4291 -38.164 ## + x2 1 1.2593 3.4787 -30.982 ## &lt;none&gt; 4.7380 -26.802 ## ## Step: AIC=-39.06 ## y ~ x3 ## ## Df Sum of Sq RSS AIC ## + x1 1 0.57921 1.7435 -42.797 ## + x2 1 0.30171 2.0210 -39.843 ## &lt;none&gt; 2.3227 -39.060 ## - x3 1 2.41534 4.7380 -26.802 ## ## Step: AIC=-42.8 ## y ~ x3 + x1 ## ## Df Sum of Sq RSS AIC ## + x2 1 0.44098 1.3025 -46.629 ## &lt;none&gt; 1.7435 -42.797 ## - x1 1 0.57921 2.3227 -39.060 ## - x3 1 0.68561 2.4291 -38.164 ## ## Step: AIC=-46.63 ## y ~ x3 + x1 + x2 ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.3025 -46.629 ## - x3 1 0.24029 1.5428 -45.243 ## - x2 1 0.44098 1.7435 -42.797 ## - x1 1 0.71848 2.0210 -39.843 ## ## Call: ## lm(formula = y ~ x3 + x1 + x2) ## ## Coefficients: ## (Intercept) x3 x1 x2 ## -4.71489 0.04924 0.06091 0.03563 结果解读 1.1 Start: AIC=-26.8; y ~ 1: 空载模型 Step: AIC=-39.06;y ~ x3:剔除x3后，AIC变小，说明X3应该在方程内 Step: AIC=-46.63;y ~ x3 + x1 + x2: 直至最后一步，AIC仍在变小，说明x1、x2、x3均应纳入回归方程。 若有一步剔除某个变量后，AIC值没有变化或者变大，则说明该变量应该从回归方程中剔除。 summary (fit1) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3749 -0.2747 0.1042 0.1820 0.4277 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.71489 1.30082 -3.625 0.00228 ** ## x1 0.06091 0.02050 2.971 0.00901 ** ## x2 0.03563 0.01531 2.327 0.03339 * ## x3 0.04924 0.02866 1.718 0.10507 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2853 on 16 degrees of freedom ## Multiple R-squared: 0.7251, Adjusted R-squared: 0.6736 ## F-statistic: 14.07 on 3 and 16 DF, p-value: 9.464e-05 y #观察值 ## [1] 2.99 3.11 1.91 2.63 2.86 1.91 2.98 3.28 2.52 3.27 3.10 3.28 1.92 3.27 2.64 ## [16] 2.85 3.16 2.51 3.15 1.92 fitted (fit1) #预测值 ## 1 2 3 4 5 6 7 8 ## 2.824232 2.926867 2.208092 2.908425 2.432268 2.284897 2.838810 3.212696 ## 9 10 11 12 13 14 15 16 ## 2.338328 3.227273 2.912290 3.509504 2.193514 3.504774 2.923002 2.432461 ## 17 18 19 20 ## 3.002704 2.311569 2.988127 2.280167 residuals (fit1) #残差 ## 1 2 3 4 5 6 ## 0.16576766 0.18313311 -0.29809186 -0.27842491 0.42773214 -0.37489727 ## 7 8 9 10 11 12 ## 0.14119028 0.06730433 0.18167211 0.04272695 0.18771049 -0.22950412 ## 13 14 15 16 17 18 ## -0.27351448 -0.23477387 -0.28300229 0.41753882 0.15729565 0.19843126 ## 19 20 ## 0.16187303 -0.36016703 fit3&lt;-lm(y~ x1+x2)#拟合一个不包含x3的模型 anova(fit1,fit2)#比较含有x3和不含x3模型的差别 ## Analysis of Variance Table ## ## Model 1: y ~ x1 + x2 + x3 ## Model 2: y ~ 1 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 16 1.3025 ## 2 19 4.7380 -3 -3.4355 14.067 9.464e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 detach (Example10_1) 结果解读 1.2 事实证明，对于是否含有x3，对模型来说意义不大 7.5 案例二 例 10-2 一家皮鞋零售店将其连续18 个月的库存占用资金情况、广告投入的费用、员工薪酬以及销售额等方面的数据作了一个汇总， 如表10-2 所示。该皮鞋店老板试图根据这些数据找到销售额与其他3 个变量之间的关系，以便进行销售额预测并为未来的预算工作提供参考。根据这些数据建立回归模型 # 第三节多元线性回归案例2 Example10_2 &lt;- read.table (&quot;data/example10_2.csv&quot;, header=TRUE, sep=&quot;,&quot;) library(MASS) attach(Example10_2) fit1 &lt;- lm(Y~ X1 + X2 + X3) fit2 &lt;- lm(Y ~ 1)#空模型 stepAIC(fit2,direction=&quot;both&quot;,scope=list(upper=fit1,lower=fit2)) ## Start: AIC=220.24 ## Y ~ 1 ## ## Df Sum of Sq RSS AIC ## + X1 1 2955167 363111 182.42 ## + X2 1 2778824 539453 189.54 ## + X3 1 2354756 963521 199.98 ## &lt;none&gt; 3318277 220.24 ## ## Step: AIC=182.42 ## Y ~ X1 ## ## Df Sum of Sq RSS AIC ## + X2 1 221519 141591 167.47 ## &lt;none&gt; 363111 182.42 ## + X3 1 26278 336833 183.06 ## - X1 1 2955167 3318277 220.24 ## ## Step: AIC=167.47 ## Y ~ X1 + X2 ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 141591 167.47 ## + X3 1 500 141092 169.40 ## - X2 1 221519 363111 182.42 ## - X1 1 397862 539453 189.54 ## ## Call: ## lm(formula = Y ~ X1 + X2) ## ## Coefficients: ## (Intercept) X1 X2 ## 86.953 7.109 13.684 结果解读 2.1 在排除x3后,模型的AIC值不降反增(167.47→169.40)，故x3应从模型中剔除。 Step: AIC=167.47 Y ~ X1 + X2 Df Sum of Sq RSS AIC &lt;none&gt; 141591 167.47 + X3 1 500 141092 169.40 lm(formula = Y ~ X1 + X2)：最终纳入模型的变量为x1、x2。 fit &lt;- lm(Y~ X1 + X2)#拟合一个只纳入两个变量的模型 anova(fit) ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## X1 1 2955167 2955167 313.066 1.847e-11 *** ## X2 1 221519 221519 23.467 0.0002144 *** ## Residuals 15 141591 9439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary (fit) ## ## Call: ## lm(formula = Y ~ X1 + X2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -172.812 -50.486 -5.579 55.188 186.366 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 86.953 75.117 1.158 0.265141 ## X1 7.109 1.095 6.492 1.02e-05 *** ## X2 13.684 2.825 4.844 0.000214 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 97.16 on 15 degrees of freedom ## Multiple R-squared: 0.9573, Adjusted R-squared: 0.9516 ## F-statistic: 168.3 on 2 and 15 DF, p-value: 5.32e-11 detach (Example10_2) 结果解读 2.2 在排除x3后,构建的只含有x1、x2的新模型中，x1、x2的P值均小于0.05，说明模型的x1、x2均有统计学意义 最终模型的R方值为0.957，说明因变量有95.7%是由回归解释的，决定系数0.951，证明剔除X3后，模型的拟合效果良好。 7.6 用线性回归来做ANOVA ANOVA和回归都是广义线性模型的特例。因此，本章所有的设计都可以用 lm()函数来分析。但是，为了更好地理解输出结果，需要弄明白在拟合模型时，R是如何处理类别型变量的。 "],["诊断试验的roc分析.html", "Chapter 8 诊断试验的ROC分析 8.1 诊断试验设计原理 8.2 诊断试验的统计指标 8.3 ROC分析的R语言实现", " Chapter 8 诊断试验的ROC分析 8.1 诊断试验设计原理 8.1.1 队列研究设计 8.1.2 横断面设计/单门设计 8.1.3 病例对照设计/双门设计 8.2 诊断试验的统计指标 8.2.1 基本概念 \\(敏感性=a/(a+c)\\) \\(特异性=d/(b+d)\\) 阳性预测值\\(PPV=a/(a+b)\\) 阴性预测值\\(NPV=d/(c+d)\\) 阳性似然比\\(PLR=\\cfrac{a/(a+c)}{b/(b+d)}\\) ——真阳性与假阳性之比 阴性似然比\\(NLR=\\cfrac{d/(b+d)}{c/(a+c)}\\) ——真阴性与假阴性之比 \\(准确率=(a+d)/(N)\\) 8.2.2 灵敏度和特异度之间的权衡 ➢ 减少漏诊必然会增加误诊 ➢ 减少误诊必然会增加漏诊 ➢ 提高敏感性必然降低特异性 ➢ 提高特异性必然降低敏感性 【举例】AFP和DCP诊断肝癌 ➢ 当AFP的诊断界值设定为15ng/ml时，诊断肝癌的敏感性为70%，特异性为80%。 ➢ 当DCP的诊断界值设定为20ng/ml时，诊断肝癌的敏感性为80%，特异性为70%。 ➢ 问题：AFP和DCP诊断肝癌的能力孰优孰劣呢？ 8.2.3 受试者工作特征曲线 (ROC) ➢ 曲线本质是不同界值下敏感性和特异性的组合；ROC曲线下面积(AUC) 介于0.5-1.0之间，AUC的大小反映试验诊断疾病的能力 ➢ ROC反映多个诊断界点下诊断能力的总和；便于比较两个试验的诊断能力；确定最佳诊断界值。 8.3 ROC分析的R语言实现 8.3.1 案例1 example21_2 &lt;- read.table (&quot;data/example21_2.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(example21_2) head(example21_2) ## group value ## 1 1 6.5 ## 2 1 13.5 ## 3 1 12.8 ## 4 1 6.2 ## 5 1 13.9 ## 6 1 14.7 summary(example21_2) ## group value ## Min. :0.0000 Min. : 1.700 ## 1st Qu.:0.0000 1st Qu.: 5.400 ## Median :1.0000 Median : 8.500 ## Mean :0.5556 Mean : 8.847 ## 3rd Qu.:1.0000 3rd Qu.:12.400 ## Max. :1.0000 Max. :16.800 str(example21_2) #structure of dataframe ## &#39;data.frame&#39;: 45 obs. of 2 variables: ## $ group: int 1 1 1 1 1 1 1 1 1 1 ... ## $ value: num 6.5 13.5 12.8 6.2 13.9 14.7 9.5 9 6.9 16.8 ... 我们可以看到，在使用R实现ROC分析时，需要的数据是只分成2列，用不同的数字标记方法的类型，这与SPSS中做ROC分析是完全不同的。   首先，我们构建一个逻辑回归方程 library(ROCR) pred &lt;- prediction(example21_2$value, example21_2$group) pred ## A prediction instance ## with 45 data points 接下来，我们用performance()函数来绘制一个ROC曲线 perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf,col=&quot;red&quot;) abline(a=0,b=1,col=&quot;blue&quot;) 下面是一些非经典的ROC曲线 perf1 &lt;- performance(pred, &quot;prec&quot;, &quot;rec&quot;) plot(perf1) perf2 &lt;- performance(pred, &quot;sens&quot;, &quot;spec&quot;) plot(perf2) detach(example21_2) 8.3.2 案例2 例21-3 假设某研究欲比较HPV-DNA与HPV E6/E7 mRNA对中重度子宫颈上皮不典型增生的诊断价值。试进行ROC分析。 oncology mRNA dna 1 1.720000 0.23 1 3.238792 1.25 1 8.206722 2.53 1 16.596091 2.62 1 125.790742 4.29 1 36.029169 4.40 example21_3 &lt;- read.table (&quot;data/example21_3.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(example21_3) ## The following objects are masked from example21_3 (pos = 3): ## ## dna, mRNA, oncology summary(example21_3) ## oncology mRNA dna ## Min. :0.0000 Min. : 0.0877 Min. : 0.09 ## 1st Qu.:0.0000 1st Qu.: 0.7090 1st Qu.: 0.81 ## Median :0.0000 Median : 1.4830 Median : 15.87 ## Mean :0.3126 Mean : 49.8652 Mean : 470.19 ## 3rd Qu.:1.0000 3rd Qu.: 12.9951 3rd Qu.: 200.21 ## Max. :1.0000 Max. :2128.6922 Max. :118231.00 str(example21_3)#查看数据结构 ## &#39;data.frame&#39;: 563 obs. of 3 variables: ## $ oncology: int 1 1 1 1 1 1 1 1 1 1 ... ## $ mRNA : num 1.72 3.24 8.21 16.6 125.79 ... ## $ dna : num 0.23 1.25 2.53 2.62 4.29 4.4 4.71 4.77 5.21 5.48 ... #install.packages(&quot;ROCR&quot;) library(ROCR) pred1 &lt;- prediction(example21_3$mRNA, example21_3$oncology) pred1 ## A prediction instance ## with 563 data points pred2 &lt;- prediction(example21_3$dna, example21_3$oncology) pred2 ## A prediction instance ## with 563 data points perf_1 &lt;- performance(pred1,&quot;tpr&quot;,&quot;fpr&quot;) perf_2 &lt;- performance(pred2,&quot;tpr&quot;,&quot;fpr&quot;) auc1 &lt;- performance(pred1,&quot;auc&quot;) auc1 ## A performance instance ## &#39;Area under the ROC curve&#39; auc2 &lt;- performance(pred2,&quot;auc&quot;) auc2 ## A performance instance ## &#39;Area under the ROC curve&#39; plot(perf_1, col=&quot;blue&quot;) plot(perf_2, col=&quot;red&quot;, add=TRUE) abline(a=0,b=1,col=&quot;gray&quot;) legend(locator(n=1),legend=c(&quot;mRNA&quot;,&quot;dna&quot;),lty=1,col=c(&quot;blue&quot;,&quot;red&quot;)) detach(example21_3) "],["logistic回归.html", "Chapter 9 Logistic回归 9.1 模型概述 9.2 模型结构 9.3 比值比或优势比（OR） 9.4 案例1 9.5 案例2 9.6 案例 3 9.7 案例4 9.8 扩展的Logistic回归及其变种", " Chapter 9 Logistic回归 9.1 模型概述 Logistic回归模型是一种概率模型，它是以某一事件发生与否的概率P为因变量，以影响P的因素为自变量建立的回归模型，分析某事件发生的概率与自变量之间的关系，是一种非线性回归模型。 Logistic回归模型适用的资料类型：适用于因变量为二项或多项分类（有序、无序）的资料 Logistic回归模型分类 条件Logistic 回归模型：适合于配对或配伍设计资料； 非条件Logistic回归模型：适合于成组设计的统计资料 因变量可以是：两项分类、无序多项分类、有序多项分类等 9.2 模型结构 Logistic分布函数表达：\\(F(y)=\\cfrac{e^y}{1+e^y}\\) y的取值在\\(-\\infty ~ +\\infty\\)之间，函数值F(y)在0~1之间取值且呈单调上升的S型曲线。可以将这一特征运用到临床医学和流行病学中描述事件发生的概率与影响因素的关系 logit(P)变换: \\[ \\log it(P)=\\ln (\\cfrac{P}{Q}) \\] \\[ \\log it(P)=\\beta_0+\\beta x \\] \\[ \\log it(P)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots++\\beta_px_p \\] 其实变换之后就是所谓的线性回归方程，ln(P/Q)称为logit(P)变换，P/Q称为事件的优势，流行病学中称为比值（odds） 。因此，优势的对数值与影响因素之间呈线性关系 9.3 比值比或优势比（OR） 暴露组的优势（比值）与非暴露组的优势（比值）之比 OR用于说明暴露某因素引起疾病或死亡的危险度大小 β指回归系数 9.4 案例1 ——结局是二分类变量最常用 以AER包中的数据框Affairs为例，我们将通过探究婚外情的数据来阐述Logistic回归分析的过程。首次使用该数据前，请确保已下载和安装了此软件,使用install.packages(\"AER\") 婚外情数据取自1969年（Psychology Today）所做一个非常有代表性的调查。该数据从601个参与者收集了9个变量，包括一年来婚外私通的频率以及参与者性别、年龄、婚龄、是否有小孩、宗教信仰程度（5分制，1分表示反对，5分表示非常信仰）、学历、职业（逆向编号的戈登7种分类），还有对婚姻自我评分（5分制，1表示非常不幸福，5表示非常幸福）。 # get summary statistics data(Affairs, package=&quot;AER&quot;) summary(Affairs) ## affairs gender age yearsmarried children ## Min. : 0.000 female:315 Min. :17.50 Min. : 0.125 no :171 ## 1st Qu.: 0.000 male :286 1st Qu.:27.00 1st Qu.: 4.000 yes:430 ## Median : 0.000 Median :32.00 Median : 7.000 ## Mean : 1.456 Mean :32.49 Mean : 8.178 ## 3rd Qu.: 0.000 3rd Qu.:37.00 3rd Qu.:15.000 ## Max. :12.000 Max. :57.00 Max. :15.000 ## religiousness education occupation rating ## Min. :1.000 Min. : 9.00 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:14.00 1st Qu.:3.000 1st Qu.:3.000 ## Median :3.000 Median :16.00 Median :5.000 Median :4.000 ## Mean :3.116 Mean :16.17 Mean :4.195 Mean :3.932 ## 3rd Qu.:4.000 3rd Qu.:18.00 3rd Qu.:6.000 3rd Qu.:5.000 ## Max. :5.000 Max. :20.00 Max. :7.000 Max. :5.000 table(Affairs$affairs) ## ## 0 1 2 3 7 12 ## 451 34 17 19 42 38 # create binary outcome variable Affairs$ynaffair[Affairs$affairs &gt; 0] &lt;- 1 Affairs$ynaffair[Affairs$affairs == 0] &lt;- 0 Affairs$ynaffair &lt;- factor(Affairs$ynaffair, levels=c(0,1), labels=c(&quot;No&quot;,&quot;Yes&quot;)) table(Affairs$ynaffair) ## ## No Yes ## 451 150 # fit full model fit.full &lt;- glm(ynaffair ~ gender + age + yearsmarried + children + religiousness + education + occupation +rating, data=Affairs,family=binomial()) summary(fit.full) ## ## Call: ## glm(formula = ynaffair ~ gender + age + yearsmarried + children + ## religiousness + education + occupation + rating, family = binomial(), ## data = Affairs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5713 -0.7499 -0.5690 -0.2539 2.5191 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.37726 0.88776 1.551 0.120807 ## gendermale 0.28029 0.23909 1.172 0.241083 ## age -0.04426 0.01825 -2.425 0.015301 * ## yearsmarried 0.09477 0.03221 2.942 0.003262 ** ## childrenyes 0.39767 0.29151 1.364 0.172508 ## religiousness -0.32472 0.08975 -3.618 0.000297 *** ## education 0.02105 0.05051 0.417 0.676851 ## occupation 0.03092 0.07178 0.431 0.666630 ## rating -0.46845 0.09091 -5.153 2.56e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 675.38 on 600 degrees of freedom ## Residual deviance: 609.51 on 592 degrees of freedom ## AIC: 627.51 ## ## Number of Fisher Scoring iterations: 4 结果解读 1.1 从回归系数的值（最后一栏）可以看到，性别、是否有孩子、学历和职业对方程的贡献都不显著（你无法拒绝参数为0 的假设）去除这些变量重新拟合模型，检验新模型是否拟合得好： # fit reduced model fit.reduced &lt;- glm(ynaffair ~ age + yearsmarried + religiousness + rating, data=Affairs, family=binomial()) summary(fit.reduced) ## ## Call: ## glm(formula = ynaffair ~ age + yearsmarried + religiousness + ## rating, family = binomial(), data = Affairs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6278 -0.7550 -0.5701 -0.2624 2.3998 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.93083 0.61032 3.164 0.001558 ** ## age -0.03527 0.01736 -2.032 0.042127 * ## yearsmarried 0.10062 0.02921 3.445 0.000571 *** ## religiousness -0.32902 0.08945 -3.678 0.000235 *** ## rating -0.46136 0.08884 -5.193 2.06e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 675.38 on 600 degrees of freedom ## Residual deviance: 615.36 on 596 degrees of freedom ## AIC: 625.36 ## ## Number of Fisher Scoring iterations: 4 结果解读 1.2 新模型的每个回归系数都非常显著 (p&lt;0.05) 由于两模型嵌套 (fit.reduced是fit.full 的一个子集)，你可以使用anova()函数对它们进行比较，对于广义线性回归 ，可用卡方检验 # compare models anova(fit.reduced, fit.full, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: ynaffair ~ age + yearsmarried + religiousness + rating ## Model 2: ynaffair ~ gender + age + yearsmarried + children + religiousness + ## education + occupation + rating ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 596 615.36 ## 2 592 609.51 4 5.8474 0.2108 结果解读 1.3 结果的卡方值不显著 (p=0.2 ),表明四个预测变量的新模型与九个完整预测变量的模型拟合程度一样好，这使得你更加坚信添加性别孩子、学历和职业变量不会显著提高方程的预测精度，因此可以依据更简单的模型进行解释 # interpret coefficients coef(fit.reduced) ## (Intercept) age yearsmarried religiousness rating ## 1.93083017 -0.03527112 0.10062274 -0.32902386 -0.46136144 结果解读 1.4 先看看回归系数：Logistic 回归中，响应变量是Y=1的对数优势比 (log) 回归系数的含义是当其他预测变量不变时．一单位预测变量的变化可引起的响应变量对数优势比的变化。 由于对数优势比解释性差，你可对结果进行指数化 exp(coef(fit.reduced)) #取反对数 ## (Intercept) age yearsmarried religiousness rating ## 6.8952321 0.9653437 1.1058594 0.7196258 0.6304248 结果解读 1.5 可以看到婚龄增加一年，婚外情的优势比将乘以1.106(保持年龄、宗教信仰和婚姻评定不变) 相反年龄增加一岁，婚外清的的优势比则乘以0.965 此外，随着婚龄的增加和年龄、宗教信仰与婚姻评分的降低，婚外情优势比将上升。因为预测变量不能等于0, 截距项在此处没有什么特定含义 你还可使用confint()函数获取系数的置信区间。例如，exp(confint(fit,reduced))可在优势比尺度上得到系数95%的置信区间. 结果解读 1.6 预测变量一单位的变化可能并不是我们最想关注的 对于二值型Logistic回归，某预测变量n单位的变化引起的较高值上优势比的变化为\\(\\exp (\\beta_j)^n\\)，它反映的信息可能更为重要 比如，保持其他预测变量不变，婚龄增加一年，婚外情的优势比将乘以1.106，而如果婚龄增加10年，优势比将乘以\\(1.106^{10}\\)，即2.7。 9.4.1 评价预测变量对结果概率的影响 对于我们大多数人来说，以概率的方式思考比使用优势比更直观 使用predict()函数，可以观察某个预测变量在各个水平时对结果概率的影响 首先创建一个包含你感兴趣预测变量值的虚拟数据集，然后对该数据集使用predict()函数，以预测这些值的结果概率。 现在我们使用该方法评价婚姻评分对婚外情概率的影响 首先， 建一个虚拟数据集，设定年龄、婚龄和宗教信仰为它们的均值，婚姻评分的范围为1~5 # calculate probability of extramariatal affair by marital ratings testdata &lt;- data.frame(rating = c(1, 2, 3, 4, 5), age = mean(Affairs$age), yearsmarried = mean(Affairs$yearsmarried), religiousness = mean(Affairs$religiousness)) testdata ## rating age yearsmarried religiousness ## 1 1 32.48752 8.177696 3.116473 ## 2 2 32.48752 8.177696 3.116473 ## 3 3 32.48752 8.177696 3.116473 ## 4 4 32.48752 8.177696 3.116473 ## 5 5 32.48752 8.177696 3.116473 接下来，使用测试数据集预测相应的概率 testdata$prob &lt;- predict(fit.reduced, newdata=testdata, type=&quot;response&quot;) testdata ## rating age yearsmarried religiousness prob ## 1 1 32.48752 8.177696 3.116473 0.5302296 ## 2 2 32.48752 8.177696 3.116473 0.4157377 ## 3 3 32.48752 8.177696 3.116473 0.3096712 ## 4 4 32.48752 8.177696 3.116473 0.2204547 ## 5 5 32.48752 8.177696 3.116473 0.1513079 结果解读 1.7 从这些结果可以看到，当婚姻评分从(很不幸福)变为(非常幸福)时，婚外情概率从0.53降低到了 15 (假定年龄、婚龄和宗教信仰不变) 下面我们再看看年龄的影响 # calculate probabilites of extramariatal affair by age testdata &lt;- data.frame(rating = mean(Affairs$rating), age = seq(17, 57, 10), yearsmarried = mean(Affairs$yearsmarried), religiousness = mean(Affairs$religiousness)) testdata$prob &lt;- predict(fit.reduced, newdata=testdata, type=&quot;response&quot;) testdata ## rating age yearsmarried religiousness prob ## 1 3.93178 17 8.177696 3.116473 0.3350834 ## 2 3.93178 27 8.177696 3.116473 0.2615373 ## 3 3.93178 37 8.177696 3.116473 0.1992953 ## 4 3.93178 47 8.177696 3.116473 0.1488796 ## 5 3.93178 57 8.177696 3.116473 0.1094738 结果解读 1.8 此处可以看到，当其他变量不变，年龄从 17 增加到57 时，婚外情的概率将从0.34降低到 0.11，利用该方法，你可探究每一个预测变量对结果概率的影响 在婚外情的例子中，婚外偷腥的次数被二值化为一个“是/否”的响应变量，这是因为我们最感兴趣的是在过去一年中调查对象是否有过一次婚外情。如果兴趣转移到量上（过去一年中婚外情的次数），便可直接对计数型数据进行分析。分析计数型数据的一种流行方法是泊松回归。 9.5 案例2 表11-5 是一个研究吸烟、饮酒与食管癌关系的病例-对照研究资料，试作Logistic 回归分析。设y=1 表示患有食管癌， y=0表示未患食管癌。令x1=1 表示吸烟，x1=0 表示不吸烟: x2=1表示饮酒，x2=0 表示不饮酒 # Logistic回归案例2 Example11_4 &lt;- read.table (&quot;data/example11_4.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example11_4) fit1 &lt;- glm(y~ x1 + x2, family= binomial(), data=Example11_4) summary(fit1) ## ## Call: ## glm(formula = y ~ x1 + x2, family = binomial(), data = Example11_4) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3965 -1.0193 -0.8225 0.9730 1.5800 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.9099 0.1358 -6.699 2.11e-11 *** ## x1 0.8856 0.1500 5.904 3.54e-09 *** ## x2 0.5261 0.1572 3.348 0.000815 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1228.0 on 885 degrees of freedom ## Residual deviance: 1159.4 on 883 degrees of freedom ## AIC: 1165.4 ## ## Number of Fisher Scoring iterations: 4 coefficients(fit1) ## (Intercept) x1 x2 ## -0.9099467 0.8855837 0.5261234 exp(coefficients(fit1)) #计算OR值 ## (Intercept) x1 x2 ## 0.4025457 2.4243991 1.6923589 exp (confint(fit1)) #计算95%CI ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.3071678 0.5234695 ## x1 1.8090525 3.2580064 ## x2 1.2441606 2.3048818 结果解读 2.1 吸烟的人患食管癌的概率是不吸烟的2.4243991倍，其95%CI是1.8090525~3.2580064 饮酒的人患食管癌的概率是不饮酒的1.6923589倍，其95%CI是1.2441606~2.3048818 9.5.1 因素间的交互作用 本例分析吸烟、饮酒危险因素对患食管癌的影响程度以及它们的交互影响程度。设y=1 表示患有食管癌， y=0 表示未患食管癌。 令x1=1 表示吸烟， x1=0 表示不吸烟: x2=1表示饮酒， X2=0 表示不饮酒。这样， x1和x2的交叉水平有4 个，建立4 个哑变量分别代表这4 个水平，记为x11、x10 、x01、x00，它们表示4 种不同的生活方式，即x11表示既吸烟又饮酒， X10表示吸烟但不饮酒， x01 表示不吸烟但饮酒， x00表示既不吸烟又不饮酒。将前3 个哑变量放进模型， 则可得到前3 种生活方式相对于最后一种生活方式患食管癌的相对危险度。 fit2 &lt;- glm(y~ x1 + x2 + x1:x2 , family= binomial(), data=Example11_4) summary(fit2) ## ## Call: ## glm(formula = y ~ x1 + x2 + x1:x2, family = binomial(), data = Example11_4) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4237 -0.9623 -0.8725 0.9497 1.5167 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7695 0.1524 -5.049 4.43e-07 *** ## x1 0.5107 0.2520 2.027 0.0427 * ## x2 0.2398 0.2201 1.090 0.2759 ## x1:x2 0.5815 0.3148 1.847 0.0647 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1228 on 885 degrees of freedom ## Residual deviance: 1156 on 882 degrees of freedom ## AIC: 1164 ## ## Number of Fisher Scoring iterations: 4 coefficients(fit2) ## (Intercept) x1 x2 x1:x2 ## -0.7695202 0.5106585 0.2398261 0.5814856 exp(coefficients(fit2)) ## (Intercept) x1 x2 x1:x2 ## 0.4632353 1.6663882 1.2710280 1.7886937 exp (confint(fit2)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.3415446 0.6214419 ## x1 1.0159204 2.7326277 ## x2 0.8255781 1.9585429 ## x1:x2 0.9658438 3.3214114 结果解读 2.2 由此可见，X1与X2交互项的P值为0.0647，不能很好的说明二者是否有交互，故而需要设置哑变量。 #设置哑变量 Example11_4$x11 &lt;- ifelse (x1==1 &amp; x2==1, 1, 0) Example11_4$x10 &lt;- ifelse (x1==1 &amp; x2==0, 1, 0) Example11_4$x01 &lt;- ifelse (x1==0 &amp; x2==1, 1, 0) Example11_4$x00 &lt;- ifelse (x1==0 &amp; x2==0, 1, 0) #拟合新模型 fit3 &lt;- glm(y~ x11 + x10 + x01, family= binomial(), data=Example11_4) summary(fit3) ## ## Call: ## glm(formula = y ~ x11 + x10 + x01, family = binomial(), data = Example11_4) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4237 -0.9623 -0.8725 0.9497 1.5167 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7695 0.1524 -5.049 4.43e-07 *** ## x11 1.3320 0.1834 7.264 3.76e-13 *** ## x10 0.5107 0.2520 2.027 0.0427 * ## x01 0.2398 0.2201 1.090 0.2759 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1228 on 885 degrees of freedom ## Residual deviance: 1156 on 882 degrees of freedom ## AIC: 1164 ## ## Number of Fisher Scoring iterations: 4 结果解读 2.3 结果中X11、X10和X00相比，均有统计学意义。但是X01就无统计学意义了。 coefficients(fit3) ## (Intercept) x11 x10 x01 ## -0.7695202 1.3319701 0.5106585 0.2398261 exp(coefficients(fit3)) ## (Intercept) x11 x10 x01 ## 0.4632353 3.7884999 1.6663882 1.2710280 exp(confint(fit3)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.3415446 0.6214419 ## x11 2.6552919 5.4525611 ## x10 1.0159204 2.7326277 ## x01 0.8255781 1.9585429 detach (Example11_4) 结果解读 2.4 结果表明：既吸烟又饮酒(x11)的人患食管癌的概率是不吸烟也不饮酒的人的3.7884999倍；吸烟不饮酒(x10)的人患食管癌的概率是不吸烟也不饮酒的人的1.6663882倍。 9.6 案例 3 例11-5 为了探讨冠心病发生的有关危险因素， 对26例冠心病病人和28例对照者进行病例-对照研究，各因素的说明及资料见表1和表2。试用Logistic 逐步回归分析方法筛选危险因素。 # Logistic回归案例3 Example11_5 &lt;- read.table (&quot;data/example11_5.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example11_5) fullfit &lt;- glm(y~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 , family= binomial(), data=Example11_5) summary(fullfit) ## ## Call: ## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), ## data = Example11_5) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3567 -0.4949 -0.1643 0.6351 2.3456 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.8896 1.9721 -2.986 0.00282 ** ## x1 0.6443 0.4987 1.292 0.19642 ## x2 0.9098 0.8362 1.088 0.27662 ## x3 0.9698 0.9058 1.071 0.28433 ## x4 0.9948 1.2095 0.823 0.41079 ## x5 0.7409 0.8801 0.842 0.39988 ## x6 3.4559 1.4152 2.442 0.01461 * ## x7 0.3019 0.5906 0.511 0.60917 ## x8 1.9169 0.9189 2.086 0.03697 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 74.786 on 53 degrees of freedom ## Residual deviance: 42.194 on 45 degrees of freedom ## AIC: 60.194 ## ## Number of Fisher Scoring iterations: 6 nothing &lt;- glm(y~ 1, family= binomial(), data=Example11_5) #空模型 summary(nothing) ## ## Call: ## glm(formula = y ~ 1, family = binomial(), data = Example11_5) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.146 -1.146 -1.146 1.209 1.209 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.07411 0.27235 -0.272 0.786 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 74.786 on 53 degrees of freedom ## Residual deviance: 74.786 on 53 degrees of freedom ## AIC: 76.786 ## ## Number of Fisher Scoring iterations: 3 bothways &lt;- step(nothing, list(lower=formula(nothing), upper=formula(fullfit)), direction=&quot;both&quot;) ## Start: AIC=76.79 ## y ~ 1 ## ## Df Deviance AIC ## + x6 1 63.467 67.467 ## + x5 1 67.137 71.137 ## + x8 1 67.727 71.727 ## + x2 1 68.705 72.705 ## + x1 1 68.706 72.706 ## + x7 1 69.335 73.335 ## + x3 1 69.966 73.966 ## + x4 1 70.272 74.272 ## &lt;none&gt; 74.786 76.786 ## ## Step: AIC=67.47 ## y ~ x6 ## ## Df Deviance AIC ## + x5 1 55.480 61.480 ## + x8 1 56.751 62.751 ## + x2 1 56.890 62.890 ## + x3 1 58.265 64.265 ## + x1 1 58.335 64.335 ## + x7 1 58.753 64.753 ## + x4 1 60.617 66.617 ## &lt;none&gt; 63.467 67.467 ## - x6 1 74.786 76.786 ## ## Step: AIC=61.48 ## y ~ x6 + x5 ## ## Df Deviance AIC ## + x8 1 50.402 58.402 ## + x2 1 52.108 60.108 ## + x1 1 52.454 60.454 ## &lt;none&gt; 55.480 61.480 ## + x7 1 53.709 61.709 ## + x4 1 54.124 62.124 ## + x3 1 54.174 62.174 ## - x5 1 63.467 67.467 ## - x6 1 67.137 71.137 ## ## Step: AIC=58.4 ## y ~ x6 + x5 + x8 ## ## Df Deviance AIC ## + x1 1 46.224 56.224 ## + x2 1 46.717 56.717 ## &lt;none&gt; 50.402 58.402 ## + x3 1 48.601 58.601 ## + x4 1 49.039 59.039 ## + x7 1 49.702 59.702 ## - x8 1 55.480 61.480 ## - x5 1 56.751 62.751 ## - x6 1 61.148 67.148 ## ## Step: AIC=56.22 ## y ~ x6 + x5 + x8 + x1 ## ## Df Deviance AIC ## + x2 1 44.163 56.163 ## &lt;none&gt; 46.224 56.224 ## + x3 1 44.534 56.534 ## + x4 1 45.755 57.755 ## + x7 1 45.781 57.781 ## - x1 1 50.402 58.402 ## - x5 1 50.491 58.491 ## - x8 1 52.454 60.454 ## - x6 1 56.439 64.439 ## ## Step: AIC=56.16 ## y ~ x6 + x5 + x8 + x1 + x2 ## ## Df Deviance AIC ## &lt;none&gt; 44.163 56.163 ## - x2 1 46.224 56.224 ## - x1 1 46.717 56.717 ## + x3 1 42.999 56.999 ## - x5 1 47.099 57.099 ## + x4 1 43.817 57.817 ## + x7 1 43.907 57.907 ## - x8 1 50.405 60.405 ## - x6 1 54.874 64.874 #从nothing开始，到fullfit终止 fit1 &lt;- glm(y~ x6 + x5 + x8 + x1 + x2 , family= binomial(), data=Example11_5) summary(fit1) ## ## Call: ## glm(formula = y ~ x6 + x5 + x8 + x1 + x2, family = binomial(), ## data = Example11_5) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4100 -0.6282 -0.2313 0.6034 2.2802 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.7603 1.5803 -3.012 0.00259 ** ## x6 3.3698 1.3311 2.532 0.01136 * ## x5 1.2928 0.7654 1.689 0.09122 . ## x8 2.0004 0.8730 2.291 0.02194 * ## x1 0.7460 0.4841 1.541 0.12335 ## x2 1.1453 0.8077 1.418 0.15620 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 74.786 on 53 degrees of freedom ## Residual deviance: 44.163 on 48 degrees of freedom ## AIC: 56.163 ## ## Number of Fisher Scoring iterations: 5 fit2 &lt;- glm(y~ x6 + x5 + x8 + x1, family= binomial(), data=Example11_5) summary(fit2) ## ## Call: ## glm(formula = y ~ x6 + x5 + x8 + x1, family = binomial(), data = Example11_5) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5264 -0.5445 -0.2733 0.6319 2.0339 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.7050 1.5432 -3.049 0.0023 ** ## x6 3.1355 1.2489 2.511 0.0121 * ## x5 1.4959 0.7439 2.011 0.0443 * ## x8 1.9471 0.8466 2.300 0.0215 * ## x1 0.9239 0.4766 1.939 0.0525 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 74.786 on 53 degrees of freedom ## Residual deviance: 46.224 on 49 degrees of freedom ## AIC: 56.224 ## ## Number of Fisher Scoring iterations: 5 coefficients(fit2) ## (Intercept) x6 x5 x8 x1 ## -4.7050170 3.1354895 1.4959351 1.9470861 0.9238998 exp(coefficients(fit2)) ## (Intercept) x6 x5 x8 x1 ## 0.009049761 22.999891063 4.463508324 7.008236568 2.519095102 exp (confint(fit2)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.0002770793 0.1345707 ## x6 2.9251702716 552.8160472 ## x5 1.0786693022 20.8559589 ## x8 1.4921555369 44.9031777 ## x1 1.0375623088 7.0051815 detach (Example11_5) 9.7 案例4 ——条件logistic回归 某研究机构为了研究胃癌与饮酒的相关关系，收集了病例对照资料如表1 1-9 所示，其中D和D’分别表示患有胃癌和未患有胃癌,E和E’分别表示饮酒和不饮酒。试用条件Logistic回归模型分析饮酒对胃癌的影响。 #条件Logistic回归案例 4 #install.packages(&quot;survival&quot;) library(survival) Example11_6 &lt;- read.table (&quot;data/example11_6.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(Example11_6) #拟合模型 model &lt;- clogit(outcome~ exposure+ strata(id)) # strata(id)：指定对子号 summary(model) ## Call: ## coxph(formula = Surv(rep(1, 168L), outcome) ~ exposure + strata(id), ## method = &quot;exact&quot;) ## ## n= 168, number of events= 84 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## exposure 1.030 2.800 0.521 1.976 0.0481 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## exposure 2.8 0.3571 1.009 7.774 ## ## Concordance= 0.554 (se = 0.036 ) ## Likelihood ratio test= 4.44 on 1 df, p=0.04 ## Wald test = 3.91 on 1 df, p=0.05 ## Score (logrank) test = 4.26 on 1 df, p=0.04 detach(Example11_6) 结果解读 4 使用survival包中的clogit()函数进行计算 在结果中可以看到，模型的回归系数coef是1.03；回归系数的标准误se(coef)是0.521；P值是0.0481。 故饮酒对胃癌的OR值即exp(coef)=2.8；可以理解为，饮酒的人患胃癌的概率是不饮酒的人的2.8倍。 9.8 扩展的Logistic回归及其变种 9.8.1 稳健Logistic回归 robust包中的glmRob()函数可用来拟合稳健的广义线性模型，包括稳健Logistic回归。当拟合Logistic回归模型数据出现离群点和强影响点时，稳健Logistic回归便可派上用场。 9.8.2 多项分布回归 若响应变量包含两个以上的无序类别（比如，已婚/寡居/离婚），便可使用mlogit包中的mlogit()函数拟合多项Logistic回归。 9.8.3 序数Logistic回归 若响应变量是一组有序的类别（比如，信用风险为差/良/好），便可使用rms包中的lrm()函数拟合序数Logistic回归。 "],["logistic回归模型.html", "Chapter 10 Logistic回归模型 10.1 临床预测模型的本质 10.2 临床预测模型建立 10.3 案例1 10.4 列线图及校正曲线的绘制 10.5 C-Statistics 计算 10.6 亚组分析森林图的绘制", " Chapter 10 Logistic回归模型 10.1 临床预测模型的本质 ➢ 科研预测模型是通过已知来预测未知，而模型就是一个复杂的公式。也就是把已 知的东西通过这个模型的计算来预测未知的东西。 ➢ 临床预测模型的本质就是通过回归建模分析，回归的本质就是发现规律。回归是 量化刻画，X多大程度上影响Y。尤其是多元线性、Logistic、Cox回归分析等。 ➢ 模型的验证也体现着较高技术难度。模型效能评价是统计分析、数据建模、课题 设计的关键所在。 10.2 临床预测模型建立 10.2.1 验证步骤 10.2.2 研究思路 【举例】 ➢ 临床上有多个心血管疾病风险预测工具：Framingham、QRISK、PROCAM、 ASSIGN评分。Heart发表综述《Graphics and Statistics for Cardiology: Clinical Prediction Rules》以心血管风险评分(CVD risk factor)为例探讨如何借 助图形优势构建疾病的预测模型，并提出了6个重要步骤。 选择一组预测变量作为潜在CVD影响因素纳入到风险评分中 选择一个合适的统计模型来分析预测变量和CVD之间的关系 从已有的预测变量中，选择足够重要的变量纳入到风险评分中 构造风险评分模型 评价风险评分模型 在临床实践中解释风险评分的使用。 10.2.3 预测类案例研究思路总结 10.3 案例1 ➢ Hosmer 和 Lemeshow于1989年研究了低出生体重婴儿的影响因素。结果变量为是否娩出低出生体重儿（变量名为LOW，1=低出生体重，即婴儿出生体重&lt;2500g; 0=非低出生体重），考虑的影响因素（自变量）有：产妇妊娠前体重（lwt，磅）；产妇年龄（age，岁）；产妇在妊娠期间是否吸烟（smoke，0＝未吸、1＝吸烟）；本次妊娠前早产次数（ptl，次）；是否患有高血压（ht，0＝未患、1＝患病）；子宫对按摩、催产素等刺激引起收缩的应激性（ui，0＝无、1＝有）；妊娠前三个月社区医生随访次数（ftv，次）；种族（race，1＝白人、2＝黑人、3＝其他民族）。 10.3.1 变量筛选 library(foreign) #为了载入sav等形式的外部数据 library(rms) mydata&lt;-read.spss(&quot;data/lweight.sav&quot;) mydata&lt;-as.data.frame(mydata) #将数据转换为数据框结构 head(mydata) #查看数据前6行 ## id low age lwt race smoke ptl ht ui ftv bwt ## 1 85 正常体重 19 182 黑种人 不吸烟 0 无妊高症 有 0 2523 ## 2 86 正常体重 33 155 其他种族 不吸烟 0 无妊高症 无 3 2551 ## 3 87 正常体重 20 105 白种人 吸烟 0 无妊高症 无 1 2557 ## 4 88 正常体重 21 108 白种人 吸烟 0 无妊高症 有 2 2594 ## 5 89 正常体重 18 107 白种人 吸烟 0 无妊高症 有 0 2600 ## 6 91 正常体重 21 124 其他种族 不吸烟 0 无妊高症 无 0 2622 #设置哑变量 mydata$low &lt;- ifelse(mydata$low ==&quot;低出生体重&quot;,1,0) mydata$race1 &lt;- ifelse(mydata$race ==&quot;白种人&quot;,1,0) mydata$race2 &lt;- ifelse(mydata$race ==&quot;黑种人&quot;,1,0) mydata$race3 &lt;- ifelse(mydata$race ==&quot;其他种族&quot;,1,0) attach(mydata) dd&lt;-datadist(mydata) #打包数据集 options(datadist=&#39;dd&#39;) # 拟合回归模型，不含race3是由于将其作为参照 fit1&lt;-lrm(low~age+ftv+ht+lwt+ptl+smoke+ui+race1+race2, data=mydata,x=T,y=T) fit1 ## Logistic Regression Model ## ## lrm(formula = low ~ age + ftv + ht + lwt + ptl + smoke + ui + ## race1 + race2, data = mydata, x = T, y = T) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 189 LR chi2 31.12 R2 0.213 C 0.738 ## 0 130 d.f. 9 g 1.122 Dxy 0.476 ## 1 59 Pr(&gt; chi2) 0.0003 gr 3.070 gamma 0.477 ## max |deriv| 7e-05 gp 0.207 tau-a 0.206 ## Brier 0.181 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept 1.1427 1.0873 1.05 0.2933 ## age -0.0255 0.0366 -0.69 0.4871 ## ftv 0.0321 0.1708 0.19 0.8509 ## ht=妊高症 1.7631 0.6894 2.56 0.0105 ## lwt -0.0137 0.0068 -2.02 0.0431 ## ptl 0.5517 0.3446 1.60 0.1094 ## smoke=吸烟 0.9275 0.3986 2.33 0.0200 ## ui=有 0.6488 0.4676 1.39 0.1653 ## race1 -0.9082 0.4367 -2.08 0.0375 ## race2 0.3293 0.5339 0.62 0.5374 ## #查看OR值 summary(fit1) ## Effects Response : low ## ## Factor Low High Diff. Effect S.E. Lower 0.95 Upper 0.95 ## age 19 26 7 -0.178250 0.25649 -0.68095 0.324460 ## Odds Ratio 19 26 7 0.836740 NA 0.50613 1.383300 ## ftv 0 1 1 0.032104 0.17077 -0.30260 0.366810 ## Odds Ratio 0 1 1 1.032600 NA 0.73889 1.443100 ## lwt 110 141 31 -0.424740 0.21002 -0.83637 -0.013113 ## Odds Ratio 110 141 31 0.653940 NA 0.43328 0.986970 ## ptl 0 3 3 1.655000 1.03390 -0.37141 3.681400 ## Odds Ratio 0 3 3 5.233000 NA 0.68976 39.701000 ## race1 0 1 1 -0.908230 0.43667 -1.76410 -0.052382 ## Odds Ratio 0 1 1 0.403240 NA 0.17134 0.948970 ## race2 0 1 1 0.329270 0.53392 -0.71718 1.375700 ## Odds Ratio 0 1 1 1.390000 NA 0.48813 3.958000 ## ht - 妊高症:无妊高症 1 2 NA 1.763100 0.68941 0.41191 3.114400 ## Odds Ratio 1 2 NA 5.830700 NA 1.50970 22.519000 ## smoke - 吸烟:不吸烟 1 2 NA 0.927480 0.39859 0.14626 1.708700 ## Odds Ratio 1 2 NA 2.528100 NA 1.15750 5.521800 ## ui - 有:无 1 2 NA 0.648810 0.46760 -0.26767 1.565300 ## Odds Ratio 1 2 NA 1.913300 NA 0.76516 4.784100 结果解读 1.1 从模型中可以筛选出有统计学意义(P＜0.05)的变量 结果为：ht=妊高症(P=0.0105)、lwt(P=0.0431)、smoke=吸烟(P=0.02)、race1(P=0.0375) 此处需要注意的是，为了设置哑变量而重新定义的变量race1在模型中是具有统计学意义的，即说明与其一起组成的哑变量——种族对于因变量的是有影响的。因为无论组成哑变量的哪一个变量有统计学意义，都可说明该哑变量是有统计学意义的。 10.4 列线图及校正曲线的绘制 #绘制列线图 nom1 &lt;- nomogram(fit1, fun=plogis, fun.at=c(.001, .01, .05, seq(.1,.9, by=.1), .95, .99, .999), lp=F, funlabel=&quot;Low weight rate&quot; ) plot(nom1) # fun：将线性的预测转换为具体的分值 # lp= F：不显示线性预测值，只显示最后的分值 # funlabel：不同时间的标签 # maxscale=100 刻度为0~100 # fun.at设置最后结果的范围 结果解读 1.2 函数解释：nomogram(fit1, fun=plogis,fun.at=c(.001, .01, .05, seq(.1,.9, by=.1), .95, .99, .999),lp=F, funlabel=\"Low weight rate\") fit1 对象 fun=plogis 转换刻度值，最后的分值如何转换 fun.at=设置刻度间隔 -funlabel=最后一条的标签 我们可以看到，此处绘制的列线图，将race1和race2作为两个变量单列出来，实际上这是不必要的。我们需要将种族变量转换为一个二分类变量，即“白种人”和“黑人及其他种族” #将种族变量二值化处理 mydata$race &lt;- as.factor(ifelse(mydata$race==&quot;白种人&quot;, &quot;白种人&quot;,&quot;黑人及其他种族&quot;)) #打包数据 dd&lt;-datadist(mydata) options(datadist=&#39;dd&#39;) #拟合模型（全因素） fit2&lt;-lrm(low~age+ftv+ht+lwt+ptl+smoke+ui+race,data=mydata,x=T,y=T) fit2 ## Logistic Regression Model ## ## lrm(formula = low ~ age + ftv + ht + lwt + ptl + smoke + ui + ## race, data = mydata, x = T, y = T) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 189 LR chi2 30.74 R2 0.211 C 0.735 ## 0 130 d.f. 8 g 1.112 Dxy 0.469 ## 1 59 Pr(&gt; chi2) 0.0002 gr 3.039 gamma 0.470 ## max |deriv| 7e-05 gp 0.205 tau-a 0.203 ## Brier 0.182 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept 0.0921 1.1475 0.08 0.9360 ## age -0.0272 0.0365 -0.74 0.4563 ## ftv 0.0365 0.1692 0.22 0.8293 ## ht=妊高症 1.7516 0.6885 2.54 0.0110 ## lwt -0.0125 0.0065 -1.94 0.0528 ## ptl 0.5479 0.3451 1.59 0.1124 ## smoke=吸烟 0.9737 0.3922 2.48 0.0130 ## ui=有 0.6372 0.4701 1.36 0.1753 ## race=黑人及其他种族 1.0239 0.3940 2.60 0.0094 ## summary(fit2) ## Effects Response : low ## ## Factor Low High Diff. Effect S.E. Lower 0.95 ## age 19 26 7 -0.190330 0.25550 -0.69110 ## Odds Ratio 19 26 7 0.826690 NA 0.50103 ## ftv 0 1 1 0.036487 0.16923 -0.29519 ## Odds Ratio 0 1 1 1.037200 NA 0.74439 ## lwt 110 141 31 -0.387840 0.20030 -0.78042 ## Odds Ratio 110 141 31 0.678520 NA 0.45821 ## ptl 0 3 3 1.643800 1.03540 -0.38543 ## Odds Ratio 0 3 3 5.175000 NA 0.68016 ## ht - 妊高症:无妊高症 1 2 NA 1.751600 0.68854 0.40206 ## Odds Ratio 1 2 NA 5.763700 NA 1.49490 ## smoke - 吸烟:不吸烟 1 2 NA 0.973710 0.39221 0.20499 ## Odds Ratio 1 2 NA 2.647700 NA 1.22750 ## ui - 有:无 1 2 NA 0.637190 0.47011 -0.28422 ## Odds Ratio 1 2 NA 1.891200 NA 0.75260 ## race - 黑人及其他种族:白种人 1 2 NA 1.023900 0.39404 0.25154 ## Odds Ratio 1 2 NA 2.783900 NA 1.28600 ## Upper 0.95 ## 0.310440 ## 1.364000 ## 0.368170 ## 1.445100 ## 0.004744 ## 1.004800 ## 3.673100 ## 39.374000 ## 3.101100 ## 22.223000 ## 1.742400 ## 5.711100 ## 1.558600 ## 4.752100 ## 1.796200 ## 6.026500 #绘图 nom2 &lt;- nomogram(fit2, fun=plogis, fun.at=c(.001, .01, .05, seq(.1,.9, by=.1), .95, .99, .999), lp=F, funlabel=&quot;Low weight rate&quot; ) plot(nom2) 结果解读 1.3 将种族变量二值化后，生成的列线图更加简洁。 仔细观察后发现，在列线图中还存在一个无意义的变量ftv（贡献非常小），此时我们应选择将其去掉。 现在，先绘制该模型的校正曲线 #绘制校正曲线 cal2 &lt;- calibrate(fit2, method=&#39;boot&#39;, B=1000) plot(cal2,xlim=c(0,1.0),ylim=c(0,1.0)) ## ## n=189 Mean absolute error=0.033 Mean squared error=0.00155 ## 0.9 Quantile of absolute error=0.053 结果解读 1.4 method='boot'重抽样方法， B=1000重抽样1000次。 xlim=c(0,1.0)、ylim=c(0,1.0)),x、y轴的取值都是0~1 经过以上步骤，我们确定了最后想要的模型和列线图，现在绘制最终的列线图及校正曲线 fit3&lt;-lrm(low~ht+lwt+ptl+smoke+race,data=mydata,x=T,y=T) nom3 &lt;- nomogram(fit3, fun=plogis,fun.at=c(.001, .01, .05, seq(.1,.9, by=.1), .95, .99, .999),lp=F, funlabel=&quot;Low weight rate&quot;) plot(nom3) cal3 &lt;- calibrate(fit3, method=&#39;boot&#39;, B=1000) plot(cal3,xlim=c(0,1.0),ylim=c(0,1.0)) ## ## n=189 Mean absolute error=0.019 Mean squared error=0.00065 ## 0.9 Quantile of absolute error=0.032 10.5 C-Statistics 计算 ➢ R中如何计算C-Statistics rms包中lrm函数拟合logistic回归模型，模型参数可直接读取C，Dxy library(foreign) library(rms) mydata&lt;-read.spss(&quot;data/lweight.sav&quot;) mydata&lt;-as.data.frame(mydata) mydata$low &lt;- ifelse(mydata$low ==&quot;低出生体重&quot;,1,0) mydata$race1 &lt;- ifelse(mydata$race ==&quot;白种人&quot;,1,0) mydata$race2 &lt;- ifelse(mydata$race ==&quot;黑种人&quot;,1,0) mydata$race3 &lt;- ifelse(mydata$race ==&quot;其他种族&quot;,1,0) attach(mydata) ## The following objects are masked from mydata (pos = 3): ## ## age, bwt, ftv, ht, id, low, lwt, ptl, race, race1, race2, race3, ## smoke, ui dd&lt;-datadist(mydata) options(datadist=&#39;dd&#39;) fit1&lt;-lrm(low~age+ftv+ht+lwt+ptl+smoke+ui+race1+race2,data=mydata,x=T,y=T) fit1 #直接读取模型中Rank Discrim.参数 C ## Logistic Regression Model ## ## lrm(formula = low ~ age + ftv + ht + lwt + ptl + smoke + ui + ## race1 + race2, data = mydata, x = T, y = T) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 189 LR chi2 31.12 R2 0.213 C 0.738 ## 0 130 d.f. 9 g 1.122 Dxy 0.476 ## 1 59 Pr(&gt; chi2) 0.0003 gr 3.070 gamma 0.477 ## max |deriv| 7e-05 gp 0.207 tau-a 0.206 ## Brier 0.181 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept 1.1427 1.0873 1.05 0.2933 ## age -0.0255 0.0366 -0.69 0.4871 ## ftv 0.0321 0.1708 0.19 0.8509 ## ht=妊高症 1.7631 0.6894 2.56 0.0105 ## lwt -0.0137 0.0068 -2.02 0.0431 ## ptl 0.5517 0.3446 1.60 0.1094 ## smoke=吸烟 0.9275 0.3986 2.33 0.0200 ## ui=有 0.6488 0.4676 1.39 0.1653 ## race1 -0.9082 0.4367 -2.08 0.0375 ## race2 0.3293 0.5339 0.62 0.5374 ## ROCR包中performance函数计算AUC mydata$predvalue&lt;-predict(fit2) library(ROCR) pred &lt;- prediction(mydata$predvalue, mydata$low) auc &lt;- performance(pred,&quot;auc&quot;) auc #auc即是C-statistics ## A performance instance ## &#39;Area under the ROC curve&#39; Hmisc包中的somers2函数直接计算C, Dxy library(Hmisc) somers2(mydata$predvalue, mydata$low) #somers2 {Hmisc} ## C Dxy n Missing ## 0.7344198 0.4688396 189.0000000 0.0000000 10.6 亚组分析森林图的绘制 10.6.1 数据录入 Response No.of.patients OR.95..CI. X X.1 X.2 Overall 1472 1.66(1.28,2.15) 1.66 1.28 2.15 Clinical stage NA NA NA NA Advanced 951 1.67(1.23,2.26) 1.67 1.23 2.26 non-advanced 521 1.67(1.05,2.66) 1.67 1.05 2.66 ctDNA assays NA NA NA NA qPCR 1280 1.73(1.30,2.29) 1.73 1.30 2.29 ARMS 192 1.58(0.50,5.04) 1.58 0.50 5.04 Ethnicity NA NA NA NA Asian 326 1.93(0.84,4.43) 1.93 0.84 4.43 Caucasian 1146 1.57(1.20,2.06) 1.57 1.20 2.06 10.6.2 绘制图形 library(forestplot) rs_forest &lt;- read.csv(&#39;data/rs_forest.csv&#39;,header = FALSE) # 读入数据的时候大家一定要把header设置成FALSE，保证第一行不被当作列名称。 # tiff(&#39;Figure 1.tiff&#39;,height = 1600,width = 2400,res= 300) forestplot(labeltext = as.matrix(rs_forest[,1:3]), #设置用于文本展示的列，此处我们用数据的前三列作为文本，在图中展示 mean = rs_forest$V4, #设置均值 lower = rs_forest$V5, #设置均值的lowlimits限 upper = rs_forest$V6, #设置均值的uplimits限 is.summary = c(T,T,T,F,F,T,F,F,T,F,F), #该参数接受一个逻辑向量，用于定义数据中的每一行是否是汇总值， #若是，则在对应位置设置为TRUE， #若否，则设置为FALSE；设置为TRUE的行则以粗体出现 zero = 1, #设置参照值，此处我们展示的是OR值，故参照值是1，而不是0 boxsize = 0.4, #设置点估计的方形大小 lineheight = unit(10,&#39;mm&#39;),#设置图形中的行距 colgap = unit(3,&#39;mm&#39;),#设置图形中的列间距 lwd.zero = 2,#设置参考线的粗细 lwd.ci = 1.5,#设置区间估计线的粗细 col=fpColors(box=&#39;#458B00&#39;, summary= &quot;#8B008B&quot;,lines = &#39;black&#39;,zero = &#39;#7AC5CD&#39;), #使用fpColors()函数定义图形元素的颜色， #从左至右分别对应点估计方形，汇总值，区间估计线，参考线 xlab=&quot;The estimates&quot;,#设置x轴标签 graph.pos = 3)#设置森林图的位置，此处设置为3，则出现在第三列 "],["泊松回归模型.html", "Chapter 11 泊松回归模型 11.1 拟合泊松回归 11.2 解释模型参数 11.3 过度离势", " Chapter 11 泊松回归模型 当通过一系列连续型和或类别型预测变量来预测计数型结果变量时，泊松回归是一个非常有用的工具。 ➢ 为阐述泊松回归模型的拟合过程，并探讨一些可能出现的问题，我们使用robust 程辑包中的Breslow癫痫数据集（Breslow, 1993）。特别地，我们将讨论在治疗 初期的8周内，抗癫痫药物对癫痫发病数的影响。请提前安装robust包。 ➢ 我们就遭受轻微或严重间歇性癫痫的病人的年龄和癫痫发病数收集了数据，包含 病人被随机分配到药物组或者安慰剂组前8周和随机分配后8周两种情况。响应变 量为sumY（随机化后8周内癫痫发病数），预测变量为治疗条件（Trt）、年龄 （Age）和前8周内的基础癫痫发病数 ➢ 之所以包含基础癫病发病数和年龄是因为它们对响应变量有潜在影响。在解释这些变量后，我们感兴趣的是药物治疗是否能减少癫病发病数 ➢ 首先， 看看数据集的统计汇总信息 library(robust) # look at dataset data(breslow.dat, package=&quot;robust&quot;) names(breslow.dat) ## [1] &quot;ID&quot; &quot;Y1&quot; &quot;Y2&quot; &quot;Y3&quot; &quot;Y4&quot; &quot;Base&quot; &quot;Age&quot; &quot;Trt&quot; &quot;Ysum&quot; ## [10] &quot;sumY&quot; &quot;Age10&quot; &quot;Base4&quot; summary(breslow.dat[c(6, 7, 8, 10)]) ## Base Age Trt sumY ## Min. : 6.00 Min. :18.00 placebo :28 Min. : 0.00 ## 1st Qu.: 12.00 1st Qu.:23.00 progabide:31 1st Qu.: 11.50 ## Median : 22.00 Median :28.00 Median : 16.00 ## Mean : 31.22 Mean :28.34 Mean : 33.05 ## 3rd Qu.: 41.00 3rd Qu.:32.00 3rd Qu.: 36.00 ## Max. :151.00 Max. :42.00 Max. :302.00 注意，虽然数据集有 12 变量，但是我们只关注之前描述的四个变量。 基础和随机化后的癫痫病发病数都有很高的偏度。 现在，我们通过绘制图形更详细地考察响应变量： # plot distribution of post-treatment seizure counts opar &lt;- par(no.readonly=TRUE) par(mfrow=c(1, 2)) attach(breslow.dat) hist(sumY, breaks=20, xlab=&quot;Seizure Count&quot;, main=&quot;Distribution of Seizures&quot;) boxplot(sumY ~ Trt, xlab=&quot;Treatment&quot;, main=&quot;Group Comparisons&quot;) par(opar) 从上图中我们可以清楚的看到因变量的便宜特性及可能的离群点。初看图形时，药物治疗下癫痫的发病数似乎变小了，且方差也变小了（泊松回归中，较小的方差伴随着较小的均值）。与标准最小二乘回归不同，泊松回归并不关注方差异质性。 11.1 拟合泊松回归 # fit regression fit &lt;- glm(sumY ~ Base + Age + Trt, data=breslow.dat, family=poisson()) summary(fit) ## ## Call: ## glm(formula = sumY ~ Base + Age + Trt, family = poisson(), data = breslow.dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.0569 -2.0433 -0.9397 0.7929 11.0061 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.9488259 0.1356191 14.370 &lt; 2e-16 *** ## Base 0.0226517 0.0005093 44.476 &lt; 2e-16 *** ## Age 0.0227401 0.0040240 5.651 1.59e-08 *** ## Trtprogabide -0.1527009 0.0478051 -3.194 0.0014 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2122.73 on 58 degrees of freedom ## Residual deviance: 559.44 on 55 degrees of freedom ## AIC: 850.71 ## ## Number of Fisher Scoring iterations: 5 输出结果列出了偏差、回归系数、标准误和参数为0的检验。此处预测变量在 p&lt;0.05的水平下都非常显著。 11.2 解释模型参数 coef(fit) ## (Intercept) Base Age Trtprogabide ## 1.94882593 0.02265174 0.02274013 -0.15270095 在泊松回归中，因变量以条件均值的对数形式 \\(\\log (\\lambda)\\) 来建模. 年龄的回归参数为0.0227, 表明保持其他预测变量不变，年龄增加一岁，癫痫病发病数的对数均值将相应增加0.03。截距项即当 预测变量都为0时，癫病发病数的对数均值。由于不可能为0岁，且调查对象的基础癫痫病发病数均不为0, 因此本例中截距项没有意义。 通常在因变量的初始尺度（癫病发病数，而非发病数的对数）上解释回归系数比较容易。因此，指数化系数： exp(coef(fit)) ## (Intercept) Base Age Trtprogabide ## 7.0204403 1.0229102 1.0230007 0.8583864 现在可以看到，保持其他变量不变，年龄增加一岁，期望的癫病发病数将乘以 1.023。这意味着年龄的增加与较高的癫痫病发病数相关联。更为重要的是，一单位Trt的变化（即从安慰剂到治疗组），期望的癫痫病发病数将乘以 0.86,也就是说，保持基础癫痫病发病数和年龄不变，服药组相对于安慰剂组癫痫病发病数降低了20%。 另外需要牢记的是，与Logisic回归中的指数化参数相似，泊松模型中的指数参数对响应变量的影响都是成倍增加的．而不是线性相加。 同样，你还需要评价泊松模型的过度离势。 11.3 过度离势 泊松分布的方差和均值相等。当响应变量观测的方差比依据泊松分布预测的方差大时，泊松回归可能发生过度离势。处理计数型数据时经常发生过度离势，且过度离势会对结果的可解释性造成负面影响，因此我们需要花些时间讨论该问题。 可能发生过度离势的原因有如下几个： 遗漏了某个重要的预测变量 可能因为事件相关 。 在泊松分布的观测中，计数中每次事件都被认为是独立发生的。 以癫痫病数据为例，这意味着对于任何病人，每次癫痫病发病的概率与其他癫痫病发病的概率相互独立。但是这个假设通常都无法满足。对于某个病人，在已知他已经发生了 39 次癫痫时，第一次发生癫痫的概率不可能与第40次发生癫痫的概率相同。 在纵向数据分析中，重复测量的数据由于内在群聚特性可导致过度离势。 此处并不讨论纵向泊松模型。 如果存在过度离势，在模型中你无法进行解释，那么可能会得到很小的标准误和置信区间，并且显著性检验也过于宽松（也就是说，你将会发现并不真实存在的效应） 与Logistic 回归类似，此处如果残差偏差与残差自由度的比例远远大于1，那么表明存在过度离势。对于癫痫病数据，它的比例为： deviance(fit)/df.residual(fit) ## [1] 10.1717 很显然，比例远远大于1。 qcc包提供了一个对泊松模型过度离势的检验方法 （在首次使用前，请确保巳经下载和安装此包）如下代码可进行癫痫病数据过度离势的检验： library(qcc) qcc.overdispersion.test(breslow.dat$sumY, type=&quot;poisson&quot;) ## ## Overdispersion test Obs.Var/Theor.Var Statistic p-value ## poisson data 62.87013 3646.468 0 意料之中，显著性检验的p值果然小于0.05, 进一步表明确实存在过度离势。 通过用famly=\"quasipoisson\" 替换 family=\"poisson\", 你仍然可以使用glm() 函数 对该数据进行拟合。这与Logistic回归处理过度离势的方法是相同的。 fit.od &lt;- glm(sumY ~ Base + Age + Trt, data=breslow.dat, family=quasipoisson()) summary(fit.od) ## ## Call: ## glm(formula = sumY ~ Base + Age + Trt, family = quasipoisson(), ## data = breslow.dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.0569 -2.0433 -0.9397 0.7929 11.0061 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.948826 0.465091 4.190 0.000102 *** ## Base 0.022652 0.001747 12.969 &lt; 2e-16 *** ## Age 0.022740 0.013800 1.648 0.105085 ## Trtprogabide -0.152701 0.163943 -0.931 0.355702 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 11.76075) ## ## Null deviance: 2122.73 on 58 degrees of freedom ## Residual deviance: 559.44 on 55 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 注意．使用类泊松(quasi-Poisson)方法所得的参数估计与泊松方法相同，但标准误变大了许多。此处，标准误越大将会导致Trt (和Age) 的p值越大于0.05。 当考虑过度离势，并控制基础癫痫病数和年龄时，并没有充足的证据表明药物治疗相对于使用安慰剂能显著降低癫痫病的发病次数。 不过请记住，本例只是用于阐释泊松模型，它的结果并不能用来反映真实世界中的普罗加比（治疗癫病）药效问题。 注：本章内容来自《R语言实战》第13章第3节泊松回归 "],["生存分析.html", "Chapter 12 生存分析 12.1 生存资料的描述 12.2 生存率的估计与生存曲线 12.3 组间生存曲线的比较 12.4 画一幅高水准的生存曲线", " Chapter 12 生存分析 12.1 生存资料的描述 12.1.1 生存分析方法的正确表达 阅读临床研究文献的时候经常看到如下表述，这其中的统计学方法该何如理解？ ➢ Aims compared treatments with respect to time to LR, time to distant recurrence, RFS, and OS via log-rank and Cox regression methods. Log-rank tests were exact, being based on hypergeometric probabilities. RFS and OS curves were estimated using the Kaplan-Meier method. ➢ Time-to-event curves were estimated by the Kaplan–Meier method and compared with the use of a two-sided log-rank test. 12.1.2 生存分析的基本概念 ➢ 生存资料(time to event)的统计方法统称为生存分析，(survival analysis)，它是将事件的结局和发生这种结局所经历的时间两个因素综合起来分析的一种统计方法。它能够处理截尾数据，并对整个生存过程进行分析或比较 12.1.3 生存分析的几个重要名词 ➢ 1. 终点事件: 终点事件 (terminal event)又称失效事件(failure event)或“死亡”事件(deathevent) ，泛指标志某种措施失败或失效的事件或反映治疗效果特征的事件，是根据研究目的确定的。如乳腺癌术后死亡、白血病化疗后复发、肾移植术后的肾衰、术后下床活动等，均可作为“死亡”事件。 ➢ 【举例】RFS该如何定义？ 字面意思是“无复发生存”，那“结局事件”一般定义为：复发+转移+复发同时转 移+癌症相关死亡，其余为“删失” ➢ 2. 生存时间: 生存时间(survival time)也是一个广义概念，泛指所关心的某现象的持续时间，即 随访观察持续的时间，常用符号t表示 ➢ 3. 死亡概率: 死亡概率(probability of death)表示单位时间段开始存活的个体，在 该段时间内死亡的可能性。符号q表示。 \\[ q= \\cfrac{某年内死亡人数}{某年年初人口数} \\] ➢ 4. 生存概率: 生存概率(probability of survival)表示单位时间段开始存活的个体， 到该段时间结束时仍存活的可能性。符号p表示 \\[ p=\\cfrac{某年活满一年人数}{某年年初人口数} \\] \\[ p=1-q \\] ➢ 5. 生存率: 生存率(survival rate, survival function)表示观察对象经历tk个单位 时间段后仍存活的可能性。 若无截尾数据，则: \\[ S(t_k)= P(T&gt;t_k) = \\cfrac{t_k时刻仍存活的例数}{观察总例数} \\] 其中： \\[ 0≤S(t)≤1 \\] 若有截尾数据，须分时段计算生存概率。假定观察对象在各个时段的生存事件独立，应用概率乘法定理： \\[ S(t_k)= P(T&gt;t_k) = p_1 ·p_2 \\cdots p_k \\] Pi 是某时段的生存概率，故生存率又称累积生存概率 (Cumulative Probabilityof Survival)。 生存率与生存概率的区别 ➢ 6. 生存曲线: 生存曲线(survival curve)：生存时间为横轴，将各时点所对应的生 存率连接在一起的曲线图。 12.1.4 生存数据的两种类型 ➢ 完全数据(complete data)：指从观察起点到发生“死亡”事件所经历的时 间。提供了观察对象确切的生存时间。 ➢ 截尾数据(censored data)：亦称截尾值(censored value)或终检值。指从 观察起点到发生非“死亡”事件所经历的时间。 截尾原因大致有三种情况： ➢ 失访：未继续就诊、拒绝访问或搬迁而失去联系。 ➢ 死于与研究疾病无关的原因：由于其他原因死亡。 ➢ 研究终止：研究结束时终点事件尚未发生。 ➢ 截尾数据不能提供完全的信息，真实的生存时间未知，只知道比观察到的截尾时间长，常用符号“＋”表示 生存资料的特点： ➢ 有生存结局、生存时间 ➢ 有不确定数据（截尾数据） ➢ 分布呈指数分布、Weibull分布、对数正态分布、对数logistic分布等 12.1.5 生存分析的主要任务 ➢ 统计描述： 计算生存率、绘制生存率曲线、计算中位生存时间等。 ➢ 统计推断： 估计总体生存率的可信区间、生存率曲线的比较。 12.1.6 生存分析基本方法 ➢ 非参数法: 不论资料是什么样的分布类型，只根据样本提供的顺序统计量对生存率进行估计，常用乘积极限法和寿命表法。 ➢ 参数法: 假定生存时间服从于特定的参数分布，根据已知分布的特点对影响生存的时间进行分析，常用指数分布法、Weibull分布法、对数正态回归分析法和对数Logistic回归分析法。 ➢ 半参数法: 介于参数法和非参数法之间，一般属多因素分析方法，用于探讨生存过程的主要影响因素，其经典方法是Cox比例风险回归模型。 12.1.7 随访研究的几个问题 12.1.7.1 随访时间 ➢ 1. 开始随访的时间： 入(出)院时间、确诊时间、开始治疗时间等可作为随访开始的时间。如乳腺癌的乳腺切除术后第一天或出院日、白血病化疗后缓解出院日等，也可规定开始治疗日为随访开始时间。 ➢ 2. 随访结局和终止随访时间: 随访的结局可能有以下几种： 死亡：泛指处理措施失败的事件。如肿瘤化疗后的复发、肾移植因肾衰或与之有关的原因而死亡等。终止随访时间为“死亡”时间。 失访：拒绝随访、失去联系或中途退出等。终止随访时间为最后一次时间。 死于与研究疾病无关的原因：终止随访时间为死亡时间。 研究终止。研究终止时观察对象仍然存活。终止随访时间为研究终止时间。 ➢ 3. 影响生存的有关因素： 如患者年龄、病情、病程、术前健康等情况，以便分析这些因素对生存率的影响。 12.1.7.2 随访方式 1. 规定时点入组，统一分组，同一起点开始随访 全部观察对象同时接受处理措施，观察到最后一例出现结果或事先规定的随访截止时间。 2. 研究对象入组时间不定，随时入组，即刻随访 观察对象在不同时间接受处理措施，完成一定数量随访病例或按事先规定的时间停止随访 12.2 生存率的估计与生存曲线 12.2.1 Kaplan-Meier法 ➢ 乘积极限法(product-limit estimate)又称Kaplan-Meier法，适用于分组生存资料的分析，需要已知每例患者的生存时间与状态 【案例】14例膀胱肿瘤 &lt; 3.0cm患者和16例膀胱肿瘤 ≥3.0患者的生存时间（月）如下，试估计两组各时点生存率及其标准误、各时点总体生存率的95%可信区间、中位生存时间，并绘制生存曲线 [1]. 生存时间t：由小到大排列，遇非截尾和截尾值相同，截尾值排后。 [2]. 死亡数dt：与生存时间t对应。注意：截尾值对应的个体未发生“死亡”事件，故死亡数为0。 [3]. 期初病例数nt，表示恰好在该时刻以前的病例数。如n29为10，表示恰好在29月时点前有10人存活。 [4]. 死亡概率qt，表示t月前的观察对象恰好在t月时点死亡的概率。 [5]. 生存概率pt，表示t月前的观察对象恰好在t月时点存活的概率。 [6]. 生存率S(t)。表示该人群恰好活过t 时刻的概率。它为小于和等于 t 时刻的各时点生存概率的乘积。 [7]. 生存率的标准误SES(t) 。 \\[ SE[S(t)]=\\sqrt{\\cfrac{1-S(t)}{n_t-d_t}} \\] ➢ 总体生存率的可信区间计算 假定生存率近似服从正态分布，某时点总体生存率的（1－a）％可信区间，公 式为： \\[ S(t) \\pm u_{\\alpha /2}SE[S(t)] \\] 本例28月总体生存率的95％可信区间：\\(0.7144 \\pm 1.96 \\times 0.1207\\) 即膀胱肿瘤&lt;3.0cm患者28月生存率的95%可信区间为47.78％~95.10%。生存曲线尾部的生存率不适合于用该法计算总体生存率的可信区间。 ➢ 中位生存时间计算: 由表2可见，中位生存时间估计在36月。 采用线性内插法计算：找到与生存率50％相邻的上下两个生存率及其生存 时间，利用线性比例关系求解中位生存时间（后面有举例）。 若生存率0.5处所对应的曲线与X轴平行，则中位生存时间不止一个。 若各时间点生存率均大于50％，则无法估计中位生存时间。 12.2.2 寿命表法 ➢ 适用于未分组的生存资料，不需要知道每例患者的生存时间与状态。 ①. 实际工作中，随访结果常常没有每个观察对象确切的生存时间，只能获得按随访时间分段的资料。 ②. 当样本较大(如 n ≥ 50)时，采用乘积极限法估计生存率及其标准误往往较为繁琐。 ➢ 生存率的计算: 确诊后年数t～：“0～”表示从确诊日起不满一年，“1～”表示确诊后1年至不满2年，依次类推。 期内死亡数dt ：指期内死于某恶性肿瘤的人数。 期内截尾数ct：泛指具有截尾数据的人，表示随访已满t年，但在未满t+1月期间失访的人。 期初观察例数n’t ：指时刻t以前的人数。 期初有效例数nt ：相当于实际观察人时数。在各年年初观察人数中减去同年截尾数的一半。 死亡概率qt，指活满t年的病人在t＋1年内死亡的的概率。 生存概率pt，指活满t年的病人在t＋1年内存活的概率。 生存率S(t)，表示活过t年的概率。它为小于和等于 t 时刻的各时点生存概 率的乘积。 生存率的标准误SES(t) 。 12.2.3 生存分析对生存资料的基本要求 样本由随机抽样方法获得，并应有足够的数量； 死亡例数不能太少(≥30)； 截尾值比例不能太大； 生存时间尽可能精确到天数，因为多数生存分析方法都在生存时间排序的基础上作统计处理的，即使是小小的舍入误差，也可能改变生存时间顺序而影响结果。 12.2.4 Kaplan-Meier法估计案例 【案例】我们将使用ISwR包中包含的恶性黑色素瘤后的生存数据集(melanom)，该数据集信息如下 描述：该数据集包含205行、7列，包含了患恶性黑色素瘤病人术后的生存数据，由 Odense 大学医院的K.T.Drzewiecki收集。 用法：melanom 格式：该数据框包含如下列∶ no: 数值向量，病人编号。 status: 数值向量，编码表示生存状态 1∶死于黑色素瘤 2∶存活 3∶死于其他原因 days: 数值向量，观测时间。 ulc:数值向量编码， ulceration: 1∶存在; 2∶缺失 thick: 数值向量，肿瘤厚度（1/100 mm）。 sex 数值向量编码; 1∶女性，2;男性。 代码如下： library(survival) library(ISwR) attach(melanom) names(melanom) ## [1] &quot;no&quot; &quot;status&quot; &quot;days&quot; &quot;ulc&quot; &quot;thick&quot; &quot;sex&quot; Surv(days, status==1) #定义资料中的终点 ## [1] 10+ 30+ 35+ 99+ 185 204 210 232 232+ 279 295 355+ ## [13] 386 426 469 493+ 529 621 629 659 667 718 752 779 ## [25] 793 817 826+ 833 858 869 872 967 977 982 1041 1055 ## [37] 1062 1075 1156 1228 1252 1271 1312 1427+ 1435 1499+ 1506 1508+ ## [49] 1510+ 1512+ 1516 1525+ 1542+ 1548 1557+ 1560 1563+ 1584 1605+ 1621 ## [61] 1627+ 1634+ 1641+ 1641+ 1648+ 1652+ 1654+ 1654+ 1667 1678+ 1685+ 1690 ## [73] 1710+ 1710+ 1726 1745+ 1762+ 1779+ 1787+ 1787+ 1793+ 1804+ 1812+ 1836+ ## [85] 1839+ 1839+ 1854+ 1856+ 1860+ 1864+ 1899+ 1914+ 1919+ 1920+ 1927+ 1933 ## [97] 1942+ 1955+ 1956+ 1958+ 1963+ 1970+ 2005+ 2007+ 2011+ 2024+ 2028+ 2038+ ## [109] 2056+ 2059+ 2061 2062 2075+ 2085+ 2102+ 2103 2104+ 2108 2112+ 2150+ ## [121] 2156+ 2165+ 2209+ 2227+ 2227+ 2256 2264+ 2339+ 2361+ 2387+ 2388 2403+ ## [133] 2426+ 2426+ 2431+ 2460+ 2467 2492+ 2493+ 2521+ 2542+ 2559+ 2565 2570+ ## [145] 2660+ 2666+ 2676+ 2738+ 2782 2787+ 2984+ 3032+ 3040+ 3042 3067+ 3079+ ## [157] 3101+ 3144+ 3152+ 3154+ 3180+ 3182+ 3185+ 3199+ 3228+ 3229+ 3278+ 3297+ ## [169] 3328+ 3330+ 3338 3383+ 3384+ 3385+ 3388+ 3402+ 3441+ 3458+ 3459+ 3459+ ## [181] 3476+ 3523+ 3667+ 3695+ 3695+ 3776+ 3776+ 3830+ 3856+ 3872+ 3909+ 3968+ ## [193] 4001+ 4103+ 4119+ 4124+ 4207+ 4310+ 4390+ 4479+ 4492+ 4668+ 4688+ 4926+ ## [205] 5565+ survfit(Surv(days, status==1)~1)#将其拟合，可以看到简要的信息 ## Call: survfit(formula = Surv(days, status == 1) ~ 1) ## ## n events median 0.95LCL 0.95UCL ## [1,] 205 57 NA NA NA surv.all &lt;- survfit(Surv(days,status==1)~1) summary(surv.all) ## Call: survfit(formula = Surv(days, status == 1) ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 185 201 1 0.995 0.00496 0.985 1.000 ## 204 200 1 0.990 0.00700 0.976 1.000 ## 210 199 1 0.985 0.00855 0.968 1.000 ## 232 198 1 0.980 0.00985 0.961 1.000 ## 279 196 1 0.975 0.01100 0.954 0.997 ## 295 195 1 0.970 0.01202 0.947 0.994 ## 386 193 1 0.965 0.01297 0.940 0.991 ## 426 192 1 0.960 0.01384 0.933 0.988 ## 469 191 1 0.955 0.01465 0.927 0.984 ## 529 189 1 0.950 0.01542 0.920 0.981 ## 621 188 1 0.945 0.01615 0.914 0.977 ## 629 187 1 0.940 0.01683 0.907 0.973 ## 659 186 1 0.935 0.01748 0.901 0.970 ## 667 185 1 0.930 0.01811 0.895 0.966 ## 718 184 1 0.925 0.01870 0.889 0.962 ## 752 183 1 0.920 0.01927 0.883 0.958 ## 779 182 1 0.915 0.01981 0.877 0.954 ## 793 181 1 0.910 0.02034 0.871 0.950 ## 817 180 1 0.904 0.02084 0.865 0.946 ## 833 178 1 0.899 0.02134 0.859 0.942 ## 858 177 1 0.894 0.02181 0.853 0.938 ## 869 176 1 0.889 0.02227 0.847 0.934 ## 872 175 1 0.884 0.02272 0.841 0.930 ## 967 174 1 0.879 0.02315 0.835 0.926 ## 977 173 1 0.874 0.02357 0.829 0.921 ## 982 172 1 0.869 0.02397 0.823 0.917 ## 1041 171 1 0.864 0.02436 0.817 0.913 ## 1055 170 1 0.859 0.02474 0.812 0.909 ## 1062 169 1 0.854 0.02511 0.806 0.904 ## 1075 168 1 0.849 0.02547 0.800 0.900 ## 1156 167 1 0.844 0.02582 0.794 0.896 ## 1228 166 1 0.838 0.02616 0.789 0.891 ## 1252 165 1 0.833 0.02649 0.783 0.887 ## 1271 164 1 0.828 0.02681 0.777 0.883 ## 1312 163 1 0.823 0.02713 0.772 0.878 ## 1435 161 1 0.818 0.02744 0.766 0.874 ## 1506 159 1 0.813 0.02774 0.760 0.869 ## 1516 155 1 0.808 0.02805 0.755 0.865 ## 1548 152 1 0.802 0.02837 0.749 0.860 ## 1560 150 1 0.797 0.02868 0.743 0.855 ## 1584 148 1 0.792 0.02899 0.737 0.851 ## 1621 146 1 0.786 0.02929 0.731 0.846 ## 1667 137 1 0.780 0.02963 0.725 0.841 ## 1690 134 1 0.775 0.02998 0.718 0.836 ## 1726 131 1 0.769 0.03033 0.712 0.831 ## 1933 110 1 0.762 0.03085 0.704 0.825 ## 2061 95 1 0.754 0.03155 0.694 0.818 ## 2062 94 1 0.746 0.03221 0.685 0.812 ## 2103 90 1 0.737 0.03290 0.676 0.805 ## 2108 88 1 0.729 0.03358 0.666 0.798 ## 2256 80 1 0.720 0.03438 0.656 0.791 ## 2388 75 1 0.710 0.03523 0.645 0.783 ## 2467 69 1 0.700 0.03619 0.633 0.775 ## 2565 63 1 0.689 0.03729 0.620 0.766 ## 2782 57 1 0.677 0.03854 0.605 0.757 ## 3042 52 1 0.664 0.03994 0.590 0.747 ## 3338 35 1 0.645 0.04307 0.566 0.735 plot(surv.all,col=&quot;#009999&quot;)#绘制带可信区间的生存曲线 #拟合以性别作为分组的函数 surv.bysex &lt;- survfit(Surv(days,status==1)~sex) plot(surv.bysex) plot(surv.bysex, conf.int=T, col=c(&quot;#3399CC&quot;,&quot;#FF0099&quot;)) #画出的生存曲线过于凌乱，我们可以隐藏可信区间 plot(surv.bysex, conf.int=F, col=c(&quot;#3399CC&quot;,&quot;#FF0099&quot;)) detach(melanom) 12.2.5 LifeTable寿命表法 ➢ 【案例】某医生比较两种药物治疗HIV感染患者后的生存时间t(月)，试用寿命表 法分析患者生存时间 hmohiv&lt;-read.table(&quot;data/hmohiv.csv&quot;, sep=&quot;,&quot;, header = TRUE) attach(hmohiv) ## The following object is masked from mydata (pos = 10): ## ## age ## The following object is masked from mydata (pos = 11): ## ## age head(hmohiv) ## 锘縩o entdate enddate time age drug censor ## 1 1 15-May-90 14-Oct-90 5 46 0 1 ## 2 2 19-Sep-89 20-Mar-90 6 35 1 0 ## 3 3 21-Apr-91 20-Dec-91 8 30 1 1 ## 4 4 3-Jan-91 4-Apr-91 3 30 1 1 ## 5 5 18-Sep-89 19-Jul-91 22 36 0 1 ## 6 6 18-Mar-91 17-Apr-91 1 32 1 0 library(KMsurv) library(nlme) t6m&lt;-floor(time/6) tall&lt;-data.frame(t6m, censor) die&lt;-gsummary(tall, sum, groups=t6m)#分组 total&lt;-gsummary(tall, length, groups=t6m) rm(t6m) ltab.data&lt;-cbind(die[,1:2], total[,2]) detach(hmohiv) attach(ltab.data) lt=length(t6m) t6m[lt+1]=NA nevent=censor nlost=total[,2] - censor mytable&lt;-lifetab(t6m, 100, nlost, nevent) mytable[,1:5] ## nsubs nlost nrisk nevent surv ## 0-1 100 10 95.0 41 1.00000000 ## 1-2 49 3 47.5 21 0.56842105 ## 2-3 25 2 24.0 6 0.31711911 ## 3-4 17 1 16.5 1 0.23783934 ## 4-5 15 1 14.5 0 0.22342483 ## 5-6 14 0 14.0 5 0.22342483 ## 6-7 9 0 9.0 1 0.14363025 ## 7-8 8 0 8.0 1 0.12767133 ## 8-9 7 0 7.0 1 0.11171242 ## 9-10 6 1 5.5 3 0.09575350 ## 10-NA 2 2 1.0 0 0.04352432 plot(t6m[1:11], mytable[,5], type=&quot;s&quot;, xlab=&quot;Survival time in every 6 month&quot;, ylab=&quot;Proportion Surviving&quot;) detach(ltab.data) 这段代码与分析结果又臭又长，很不好理解。但是实际上在临床中将数据收集成寿命表形式是一个非常愚蠢的行为，这段代码大部分精力都集中于如何将原始数据转换为寿命表，所以没有什么掌握的必要，看看就好，提醒自己不要做这种愚蠢的事情。如果真的做了，可以投入SPSS的怀抱，比用R语言好的多（大概他们做包的时候没有想到会有这么蠢的人）。 12.3 组间生存曲线的比较 12.3.1 Log-rank检验 ➢ 对数秩检验，非参数检验法，其零假设为两总体生存曲线相同，但检验过程一 般不估计生存率，而利用死亡数和死亡率函数作统计推断。 ➢ 基本思想：当\\(H_0\\)成立时，根据t时点的死亡率，计算出各组的理论死亡数，则 检验统计量： \\[ \\chi^2 = \\cfrac{(A_g-T_g)^2}{V_g} \\quad \\quad V_g= \\sum \\cfrac{n_{gi}}{ni}(1-\\cfrac{n_{gi}}{ni})(\\cfrac{n_i-d_i}{n_i-1})d_i \\] 亦可用公式： \\(\\chi^2=\\sum \\cfrac{(A-T)^2}{T}\\) 检验统计量\\(\\chi^2\\)近似服从\\(v=(组数-1)\\)的\\(\\chi^2\\)分布 ➢ Log-rank检验应用及注意事项： [1]. 相对死亡比（relative death ratio）：实际死亡数A与理论死亡数T之比， 则相对危险度（relative risk , RR）估计值为两组相对死亡比率之比。 \\[ RR=\\cfrac{R_1}{R_2}=\\cfrac{A_1 / T_1}{A2/T2} \\] [2]. log-rank检验：用于整条生存曲线的比较，若比较两条生存曲线某时点的 生存率，如2年生存率，按下式: \\[ u=\\cfrac{S_1(t)-S_2(t)}{\\sqrt{SE^2[S_1(t)]+SE^2[S_2(t)]}} \\] [3]. 若比较多个时点生存率，检验水准应取Bonferroni校正，即\\(\\alpha&#39;=\\alpha/k\\)其中k为比较次数，以保证总的Ⅰ型错误概率不超过α。 [4]. log-rank检验：单因素分析，应用条件是除比较因素外，影响生存率的各混杂因素组间均衡可比，否则采用Cox比例风险回归模型。 [5]. 对数秩检验也可用于三组生存曲线的比较。 [6]. 由对数秩检验过程可知，若每一时点A组死亡率都高一点(生存率低一点)，则检验结果必然为A不同于B。因此，在比较的两条生存率曲线无交叉时，直接用对数秩检验是合适的。反之，就需进一步分析原因，了解是否存在混杂因素的影响。 12.3.2 案例1 例 观察两组卵巢腺癌患者的病程天数如下。请用乘积极限法进行描述，并比较两组的生存期差异有无统计学意义，并作生存率曲线。如表15-2 所示。 #install.packages(&quot;survival&quot;) library(survival) example15_3 &lt;- read.table (&quot;data/example15_3.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(example15_3) t1&lt;-as.numeric(example15_3$t) total &lt;- survfit(Surv(t1, censor)~1) summary(total) plot(total,conf.int = F) separate &lt;- survfit(Surv(t1, censor)~group) summary(separate) plot(separate, lty = c(&#39;solid&#39;,&#39;dashed&#39;), col=c(&#39;black&#39;,&#39;blue&#39;), xlab=&#39;survival time in days&#39;,ylab=&#39;survival probabilities&#39;) legend(&#39;topright&#39;, c(&#39;Group A&#39;,&#39; Group B&#39;), lty=c(&#39;solid&#39;,&#39;dashed&#39;), col=c(&#39;black&#39;,&#39;blue&#39;)) survdiff(Surv(t1, censor)~group) survdiff(Surv(t1, censor)~group,rho=1) # rho = 1 it is equivalent to the Peto &amp; Peto modification of the Gehan-Wilcoxon test. 12.3.3 案例2 library(coin) data(glioma) attach(glioma) library(survival) g3 &lt;- subset(glioma, histology ==&#39;Grade3&#39;)#取亚集 fit &lt;- survfit(Surv(time, event)~group,data = g3) plot(fit, lty = c(2,1), col = c(2,1)) legend(&#39;bottomright&#39;, legend = c(&#39;Control&#39;,&#39;Treatment&#39;), lty = c(2,1), col = c(2,1)) survdiff(Surv(time, event)~group,data = g3) ## Call: ## survdiff(formula = Surv(time, event) ~ group, data = g3) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## group=Control 6 4 1.49 4.23 6.06 ## group=RIT 11 2 4.51 1.40 6.06 ## ## Chisq= 6.1 on 1 degrees of freedom, p= 0.01 logrank_test(Surv(time, event)~group,data = g3, distribution =&quot;exact&quot;) ## ## Exact Two-Sample Logrank Test ## ## data: Surv(time, event) by group (Control, RIT) ## Z = -2.1711, p-value = 0.02877 ## alternative hypothesis: true theta is not equal to 1 logrank_test(Surv(time, event)~group|histology,data = glioma, distribution = approximate(B = 1000)) #两组比较,coin包 logrank_test函数#SurvivalTests {coin} ## Warning in approximate(B = 1000): &#39;B&#39; is deprecated; use &#39;nresample&#39; instead ## ## Approximative Two-Sample Logrank Test ## ## data: Surv(time, event) by ## group (Control, RIT) ## stratified by histology ## Z = -3.6704, p-value &lt; 0.001 ## alternative hypothesis: true theta is not equal to 1 12.4 画一幅高水准的生存曲线 library(survminer) library(survival) library(coin) data(glioma) attach(glioma) g3 &lt;- subset(glioma, histology ==&#39;Grade3&#39;) fit &lt;- survfit(Surv(time, event)~group,data = g3) ggsurvplot(fit, pval = TRUE, # 在图上添加log rank检验的p值 conf.int = TRUE,# 添加置信区间 risk.table = TRUE, # 在图下方添加风险表 risk.table.col = &quot;strata&quot;, # 根据数据分组为风险表添加颜色 linetype = &quot;strata&quot;, # 改变不同组别的生存曲线的线型 surv.median.line = &quot;hv&quot;, # 标注出中位生存时间 ggtheme = theme_bw(), # 改变图形风格 palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;)) # 图形颜色风格 ggsurvplot( fit, pval = FALSE, conf.int = TRUE, fun = &quot;cumhaz&quot;, conf.int.style = &quot;ribbon&quot;, # 设置置信区间的风格 xlab = &quot;Time in days&quot;, # 设置x轴标签 break.time.by = 20, # 将x轴按照20为间隔进行切分 ggtheme = theme_light(), # 设置图形风格 risk.table = &quot;abs_pct&quot;, # 在风险表中添加绝对数和相对数 risk.table.y.text.col = TRUE,# 设置风险表的文字颜色 risk.table.y.text = FALSE,# 以条柱展示风险表的标签，而非文字 ncensor.plot = TRUE, # 展示随访过程中不同时间点死亡和删失的情况 surv.median.line = &quot;hv&quot;, # 添加中位生存时间 legend.labs = c(&quot;Male&quot;, &quot;Female&quot;), # 改变图例标签 palette = c(&quot;#FF6666&quot;, &quot;#33CC66&quot;) # 设置颜色 ) ## Warning in .add_surv_median(p, fit, type = surv.median.line, fun = fun, : Adding ## survival median lines is not allowed when fun is: cumhaz ggsurvplot(fit, conf.int = TRUE, risk.table.col = &quot;strata&quot;, ggtheme = theme_bw(), palette = c(&quot;#009999&quot;, &quot;#CCCC00&quot;), fun = &quot;cumhaz&quot;) dev.off() ## null device ## 1 "],["cox比例风险模型.html", "Chapter 13 COX比例风险模型 13.1 模型简介 13.2 COX分析的一般步骤 13.3 案例 13.4 列线图绘制 13.5 亚组分析森林图的绘制", " Chapter 13 COX比例风险模型 13.1 模型简介 ➢ 1972年，英国统计学家 D. R. Cox 博士提出了一种比例风险回归模型（Cox Proportional Hazard Model），简称Cox模型。它可以分析多种因素对生存时间的影响，而且允许有“截尾”存在。是生存分析中最重要的模型之一。 ➢ Cox回归模型主要用于肿瘤和其它慢性病的预后因素分析，也可以用于一般的临床疗效评价和队列的病因探索。 13.1.1 COX模型的基本结构 ➢ COX模型不直接考察生存时间与各自变量的关系，而是用风险率作为因变量。 COX模型的基本结构为： \\[ h(t,X)=h_0(t)\\exp(\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_mX_m) \\quad \\quad (1) \\] ➢ \\(h(t,X)\\)：t时点上m个危险因素起作用时的风险率，即在时间t上的死亡率； ➢ \\(h0(t)\\)：某时间t上当m个危险因素为0时的基准风险率； ➢ \\(X(X_1, X_2,…, X_m)\\)：与生存时间可能有关的自变量； ➢ \\(β＝(β_1, β_2,…, β_m)\\)：COX模型的回归系数。 ➢ \\(β_j\\)与\\(h(t,X)\\)之间有如下关系： （1）\\(β_j&gt;0\\)，则\\(X_j\\)取值越大，\\(h(t,X)\\)的值越大，表示病人死亡的风险率越大； （2）\\(β_j＝0\\)，则\\(X_j\\)取值对\\(h(t,X)\\)无影响； （3）\\(β_j&lt;0\\)，则\\(X_j\\)取值越大，\\(h(t,X)\\)的值越小，表示病人死亡的风险率越小。 ➢ \\(h(t)\\)和\\(h_0(t)\\)成比例，比例系数是： \\[ h(t,X) / h_0(t) = \\exp(\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_mX_m) \\] ➢ 故COX模型又称比例风险模型，将上式两边取自然对数，得： \\[ \\ln[h(t,X) / h_0(t)]=\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_mX_m \\] ➢ 此式与多元线性回归模型非常类似，故有人称COX模型为COX回归。 ➢ 由此式可见\\(β_j\\)的含义是：在其他自变量不变前提下，自变量\\(X_j\\)改变一个单位，引 起的死亡风险改变的自然对数值。 式(1)可改写为： \\[ h(t,X)=h_0(t)\\exp(\\beta_1X_1)\\exp(\\beta_2X_2) \\cdots \\exp(\\beta_mX_m) \\] 故相对危险度(RR)即： \\[ \\text{RR} = \\exp\\beta_j(X_{j2}-X_{j1}) \\] 如Xj为 0~1 数据，则：\\(\\text{RR} = \\exp\\beta_j\\) ➢ RR含义：在其他自变量保持不变前提下，自变量\\(X_j\\)改变一个单位，死亡风险比原水平改变\\(\\exp(\\beta_j)\\)倍。RR是一个与时间无关的变量 。 h0(t)分布类型未作任何限定；但h(t)随变量X的变化假定为指数函数exp(bX)。故COX模型为半参数模型。而且h0(t)分布类型未作任何限定，因而应用COX模型不必考虑资料的属于那一种具体的分布。故适用范围广泛，类似于非参数方法，但其检验效率高于非参数模型，接近于参数模型。 13.1.2 COX回归模型的构建 ➢ 构造偏似然函数，然后用最大似然法求出各参数估计值bj, 须借助计算机完成。 13.1.3 COX回归模型的主要用途 （1）建立以多个危险因素估计生存或死亡的 风险模型，并由模型估计对多个 危险因素导致死亡的相对危险度（RR) （2）用已建立的模型，估计患病后随时间变化的生存率 （3）用已建立的模型，估计患病后的危险指数, 或预后指数（PI） 13.1.4 COX回归模型的应用条件 （1）已知观察对象的生存时间； （2）已知观察对象在事先确定的观察时间内，其是否发生某事件的结果； （3）自变量可以是计量资料、计数资料、分类资料或等级资料。 （4）等比例风险（PH）。指在协变量不同状态的病人的风险在不同的时间保 持不变。如在研究的10年中，糖尿病人心脏病发作的可能性是非糖尿病人的3 倍，无论在第1年，第2年…….等都如此。 等比例风险的验证 （1）按协变量分组的Kaplan-Meier生存曲线，如生存曲线明显交叉，则不满足PH假定。 （2）将协变量与时间作为交互项引入模型，如果交互项没有统计学意义，则等比例风险成立，若有统计学意义，则不成立。与时间有关的风险称为非比例风险，采用非比例风险模型分析。 13.1.5 COX回归分析的假设检验 （1）Cox回归方程的检验方法： ➢ 最大似然比检验（maximumLike-lihood Ratio) ➢常用Wald检验、得分检验（Score) 13.2 COX分析的一般步骤 13.2.1 收集资料 首先确定观察指标并将其数量化，表1（数量化表），然后收集资料，表2（随访表）。收集到资料后，建立数据文件。（用SPSS或Excel) 13.2.2 因子初步筛选 剔除数据缺失较多的因子 剔除变异几乎为0的因子 对所有因子逐个做单因素Cox模型分析，选择有统计学意义的变量做多因素Cox模型分析,此时的α值可以取稍大一些，如α=0.1. 13.2.3 拟合多因素模型 规定检验水准α，初步探索性研究，可取α=0.10或α=0.15,；严谨的、证实性研究，取α=0.05或0.01 筛选因子方法：前进法、后退法、逐步法 13.2.4 结果解析与评价 模型在一定的检验水准α下，入选哪些因素？ 入选因素哪些是保护因素，哪些是危险因素？ 入选因素哪个对因变量影响（贡献）最大？ 13.2.5 标准回归系数的计算方法 对原始变量的观察值做标准正态化变换后，在拟合回归方程得到的回归系数，即为标准化回归系数 可直接用以下公式计算： \\[ \\beta&#39;=SD · \\beta \\; \\; \\; \\; \\; \\; \\; SE(\\beta&#39;)=SD ×SE(\\beta) \\] 式中，SD为X的标准差,SE(β)为β的标准误 13.3 案例 13.3.1 案例1 以下数据是一项关于胰脏癌手术中接受放射治疗是否会延长病人生存时间的研究数据。该研究的主要终点为死亡，接受手术被定义为计算生存时间的起点。由于该研究是一项未经随机化的观察性研究，要正确估计术中接受放射治疗提高患者生存时间的效果，还需要考虑对其他因子的效果进行调整。数据的详细说明见下表。 caseno time censor age sex trt bui ch p stage 1 2.4 死亡 66 男 无术中放疗 头部以外 CH3 有 IV期 2 1.7 死亡 69 男 无术中放疗 头部以外 CH3 有 IV期 3 0.1 死亡 48 男 无术中放疗 头部以外 CH0 无 III期 4 1.0 死亡 73 男 无术中放疗 头部以外 CH3 无 III期 5 4.8 死亡 65 男 无术中放疗 头部以外 CH3 无 IV期 6 6.4 死亡 38 男 无术中放疗 胰脏头部 CH3 无 III期 代码清单 导入数据 library(foreign) library(survival) pancer &lt;- read.spss(&#39;data/pancer.sav&#39;) pancer &lt;- as.data.frame(pancer) head(pancer) 2.定义变量名 pancer$censor &lt;- ifelse(pancer$censor==&#39;死亡&#39;,1,0) pancer$Gender &lt;- as.factor(ifelse(pancer$sex==&#39;男&#39;,&quot;Male&quot;,&quot;Female&quot;)) pancer$ch &lt;- as.factor(ifelse(pancer$ch==&#39;CH3&#39;, &quot;ch&quot;,&quot;nonch&quot;)) 在这行代码 ifelse(pancer$ch=='CH3', \"ch\",\"nonch\")中，我们将ch3，作为有胆管浸润，将0、1、2实际上定义为了无胆管浸润，现在可以看看数据发生了什么改变： caseno time censor age sex trt bui ch p stage Gender 1 2.4 1 66 男 无术中放疗 头部以外 ch 有 IV期 Male 2 1.7 1 69 男 无术中放疗 头部以外 ch 有 IV期 Male 3 0.1 1 48 男 无术中放疗 头部以外 nonch 无 III期 Male 4 1.0 1 73 男 无术中放疗 头部以外 ch 无 III期 Male 5 4.8 1 65 男 无术中放疗 头部以外 ch 无 IV期 Male 6 6.4 1 38 男 无术中放疗 胰脏头部 ch 无 III期 Male 拟合模型 #pancer$ch &lt;- relevel(pancer$ch,ref=&quot;CH0&quot;) #设置因子的参照水平 #pancer$ch&lt;- factor(pancer$ch,order=TRUE) #设置为等级变量 #options(contrasts=c(&quot;contr.treatment&quot;, &quot;contr.treatment&quot;)) #指定等级变量的参照水平 #pancer$Gender &lt;- relevel(pancer$Gender,ref=&#39;Female&#39;) f&lt;-coxph(Surv(time,censor==1)~age+Gender+trt+bui+ch+p+stage,data=pancer) summary(f) Call: coxph(formula = Surv(time, censor == 1) ~ age + Gender + trt + bui + ch + p + stage, data = pancer) n= 83, number of events= 82 coef exp(coef) se(coef) z Pr(&gt;|z|) age 0.01575 1.01588 0.01322 1.191 0.23356 GenderMale 0.07528 1.07819 0.24358 0.309 0.75727 trt有术中放疗 -0.88487 0.41277 0.32113 -2.756 0.00586 ** bui头部以外 0.45223 1.57181 0.29462 1.535 0.12480 chnonch 0.20457 1.22699 0.30375 0.673 0.50065 p有 0.29321 1.34072 0.27587 1.063 0.28784 stageIV期 0.23301 1.26240 0.25688 0.907 0.36437 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 exp(coef) exp(-coef) lower .95 upper .95 age 1.0159 0.9844 0.9899 1.0426 GenderMale 1.0782 0.9275 0.6689 1.7379 trt有术中放疗 0.4128 2.4227 0.2200 0.7746 bui头部以外 1.5718 0.6362 0.8823 2.8002 chnonch 1.2270 0.8150 0.6765 2.2254 p有 1.3407 0.7459 0.7808 2.3023 stageIV期 1.2624 0.7921 0.7630 2.0886 Concordance= 0.646 (se = 0.036 ) Likelihood ratio test= 14.5 on 7 df, p=0.04 Wald test = 14.84 on 7 df, p=0.04 Score (logrank) test = 15.2 on 7 df, p=0.03 结果解读1.1 exp(coef)即回归模型的反对数，可以将其近似的看做RR值。 Likelihood ratio test; Wald test; Score (logrank) test 即回归模型的似然比检验、瓦尔德检验以及比分检验的结果、自由度和P值。 Concordance= 0.646 即C统计量 计算C统计量的95%可信区间 公式如下： \\[ \\text{95%CI}= \\text{C-index} \\pm 1.96\\text{SE(c)} \\] sum.surv&lt;-summary(f) c_index&lt;-sum.surv$concordance c_index ## C se(C) ## 0.64582715 0.03617383 0.64582715+1.96*0.03617383 #95%CI上限 ## [1] 0.7167279 0.64582715-1.96*0.03617383 #95%CI下限 ## [1] 0.5749264 结果解读 1.2 C统计量的95%CI为：0.03617383~0.03617383 13.3.2 案例2 随访25例分别以A 、B 治疗方法治疗的癌症病人的生存情况，资料如表15-3 所示，+为截尾值。1 : 有肾功能损害， 0: 无肾功能损害 查看原始数据 ## The following object is masked from glioma (pos = 3): ## ## group ## The following object is masked from glioma (pos = 6): ## ## group group renal days censor 1 1 8 1 1 0 52 1 1 1 58 1 1 1 63 1 1 1 63 1 1 0 220 1 代码如下 library(survival) example15_4 &lt;- read.table (&quot;data/example15_4.csv&quot;, header=TRUE, sep=&quot;,&quot;) attach(example15_4) coxmodel &lt;- coxph(Surv(days, censor)~group) summary(coxmodel) ## Call: ## coxph(formula = Surv(days, censor) ~ group) ## ## n= 25, number of events= 20 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## group 0.3652 1.4408 0.4592 0.795 0.426 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## group 1.441 0.694 0.5858 3.544 ## ## Concordance= 0.536 (se = 0.064 ) ## Likelihood ratio test= 0.64 on 1 df, p=0.4 ## Wald test = 0.63 on 1 df, p=0.4 ## Score (logrank) test = 0.64 on 1 df, p=0.4 coxmode2 &lt;- coxph(Surv(days, censor)~group+renal) summary(coxmode2) ## Call: ## coxph(formula = Surv(days, censor) ~ group + renal) ## ## n= 25, number of events= 20 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## group 0.9835 2.6738 0.5231 1.880 0.060068 . ## renal 4.2718 71.6515 1.1588 3.686 0.000228 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## group 2.674 0.37399 0.9592 7.454 ## renal 71.652 0.01396 7.3930 694.432 ## ## Concordance= 0.773 (se = 0.055 ) ## Likelihood ratio test= 23.82 on 2 df, p=7e-06 ## Wald test = 14.33 on 2 df, p=8e-04 ## Score (logrank) test = 31.13 on 2 df, p=2e-07 anova(coxmodel,coxmode2) ## Analysis of Deviance Table ## Cox model: response is Surv(days, censor) ## Model 1: ~ group ## Model 2: ~ group + renal ## loglik Chisq Df P(&gt;|Chi|) ## 1 -52.715 ## 2 -41.122 23.186 1 1.471e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 detach(example15_4) 13.3.3 案例3 library(TH.data) data(&#39;GBSG2&#39;,package = &#39;TH.data&#39;) attach(GBSG2) head(GBSG2) ## horTh age menostat tsize tgrade pnodes progrec estrec time cens ## 1 no 70 Post 21 II 3 48 66 1814 1 ## 2 yes 56 Post 12 II 7 61 77 2018 1 ## 3 yes 58 Post 35 II 9 52 271 712 1 ## 4 yes 59 Post 17 II 4 60 29 1807 1 ## 5 no 73 Post 35 II 1 26 65 772 1 ## 6 no 32 Pre 57 III 24 0 13 448 1 # ff&lt;-survfit(Surv(time, cens)~horTh,data = GBSG2) # summary(ff) plot(survfit(Surv(time, cens)~horTh,data = GBSG2),lty = c(2,1), col = c(2,1), mark.time = T) legend(&#39;bottomright&#39;, legend = c(&#39;yes&#39;,&#39;no&#39;), lty = c(2,1), col = c(2,1)) 结果解读 2.1 我们可以通过summary()函数可以看到预后情况，寻找中位生存时间，确定不同组的生存情况 也可以画一条生存曲线 coxreg &lt;- coxph(Surv(time,cens)~.,data = GBSG2) #~后加&quot;.&quot;指加入所有的变量 summary(coxreg) ## Call: ## coxph(formula = Surv(time, cens) ~ ., data = GBSG2) ## ## n= 686, number of events= 299 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## horThyes -0.3462784 0.7073155 0.1290747 -2.683 0.007301 ** ## age -0.0094592 0.9905854 0.0093006 -1.017 0.309126 ## menostatPost 0.2584448 1.2949147 0.1834765 1.409 0.158954 ## tsize 0.0077961 1.0078266 0.0039390 1.979 0.047794 * ## tgrade.L 0.5512988 1.7355056 0.1898441 2.904 0.003685 ** ## tgrade.Q -0.2010905 0.8178384 0.1219654 -1.649 0.099199 . ## pnodes 0.0487886 1.0499984 0.0074471 6.551 5.7e-11 *** ## progrec -0.0022172 0.9977852 0.0005735 -3.866 0.000111 *** ## estrec 0.0001973 1.0001973 0.0004504 0.438 0.661307 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## horThyes 0.7073 1.4138 0.5492 0.9109 ## age 0.9906 1.0095 0.9727 1.0088 ## menostatPost 1.2949 0.7723 0.9038 1.8553 ## tsize 1.0078 0.9922 1.0001 1.0156 ## tgrade.L 1.7355 0.5762 1.1963 2.5178 ## tgrade.Q 0.8178 1.2227 0.6439 1.0387 ## pnodes 1.0500 0.9524 1.0348 1.0654 ## progrec 0.9978 1.0022 0.9967 0.9989 ## estrec 1.0002 0.9998 0.9993 1.0011 ## ## Concordance= 0.692 (se = 0.015 ) ## Likelihood ratio test= 104.8 on 9 df, p=&lt;2e-16 ## Wald test = 114.8 on 9 df, p=&lt;2e-16 ## Score (logrank) test = 120.7 on 9 df, p=&lt;2e-16 13.4 列线图绘制 载入数据 library(foreign) library(survival) library(rms) pancer &lt;- read.spss(&#39;data/pancer.sav&#39;) pancer &lt;- as.data.frame(pancer) head(pancer) ## caseno time censor age sex trt bui ch p stage ## 1 1 2.4 死亡 66 男 无术中放疗 头部以外 CH3 有 IV期 ## 2 2 1.7 死亡 69 男 无术中放疗 头部以外 CH3 有 IV期 ## 3 3 0.1 死亡 48 男 无术中放疗 头部以外 CH0 无 III期 ## 4 4 1.0 死亡 73 男 无术中放疗 头部以外 CH3 无 III期 ## 5 5 4.8 死亡 65 男 无术中放疗 头部以外 CH3 无 IV期 ## 6 6 6.4 死亡 38 男 无术中放疗 胰脏头部 CH3 无 III期 重命名变量 pancer$censor &lt;- ifelse(pancer$censor==&#39;死亡&#39;,1,0) pancer$Gender &lt;- as.factor(ifelse(pancer$sex==&#39;男&#39;,&quot;Male&quot;,&quot;Female&quot;)) pancer$ch &lt;- as.factor(ifelse(pancer$ch==&#39;CH3&#39;, &quot;ch&quot;,&quot;nonch&quot;))#将等级变量二值化 也可使用这种方式设置哑变量 #pancer$ch1 &lt;- as.factor(ifelse(pancer$ch==&#39;CH1&#39;,1,0)) #pancer$ch2 &lt;- as.factor(ifelse(pancer$ch==&#39;CH2&#39;,1,0)) #pancer$ch3 &lt;- as.factor(ifelse(pancer$ch==&#39;CH3&#39;,1,0)) #pancer$ch0 &lt;- as.factor(ifelse(pancer$ch==&#39;CH0&#39;,1,0)) #pancer$ch &lt;- relevel(pancer$ch,ref=&quot;CH0&quot;) #设置因子的参照水平 #pancer$ch&lt;- factor(pancer$ch,order=TRUE) #设置为等级变量 #options(contrasts=c(&quot;contr.treatment&quot;, &quot;contr.treatment&quot;)) #指定等级变量的参照水平 #pancer$Gender &lt;- relevel(pancer$Gender,ref=&#39;Female&#39;) #打包数据 dd&lt;-datadist(pancer) options(datadist=&#39;dd&#39;) coxm1 &lt;- cph(Surv(time,censor==1)~age+Gender+trt+bui+ch+p+stage, x=T,y=T, data=pancer, surv=T)#rms包中的cox回归 coxm1 ## Cox Proportional Hazards Model ## ## cph(formula = Surv(time, censor == 1) ~ age + Gender + trt + ## bui + ch + p + stage, data = pancer, x = T, y = T, surv = T) ## ## Model Tests Discrimination ## Indexes ## Obs 83 LR chi2 14.50 R2 0.160 ## Events 82 d.f. 7 Dxy 0.292 ## Center 1.0416 Pr(&gt; chi2) 0.0429 g 0.543 ## Score chi2 15.20 gr 1.721 ## Pr(&gt; chi2) 0.0335 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## age 0.0158 0.0132 1.19 0.2336 ## Gender=Male 0.0753 0.2436 0.31 0.7573 ## trt=有术中放疗 -0.8849 0.3211 -2.76 0.0059 ## bui=头部以外 0.4522 0.2946 1.53 0.1248 ## ch=nonch 0.2046 0.3038 0.67 0.5007 ## p=有 0.2932 0.2759 1.06 0.2878 ## stage=IV期 0.2330 0.2569 0.91 0.3644 ## surv &lt;- Survival(coxm1) # 定义时间分割的范围，以月为单位 surv1 &lt;- function(x)surv(1*3,lp=x) surv2 &lt;- function(x)surv(1*6,lp=x) surv3 &lt;- function(x)surv(1*12,lp=x) nom1&lt;-nomogram(coxm1,fun=list(surv1,surv2,surv3), lp= F, funlabel=c( &#39;3-Month Survival probability&#39;, &#39;6-Month survival probability&#39;, &#39;12-Month survival probability&#39;), maxscale=100, fun.at=c(&#39;0.9&#39;,&#39;0.85&#39;,&#39;0.80&#39;,&#39;0.70&#39;, &#39;0.6&#39;,&#39;0.5&#39;,&#39;0.4&#39;,&#39;0.3&#39;,&#39;0.2&#39;,&#39;0.1&#39;) ) # fun：将线性的预测转换为具体的分值 # lp= F：不显示线性预测值，只显示最后的分值 # funlabel：不同时间的标签 # maxscale=100 刻度为0~100 # fun.at设置最后结果的范围 plot(nom1,xfrac=.60)#xfrac为文字和图的距离 从图中我们可以看到，Gender和ch两个变量对模型的意义不大，我们可以尝试删除这两个变量。 拟合一个新的模型 coxm2 &lt;- cph(Surv(time,censor==1)~age+trt+bui+p+stage, x=T,y=T,data=pancer,surv=T) coxm2 ## Cox Proportional Hazards Model ## ## cph(formula = Surv(time, censor == 1) ~ age + trt + bui + p + ## stage, data = pancer, x = T, y = T, surv = T) ## ## Model Tests Discrimination ## Indexes ## Obs 83 LR chi2 13.75 R2 0.153 ## Events 82 d.f. 5 Dxy 0.305 ## Center 0.9655 Pr(&gt; chi2) 0.0173 g 0.518 ## Score chi2 14.53 gr 1.679 ## Pr(&gt; chi2) 0.0125 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## age 0.0161 0.0132 1.22 0.2225 ## trt=有术中放疗 -0.7672 0.2732 -2.81 0.0050 ## bui=头部以外 0.4224 0.2905 1.45 0.1460 ## p=有 0.2987 0.2615 1.14 0.2533 ## stage=IV期 0.2830 0.2436 1.16 0.2452 ## 我们可以看到c统计量实际上是有轻微的上升的，故用这个(coxm2)模型来绘制列线图 surv &lt;- Survival(coxm2) # 定义时间分割的范围，以月为单位 surv1 &lt;- function(x)surv(1*3,lp=x) surv2 &lt;- function(x)surv(1*6,lp=x) surv3 &lt;- function(x)surv(1*12,lp=x) #绘制列线图 nom2&lt;-nomogram(coxm2, fun=list(surv1,surv2,surv3), lp= F, funlabel=c(&#39;3-Month Survival probability&#39;, &#39;6-Month survival probability&#39;, &#39;12-Month survival probability&#39;), maxscale=100, fun.at=c(&#39;0.9&#39;,&#39;0.85&#39;,&#39;0.80&#39;,&#39;0.70&#39;,&#39;0.6&#39;, &#39;0.5&#39;,&#39;0.4&#39;,&#39;0.3&#39;,&#39;0.2&#39;,&#39;0.1&#39;,&#39;0.01&#39;)) plot(nom2) # 绘制校正曲线 cal2 &lt;- calibrate(coxm2, cmethod=&#39;KM&#39;, method=&#39;boot&#39;, u=6, m=20, B=1000) ## Using Cox survival estimates at 2.5 Days #u=6:时间间隔 #m=20：总样本量为83，设为20，就有4个节点，一般设置3~4个节点 plot(cal2, lwd=2,lty=1, errbar.col=c(rgb(0,118,192,maxColorValue=255)), xlim=c(0,1),ylim=c(0,1), xlab=&quot;Nomogram-Predicted Probabilityof 6 m OS&quot;, ylab=&quot;Actual 6 m OS (proportion)&quot;, col=c(rgb(192,98,83,maxColorValue=255))) 13.5 亚组分析森林图的绘制 library(forestplot) test_forest &lt;- read.csv(&#39;data/forest_test2.csv&#39;,header = FALSE) attach(test_forest) forestplot(labeltext = as.matrix(test_forest[,1:6]), #设置用于文本展示的列，此处我们用数据的前六列作为文本，在图中展示 mean = test_forest$V7, #设置均值 lower = test_forest$V8, #设置均值的下限 upper = test_forest$V9, #设置均值的上限 is.summary = c(T,T,F,F,T,F,F,T,F,F,F,F,T,F,F,F,T,F,T,F,F,T), #该参数接受一个逻辑向量，用于定义数据中的每一行是否是汇总值，若是，则在对应位置设置为TRUE，若否，则设置为FALSE；设置为TRUE的行则以粗体出现 zero = 1, #设置参照值，此处我们展示的是HR值，故参照值是1，而不是0 boxsize = 0.2, #设置点估计的方形大小 lineheight = unit(10,&#39;mm&#39;),#设置图形中的行距 colgap = unit(2,&#39;mm&#39;),#设置图形中的列间距 lwd.zero = 2,#设置参考线的粗细 lwd.ci = 2,#设置区间估计线的粗细 lwd.xaxis=2,#设置X轴线的粗细 xlog=FALSE, ci.vertices.height = 0.1, clip = c(0.2,1.3), # 设置森林图展示的可信区间范围，超过的部分用箭头展示 grid = FALSE, lty.ci = 1, col=fpColors(box=&#39;#458B00&#39;, summary= &quot;#8B008B&quot;,lines = &#39;black&#39;,zero = &#39;#7AC5CD&#39;), #使用fpColors()函数定义图形元素的颜色，从左至右分别对应点估计方形，汇总值，区间估计线，参考线 xlab=&quot;Hzard Ratio(HR)&quot;,#设置x轴标签 graph.pos = 5)#设置森林图的位置，此处设置为5，则出现在第五列 注意：数据过多时，一张图可能画不下，故可以将数据分割，分成几次绘制。 "],["竞争风险模型.html", "Chapter 14 竞争风险模型 14.1 竞争风险的概念 14.2 单因素竞争风险模型 14.3 多因素竞争风险模型", " Chapter 14 竞争风险模型 14.1 竞争风险的概念 ➢ 在观察某事件发生的时间，如果该事件被其他事件阻碍，即存在竞争风险 ➢ 【举例】：研究骨髓移植对比血液移植治疗白血病的疗效，结局定义为“复发”，假定患者移植后不幸因为移植不良反应死亡，那这些发生移植相关死亡的患者就无法观察到“复发”的终点，也就是说“移植相关死亡”与“复发”存在竞争风险。 ➢ 恶性肿瘤预后研究中这种例子很常见。 14.2 单因素竞争风险模型 1. 载入数据，并且将“D-疾病”这一列转化为因子变量 library(foreign) bmt &lt;-read.csv(&#39;data/bmtcrr.csv&#39;) head(bmt) ## Sex D Phase Age Status Source ftime ## 1 M ALL Relapse 48 2 BM+PB 0.67 ## 2 F AML CR2 23 1 BM+PB 9.50 ## 3 M ALL CR3 7 0 BM+PB 131.77 ## 4 F ALL CR2 26 2 BM+PB 24.03 ## 5 F ALL CR2 36 2 BM+PB 1.47 ## 6 M ALL Relapse 17 2 BM+PB 2.23 bmt$D &lt;- as.factor(bmt$D) 加载程辑包，未下载的包可以使用install.package()下载。 library(survival) library(cmprsk) library(splines) 3. 使用cuminc()函数拟合模型 attach(bmt) ## The following object is masked from breslow.dat: ## ## Age crmod &lt;- cuminc(ftime,Status,D) crmod ## Tests: ## stat pv df ## 1 2.8623325 0.09067592 1 ## 2 0.4481279 0.50322531 1 ## Estimates and Variances: ## $est ## 20 40 60 80 100 120 ## ALL 1 0.3713851 0.3875571 0.3875571 0.3875571 0.3875571 0.3875571 ## AML 1 0.2414530 0.2663827 0.2810390 0.2810390 0.2810390 NA ## ALL 2 0.3698630 0.3860350 0.3860350 0.3860350 0.3860350 0.3860350 ## AML 2 0.4439103 0.4551473 0.4551473 0.4551473 0.4551473 NA ## ## $var ## 20 40 60 80 100 120 ## ALL 1 0.003307032 0.003405375 0.003405375 0.003405375 0.003405375 0.003405375 ## AML 1 0.001801156 0.001995487 0.002130835 0.002130835 0.002130835 NA ## ALL 2 0.003268852 0.003373130 0.003373130 0.003373130 0.003373130 0.003373130 ## AML 2 0.002430406 0.002460425 0.002460425 0.002460425 0.002460425 NA 结果解读1 $est :列出了不同分组病人在不同时间点发生结局事件(1)和竞争事件(2)的率， 我们可以看到，1的发生率和2的发生率实质上并没有什么差别。 3.画图 plot(crmod,xlab = &#39;Month&#39;, ylab = &#39;CIF&#39;,lwd=2,lty=1, col = c(&#39;#FF6699&#39;,&#39;#0066CC&#39;,&#39;#FFCC00&#39;,&#39;#99CC33&#39;)) 14.3 多因素竞争风险模型 在上部分的分析中，我们相当于只做了一个单因素的分析，如果要探讨每一种因素对“结局”的作用，可以使用crr()函数来构建一个多因素竞争风险模型。 1. 整理数据：由于crr()函数需要数据为数据框类型的数据，所以在这里我们需要重新整理一下数据。 cov1 &lt;- data.frame(age = bmt$Age, sex_F = ifelse(bmt$Sex==&#39;F&#39;,1,0), dis_AML = ifelse(bmt$D==&#39;AML&#39;,1,0), phase_cr1 = ifelse(bmt$Phase==&#39;CR1&#39;,1,0), phase_cr2 = ifelse(bmt$Phase==&#39;CR2&#39;,1,0), phase_cr3 = ifelse(bmt$Phase==&#39;CR3&#39;,1,0), source_PB = ifelse(bmt$Source==&#39;PB&#39;,1,0)) ## 手动设置哑变量 head(cov1) ## age sex_F dis_AML phase_cr1 phase_cr2 phase_cr3 source_PB ## 1 48 0 0 0 0 0 0 ## 2 23 1 1 0 1 0 0 ## 3 7 0 0 0 0 1 0 ## 4 26 1 0 0 1 0 0 ## 5 36 1 0 0 1 0 0 ## 6 17 0 0 0 0 0 0 结果解读 2.1 我们可以看到，经过数据转换，生成了一个新的数据框cov1，在这个数据框中，性别由“1”和“0”表示。白血病类型以及移植类型也使用“1”和“0”表示。 需要注意的是，在phase这个变量中，有四种状态，如果粗暴的将其定义为“1~4”，计算机会将其作为等级资料处理，实际上他并没有顺序，所以我们还是使用老办法将其设置为协变量。 2. 拟合模型 mod1 &lt;- crr(bmt$ftime, bmt$Status, cov1, failcode=1, cencode=0) summary(mod1) ## Competing Risks Regression ## ## Call: ## crr(ftime = bmt$ftime, fstatus = bmt$Status, cov1 = cov1, failcode = 1, ## cencode = 0) ## ## coef exp(coef) se(coef) z p-value ## age -0.0185 0.982 0.0119 -1.554 0.1200 ## sex_F -0.0352 0.965 0.2900 -0.122 0.9000 ## dis_AML -0.4723 0.624 0.3054 -1.547 0.1200 ## phase_cr1 -1.1018 0.332 0.3764 -2.927 0.0034 ## phase_cr2 -1.0200 0.361 0.3558 -2.867 0.0041 ## phase_cr3 -0.7314 0.481 0.5766 -1.268 0.2000 ## source_PB 0.9211 2.512 0.5530 1.666 0.0960 ## ## exp(coef) exp(-coef) 2.5% 97.5% ## age 0.982 1.019 0.959 1.005 ## sex_F 0.965 1.036 0.547 1.704 ## dis_AML 0.624 1.604 0.343 1.134 ## phase_cr1 0.332 3.009 0.159 0.695 ## phase_cr2 0.361 2.773 0.180 0.724 ## phase_cr3 0.481 2.078 0.155 1.490 ## source_PB 2.512 0.398 0.850 7.426 ## ## Num. cases = 177 ## Pseudo Log-likelihood = -267 ## Pseudo likelihood ratio test = 24.4 on 7 df, 结果解读 2.2 这个模型的结果展示和COX回归模型十分相似。我们可以看到每个不同因素的显著性，以及他们的 exp(coef)即HR值。 在这个模型中，phase_cr1、phase_cr2是有意义的，由于这是我们设置的phase这个变量对应的哑变量，所以我们可以说phase这个变量对结果是有意义的。 以source_PB为例，它的exp(coef)=2.512，说明控制了移植相关死亡的竞争风险之后，接受PB这种骨髓移植方式的病人发生定义的终点（复发）的风险比接受BM+PB这种方式发生终点的风险高2.512倍。 3. 我们可以做一个卡方（瓦尔德）检验，看看拟合的模型是否有意义 由于只有phase一个变量有意义，所以我们只纳入数据框的4~6列，也就是phase_cr1、phase_cr2以及phase_cr3。 library(aod) ## ## 载入程辑包：&#39;aod&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## rats wald.test(mod1$var,mod1$coef,Terms = 4:6) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 14.0, df = 3, P(&gt; X2) = 0.0029 结果解读2.3 我们可以看到这个模型和空模型比较，p=0.0029，说明是有意义的。 "],["主成分分析.html", "Chapter 15 主成分分析 15.1 概述 15.2 主成分分析的数学模型 15.3 PCA的R语言实现 15.4 案例", " Chapter 15 主成分分析 15.1 概述 15.1.1 基本思想 基本思想——降维 ➢ 主成分分析的基本思想就是将彼此相关的一组指标变量转化为彼此独立的一组新的指标变量，并用其中较少的几个新指标变量，综合反映原多个指标变量中所包含的主要信息，符合专业含义。 ➢ 何为主成分? 简而言之，主成分实际上就是由原变量X1~Xm线性组合出来的m个互不相关、且未丢失任何信息的新变量，也称为综合变量。多指标的主成分分析常被用来寻找判断某种事物或现象的综合指标，并给综合指标所蕴藏的信息以恰当的解释，以便更深刻地揭示事物内在的规律。 15.1.2 模型示意图 ➢ 主成分分析（PCA）是一种数据降维技巧，它能将大量相关变量转化为一组少的不相关变量，这些无关变量称为主成分。使用PCA可将30个相关（很可能冗余）的环境变量转化为5个无关的成分变量，并且尽可能保留原始数据集的信息。 15.1.3 主成分分析基本思想应用 ➢ 比如描述儿童生长发育的指标中，身高、腿长和臂长这3个指标可能是相关的，而胸围、大腿围和臂围这3个围度指标也会有一定的相关性。如果分别用每一个指标对儿童的生长发育做出评价，那么这种评价就是孤立的、片面的，而不是综合的。仅选用几个“重要的”或“有代表性”的指标来评价，就可能失去许多有用的信息，容易得出片面的结论。我们需要一种综合性的分析方法，既可减少指标变量个数，又尽量不损失原指标变量所包含的信息，对资料进行综合分析。 15.2 主成分分析的数学模型 假设原变量指标为\\(x_1\\), \\(x_2\\), …, \\(x_k\\), 经过标准化后得到标准指标变量\\(X_1\\), \\(X_2\\)，…，\\(X_K\\): \\[ X_j=\\cfrac{x_j-\\bar x_j}{s_j}\\;\\;, \\;\\; j=1,2,\\cdots ,k \\] 其中，\\(x_j\\)是第\\(j\\)个指标变量的均值,\\(s_j\\)是第\\(j\\)个指标变量的标准差。它们的综合指标(新变量指标)为\\(z_1, z_2, …, z_m, (m&lt;k)\\) 则进行线性变换: \\[ z_1=l_{11}X_1+l_{12}X_2+\\cdots +l_{1k}X_k \\] \\[ z_2=l_{21}X_1+l_{22}X_2+\\cdots +l_{2k}X_k \\] \\[ z_k=l_{k1}X_1+l_{k2}X_2+\\cdots +l_{kk}X_k \\] 将\\(k\\)个标准指标变量\\(x_1, x_2, …, x_k\\),转换成了\\(k\\)个新变量\\(z_1, z_2, …, z_k\\) 15.3 PCA的R语言实现 15.3.1 psych包中的函数 ➢ R基础安装包提供了PCA和EFA的函数，分别是princomp()和factanal()。本章将重点介绍psych包中提供的函数。它们提供了比基础函数更丰富和有用的选项。 15.3.2 主成分与因子分析的一般步骤 数据预处理。 PCA和EFA都根据观测变量间的相关性来推导结果。用户可以输入原始数据矩阵或者相关系数矩阵到principal()和fa()函数中。若输入初始数据，相关系数矩阵将会被自动计算，在计算前请确保数据中没有缺失值。 选择因子模型。 判断是PCA（数据降维）还是EFA（发现潜在结构）更符合你的研究目标。如果选择EFA方法，你还需要选择一种估计因子模型的方法（如最大似然估计）。 判断要选择的主成分/因子数目。 选择主成分/因子。 旋转主成分/因子。 解释结果。 计算主成分或因子得分。 15.4 案例 15.4.1 案例1 ➢ 数据集USJudgeRatings包含了律师对美国高等法院法官的评分。数据框包含43个观测，12个变量。表14-2列出了所有的变量。 设置种子数 par(ask=TRUE) set.seed(1234) # make results reproducible 判断主成分的个数 library(psych) ## ## 载入程辑包：&#39;psych&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## describe ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha ## The following object is masked from &#39;package:HH&#39;: ## ## logit ## The following object is masked from &#39;package:car&#39;: ## ## logit fa.parallel(USJudgeRatings[,-1], fa=&quot;pc&quot;, n.iter=100, show.legend=FALSE, main=&quot;Scree plot with parallel analysis&quot;) ## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, : ## The estimated weights for the factor scores are probably incorrect. Try a ## different factor score estimation method. ## Parallel analysis suggests that the number of factors = NA and the number of components = 1 abline(h=1,lwd=1,col=&quot;green&quot;) 结果解读1.1 展示了基于观测特征值的碎石检验（由线段和x符号组成）、根据100个随机数据矩阵推导出来的特征值均值（虚线），以及大于1的特征值准则（y=1的水平线）。 图评价美国法官评分中要保留的主成分个数。碎石图（直线与x符号）、特征值大于1准则（水平线）和100次模拟的平行分析（虚线）都表明保留一个主成分即可 提取主成分 principal()函数可以根据原始数据矩阵或者相关系数矩阵做主成分分析。格式 为：principal(r, nfactors=, rotate=, scores=) 其中： ➢r 是相关系数矩阵或原始数据矩阵； ➢ nfactors设定主成分数（默认为1）； ➢ rotate指定旋转的方法（默认最大方差旋转（varimax）； ➢ scores设定是否需要计算主成分得分（默认不需要） pc &lt;- principal(USJudgeRatings[,-1], nfactors=1) pc ## Principal Components Analysis ## Call: principal(r = USJudgeRatings[, -1], nfactors = 1) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 h2 u2 com ## INTG 0.92 0.84 0.1565 1 ## DMNR 0.91 0.83 0.1663 1 ## DILG 0.97 0.94 0.0613 1 ## CFMG 0.96 0.93 0.0720 1 ## DECI 0.96 0.92 0.0763 1 ## PREP 0.98 0.97 0.0299 1 ## FAMI 0.98 0.95 0.0469 1 ## ORAL 1.00 0.99 0.0091 1 ## WRIT 0.99 0.98 0.0196 1 ## PHYS 0.89 0.80 0.2013 1 ## RTEN 0.99 0.97 0.0275 1 ## ## PC1 ## SS loadings 10.13 ## Proportion Var 0.92 ## ## Mean item complexity = 1 ## Test of the hypothesis that 1 component is sufficient. ## ## The root mean square of the residuals (RMSR) is 0.04 ## with the empirical chi square 6.21 with prob &lt; 1 ## ## Fit based upon off diagonal values = 1 结果解读 1.2 第一主成分（PC1）与每个变量都高度相关，也就是说，它是一个可用来进行一般性评价的维度。h2栏指成分公因子方差，即主成分对每个变量的方差解释度。u2栏指成分唯一性，即方差无法被主成分解释的比例（1–h2）。 例如，体能（PHYS）80%的方差都可用第一主成分来解释，20%不能。相比而言，PHYS是用第一主成分表示性最差的变量。SS loadings行包含了与主成分相关联的特征值，指的是与特定主成分 相关联的标准化后的方差值（本例中，第一主成分的值为10）。 最后，Proportion Var行表示的是每个主成分对整个数据集的解释程度。此处可以看到，第一主成分解释了11个变量92%的方差。 15.4.2 案例 2 Harman23.cor数据集包含305个女孩的8个身体测量指标。本例中，数据集由变量的相关系数组成，而不是原始数据集（见表14-3）。 判断主成分的个数 library(psych) fa.parallel(Harman23.cor$cov, n.obs=302, fa=&quot;pc&quot;, n.iter=100, show.legend=FALSE, main=&quot;Scree plot with parallel analysis&quot;) ## Parallel analysis suggests that the number of factors = NA and the number of components = 2 abline(h=1,lwd=1,col=&quot;green&quot;) 结果解读 2.1 碎石图（直线和x符号）、特征值大于1准则（水平线）和100次模拟（虚线）的平行分析建议保留两个主成分 提取主成分 PC &lt;- principal(Harman23.cor$cov, nfactors=2, rotate=&quot;none&quot;) PC ## Principal Components Analysis ## Call: principal(r = Harman23.cor$cov, nfactors = 2, rotate = &quot;none&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## height 0.86 -0.37 0.88 0.123 1.4 ## arm.span 0.84 -0.44 0.90 0.097 1.5 ## forearm 0.81 -0.46 0.87 0.128 1.6 ## lower.leg 0.84 -0.40 0.86 0.139 1.4 ## weight 0.76 0.52 0.85 0.150 1.8 ## bitro.diameter 0.67 0.53 0.74 0.261 1.9 ## chest.girth 0.62 0.58 0.72 0.283 2.0 ## chest.width 0.67 0.42 0.62 0.375 1.7 ## ## PC1 PC2 ## SS loadings 4.67 1.77 ## Proportion Var 0.58 0.22 ## Cumulative Var 0.58 0.81 ## Proportion Explained 0.73 0.27 ## Cumulative Proportion 0.73 1.00 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## ## Fit based upon off diagonal values = 0.99 结果解读 2.2 第一主成分解释了身体测量指标58%的方差，而第二主成分解释了22%，两者总共解释了81%的方差。对于高度变量，两者则共解释了其88%的方差。 载荷阵解释了成分和因子的含义。第一主成分与每个身体测量指标都正相关，看起来似乎是一个一般性的衡量因子；第二主成分与前四个变量（height、arm.span、forearm和lower.leg）负相关，与后四个变量（weight、bitro.diameter、chest.girth和chest.width）正相关 主成分旋转 rc &lt;- principal(Harman23.cor$cov, nfactors=2, rotate=&quot;varimax&quot;) rc ## Principal Components Analysis ## Call: principal(r = Harman23.cor$cov, nfactors = 2, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## height 0.90 0.25 0.88 0.123 1.2 ## arm.span 0.93 0.19 0.90 0.097 1.1 ## forearm 0.92 0.16 0.87 0.128 1.1 ## lower.leg 0.90 0.22 0.86 0.139 1.1 ## weight 0.26 0.88 0.85 0.150 1.2 ## bitro.diameter 0.19 0.84 0.74 0.261 1.1 ## chest.girth 0.11 0.84 0.72 0.283 1.0 ## chest.width 0.26 0.75 0.62 0.375 1.2 ## ## RC1 RC2 ## SS loadings 3.52 2.92 ## Proportion Var 0.44 0.37 ## Cumulative Var 0.44 0.81 ## Proportion Explained 0.55 0.45 ## Cumulative Proportion 0.55 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## ## Fit based upon off diagonal values = 0.99 结果解读 2.3 列的名字都从PC变成了RC，以表示成分被旋转。 观察RC1栏的载荷，你可以发现第一主成分主要由前四个变量来解释（长度变量）。RC2栏的载荷表示第二主成分主要由变量5到变量8来解释（容量变量）。 注意两个主成分仍不相关，对变量的解释性不变，这是因为变量的群组没有发生变化。另外，两个主成分旋转后的累积方差解释性没有变化（81%），变的只是各个主成分对方差的解释度（成分1从58%变为44%，成分2从22%变为37%）。 ◎ 获取主成分得分 在美国法官评分例子中，我们根据原始数据中的 11个评分变扯提取了一个主成分. 利用principal()函数，你很容易获得每个调查对象在该主成分上的得分 pc &lt;- principal(USJudgeRatings[,-1], nfactors=1, score=TRUE) head(pc$scores) ## PC1 ## AARONSON,L.H. -0.1857981 ## ALEXANDER,J.M. 0.7469865 ## ARMENTANO,A.J. 0.0704772 ## BERDON,R.I. 1.1358765 ## BRACKEN,J.J. -2.1586211 ## BURNS,E.B. 0.7669406 当scores = TRUE时,主成分得分存储在principal()函数返回对象的 scores元素中。如果有需要，你还可以获得律师与法官的接触频数与法官评分间的相关系数： cor(USJudgeRatings$CONT, pc$score) ## PC1 ## [1,] -0.008815895 显然，律师与法官的熟捻度与律师的评分毫无关联 当主成分分析基于相关系数矩阵时，原始数据便不可用了，也不可能获取每个观测的主成分得分，但是你可以得到用来计算主成分得分的系数。 在身体测量数据中，你有各个身体测抵指标间的相关系数，但是没有305个女孩的个体测量值。按照如下代码, 你可得到得分系数 rc &lt;- principal(Harman23.cor$cov, nfactors=2, rotate=&quot;varimax&quot;) round(unclass(rc$weights), 2) ## RC1 RC2 ## height 0.28 -0.05 ## arm.span 0.30 -0.08 ## forearm 0.30 -0.09 ## lower.leg 0.28 -0.06 ## weight -0.06 0.33 ## bitro.diameter -0.08 0.32 ## chest.girth -0.10 0.34 ## chest.width -0.04 0.27 利用如下公式可得到主成分得分： PC1 = 0.28*height + 0.30*arm.span + 0.30*forearm + 0.29 *lower.leg - 0.06*weight - 0.08*bitro.diameter - 0.10*chest.girth - 0.04*chest.width 和： PC2 = 0.05*height - 0.08*arm.span - 0.09*forearm - 0.06 *lower.leg + 0.33*weight + 0.32*bitro.diameter + 0.34*chest.girth + 0.27*chest.width 两个等式都假定身体测量指标都已标准化 (mean=O , sd= I) 注意，体重在PCI 上的系数约0.30. 对于PC2也是。从实际角度考虑，你可以进一步简化方法，将第一主成分看作前四个变量标准化得分的均值；类似地，将第二主成分看作后四个变 标准化得分的均值，这正是通常在实际中采用的方法 15.4.3 案例3——在线性回归中的应用 #install.packages(&quot;car&quot;) library(car) example16_2 &lt;- read.table (&quot;data/example16_2.csv&quot;, header=TRUE, sep=&quot;,&quot;) head(example16_2) ## x1 x2 x3 y ## 1 51.3 73.6 36.4 2.99 ## 2 48.9 83.9 34.0 3.11 ## 3 42.8 78.3 31.0 1.91 ## 4 55.0 77.1 31.0 2.63 ## 5 45.3 81.7 30.0 2.86 ## 6 45.3 74.8 32.0 1.91 fit &lt;- lm(y~x1+x2+x3, data=example16_2) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3, data = example16_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3749 -0.2747 0.1042 0.1820 0.4277 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.71489 1.30082 -3.625 0.00228 ** ## x1 0.06091 0.02050 2.971 0.00901 ** ## x2 0.03563 0.01531 2.327 0.03339 * ## x3 0.04924 0.02866 1.718 0.10507 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2853 on 16 degrees of freedom ## Multiple R-squared: 0.7251, Adjusted R-squared: 0.6736 ## F-statistic: 14.07 on 3 and 16 DF, p-value: 9.464e-05 vif(fit)#计算方差膨胀因子，查看是否有共线性，一般情况下vif不应大于5 ## x1 x2 x3 ## 1.533050 1.212454 1.793403 library(psych) describe(example16_2) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## x1 1 20 49.51 3.95 50.15 49.66 4.52 42.70 55.10 12.40 -0.28 -1.30 0.88 ## x2 2 20 78.79 4.71 78.25 78.38 5.11 72.50 88.40 15.90 0.55 -0.65 1.05 ## x3 3 20 33.62 3.06 33.00 33.51 4.37 30.00 38.10 8.10 0.16 -1.78 0.68 ## y 4 20 2.76 0.50 2.92 2.81 0.47 1.91 3.28 1.37 -0.69 -1.04 0.11 fa.parallel(example16_2[-4], fa=&quot;pc&quot;, n.iter=100, show.legend=FALSE, main=&quot;Screen plot with parallel analysis&quot;) ## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, : ## The estimated weights for the factor scores are probably incorrect. Try a ## different factor score estimation method. ## Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : An ## ultra-Heywood case was detected. Examine the results carefully ## Parallel analysis suggests that the number of factors = NA and the number of components = 1 abline(1,0) pc &lt;- principal(example16_2[-4], nfactors=2, rotate= &quot;varimax&quot;, score=TRUE) pc ## Principal Components Analysis ## Call: principal(r = example16_2[-4], nfactors = 2, rotate = &quot;varimax&quot;, ## scores = TRUE) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## x1 0.94 -0.04 0.88 0.12 1.0 ## x2 0.10 0.97 0.96 0.04 1.0 ## x3 0.80 0.42 0.81 0.19 1.5 ## ## RC1 RC2 ## SS loadings 1.52 1.12 ## Proportion Var 0.51 0.37 ## Cumulative Var 0.51 0.88 ## Proportion Explained 0.58 0.42 ## Cumulative Proportion 0.58 1.00 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.11 ## with the empirical chi square 1.43 with prob &lt; NA ## ## Fit based upon off diagonal values = 0.93 pc$weights ## RC1 RC2 ## x1 0.6844785 -0.2750477 ## x2 -0.1716167 0.9268662 ## x3 0.4714891 0.2054222 pc$scores ## RC1 RC2 ## [1,] 0.9284132 -0.95922278 ## [2,] -0.2325409 1.07426220 ## [3,] -1.5470885 0.19479692 ## [4,] 0.6091329 -0.89030666 ## [5,] -1.3922781 0.62301923 ## [6,] -0.8324830 -0.60100895 ## [7,] 0.9574979 -0.93977873 ## [8,] 1.2423717 0.04892078 ## [9,] -0.4044938 -1.41914949 ## [10,] 1.2714564 0.06836483 ## [11,] -0.2616256 1.05481815 ## [12,] 0.8606755 1.97132888 ## [13,] -1.5761732 0.17535287 ## [14,] 0.8624195 1.96531651 ## [15,] 0.6382176 -0.87086261 ## [16,] -1.3751199 0.62372270 ## [17,] 0.7898256 -0.03421541 ## [18,] -0.4682093 -1.42467765 ## [19,] 0.7607409 -0.05365946 ## [20,] -0.8307390 -0.60702132 newdata &lt;- data.frame(example16_2, pc$scores) head(newdata) ## x1 x2 x3 y RC1 RC2 ## 1 51.3 73.6 36.4 2.99 0.9284132 -0.9592228 ## 2 48.9 83.9 34.0 3.11 -0.2325409 1.0742622 ## 3 42.8 78.3 31.0 1.91 -1.5470885 0.1947969 ## 4 55.0 77.1 31.0 2.63 0.6091329 -0.8903067 ## 5 45.3 81.7 30.0 2.86 -1.3922781 0.6230192 ## 6 45.3 74.8 32.0 1.91 -0.8324830 -0.6010089 fit &lt;- lm(y~ RC1+RC2, data=newdata) summary(fit) ## ## Call: ## lm(formula = y ~ RC1 + RC2, data = newdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.42139 -0.22313 0.06444 0.20047 0.46719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.76300 0.06312 43.775 &lt; 2e-16 *** ## RC1 0.36253 0.06476 5.598 3.2e-05 *** ## RC2 0.21598 0.06476 3.335 0.00392 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2823 on 17 degrees of freedom ## Multiple R-squared: 0.7141, Adjusted R-squared: 0.6805 ## F-statistic: 21.23 on 2 and 17 DF, p-value: 2.386e-05 vif(fit) ## RC1 RC2 ## 1 1 "],["探索性因子分析.html", "Chapter 16 探索性因子分析 16.1 因子分析 16.2 因子分析模型形式 16.3 因子分析的R语言实现 16.4 因子分析应用案例", " Chapter 16 探索性因子分析 16.1 因子分析 探索性因子分析（EFA）是一系列用来发现一组变量潜在结构的方法。它通过寻找一组更小的、潜在的或隐藏的结构来解释已观测到的、显式的变量间的关系。 Harman74.cor包含了24个心理测验间的相互关系，受试对象为145个七年级或八年级的学生。应用EFA探索该数据表明276个测验间的相互关系可用四个学生能力的潜在因子（语言能力、反应速度、推理能力和记忆能力）进行解释。 16.2 因子分析模型形式 \\[ X_i=a_1F_1+a_2F_2+\\cdots +a_pF_p+U_i \\] 其中\\(X_i\\)是第i个可观测变量（\\(i=1…k\\)），\\(F_j\\)是公共因子（\\(j=1…p\\)），并且\\(p&lt;k\\)。\\(U_i\\)是\\(X_i\\)变量独有的部分（无法被公共因子解释）。\\(a_i\\)可认为是每个因子对复合而成的可观测变量的贡献值。回到本章开头的Harman74.cor的例子，我们认为每个个体在24个心理学测验上的观测得分，是根据四个潜在心理学因素的加权能力值组合而成。 虽然PCA和EFA存在差异，但是它们的许多分析步骤都是相似的。为阐述EFA的分析过程，我们用它来对六个心理学测验间的相关性进行分析。 112个人参与了六个测验，包括非语言的普通智力测验（general）、画图测验（picture）、积木图案测验（blocks）、迷宫测验（maze）、阅读测验（reading）和词汇测验（vocab）。 我们如何用一组较少的、潜在的心理学因素来解释参与者的测验得分呢？数据集ability.cov提供了变量的协方差矩阵，你可用cov2cor()函数将其转化为相关系数矩阵。数据集没有缺失值。 16.3 因子分析的R语言实现 ## Exploratory factor analysis of ability.cov data options(digits=2) library(psych) covariances &lt;- ability.cov$cov # convert covariances to correlations correlations &lt;- cov2cor(covariances) correlations ## general picture blocks maze reading vocab ## general 1.00 0.47 0.55 0.34 0.58 0.51 ## picture 0.47 1.00 0.57 0.19 0.26 0.24 ## blocks 0.55 0.57 1.00 0.45 0.35 0.36 ## maze 0.34 0.19 0.45 1.00 0.18 0.22 ## reading 0.58 0.26 0.35 0.18 1.00 0.79 ## vocab 0.51 0.24 0.36 0.22 0.79 1.00 因为要寻求用来解释数据的潜在结构，可使用EFA方法。与使用PCA相同，下一步工作为判断需要提取几个因子。 16.3.1 判断需提取的公共因子数 用fa.parallel()函数可判断需提取的因子数： # determine number of factors to extract fa.parallel(correlations, n.obs=112, fa=&quot;both&quot;, n.iter=100, main=&quot;Scree plots with parallel analysis&quot;) ## Parallel analysis suggests that the number of factors = 2 and the number of components = 1 abline(h=0,lwd=1,col=&quot;green&quot;) 结果解读 1 代码中使用了fa=\"both\",因子图形将会同时展示主成分和公共因子分析的结果。 图形中有几个值得注意的地方。如果使用PCA方法，你可能会选择一个成分(碎石检验和平行分析)或者两个成分(特征值大于1)当摇摆不定时，高估因子数通常比低估因子数的结果好，因为高估因子数般较少曲解“真实“情况。 观察EFA的结果，显然需提取两个因子。碎石检验的前两个特征值(△)都在拐角处之上，并且大于基于100次模拟数据矩阵的特征值均值。对于EFA, Kaiser-Harris准则的特征值数大0, 而不是1（大部分人都没有意识到这 ）图形中该准则也建议选择两个因子。 16.3.2 提取公共因子 现在你决定提取两个因子，可以使用fa ()函数获得相应的结果fa() 函数的格式如下： fa(r, nfactors=, n .obs= , rotate=, scores=, fm=) 其中： r是相关系数矩阵或者原始数据矩阵； nfactors 设定提取的因子数(默认为1); obs 观测数（输入相关系数矩阵时需要填写）； rotate设定旋转的方法（默认互变异数最小法）； scores 设定是否计算因子得分（默认不计算）； fm设定因子化方法（默认极小残差法） 与PCA不同，提取公共因子的方法很多，包括最大似然法(ml)、主轴迭代法(pa)、加权最小二乘法(wls)、广义加权最小二乘法(gls)和最小残差法(minres)。统计学家青眯使用最大似然法，因为它有良好的统计性质，不过有时候最大似然法不会收敛，此时使用主轴迭代法效果会很好。 本例使用主轴迭代法(fm=\"pa\")提取未旋转的因子 fa &lt;- fa(correlations, nfactors=2, rotate=&quot;none&quot;, fm=&quot;pa&quot;) fa ## Factor Analysis using method = pa ## Call: fa(r = correlations, nfactors = 2, rotate = &quot;none&quot;, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 h2 u2 com ## general 0.75 0.07 0.57 0.432 1.0 ## picture 0.52 0.32 0.38 0.623 1.7 ## blocks 0.75 0.52 0.83 0.166 1.8 ## maze 0.39 0.22 0.20 0.798 1.6 ## reading 0.81 -0.51 0.91 0.089 1.7 ## vocab 0.73 -0.39 0.69 0.313 1.5 ## ## PA1 PA2 ## SS loadings 2.75 0.83 ## Proportion Var 0.46 0.14 ## Cumulative Var 0.46 0.60 ## Proportion Explained 0.77 0.23 ## Cumulative Proportion 0.77 1.00 ## ## Mean item complexity = 1.5 ## Test of the hypothesis that 2 factors are sufficient. ## ## The degrees of freedom for the null model are 15 and the objective function was 2.5 ## The degrees of freedom for the model are 4 and the objective function was 0.07 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.06 ## ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.96 0.92 ## Multiple R square of scores with factors 0.93 0.84 ## Minimum correlation of possible factor scores 0.86 0.68 结果解读2 可以看到，两个因子解释了六个心理学测验60%的方差。不过因子载荷阵的意义并不太好解释，此时使用因子旋转将有助于因子的解释。 16.3.3 因子旋转 你可以使用正交旋转或者斜交旋转来旋转两个因子的结果。现在我们同时尝试两种方法，看看它们的异同。 首先使用正交旋转： fa.varimax &lt;- fa(correlations, nfactors=2, rotate=&quot;varimax&quot;, fm=&quot;pa&quot;) fa.varimax ## Factor Analysis using method = pa ## Call: fa(r = correlations, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 h2 u2 com ## general 0.49 0.57 0.57 0.432 2.0 ## picture 0.16 0.59 0.38 0.623 1.1 ## blocks 0.18 0.89 0.83 0.166 1.1 ## maze 0.13 0.43 0.20 0.798 1.2 ## reading 0.93 0.20 0.91 0.089 1.1 ## vocab 0.80 0.23 0.69 0.313 1.2 ## ## PA1 PA2 ## SS loadings 1.83 1.75 ## Proportion Var 0.30 0.29 ## Cumulative Var 0.30 0.60 ## Proportion Explained 0.51 0.49 ## Cumulative Proportion 0.51 1.00 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 2 factors are sufficient. ## ## The degrees of freedom for the null model are 15 and the objective function was 2.5 ## The degrees of freedom for the model are 4 and the objective function was 0.07 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.06 ## ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.96 0.92 ## Multiple R square of scores with factors 0.91 0.85 ## Minimum correlation of possible factor scores 0.82 0.71 结果解读3 结果显示因子变得更好解释了。 阅读和词汇在第一因子上载荷较大，画图、积木图案和迷宫在第二因子上载荷较大，非语言的普通智力测量在两个因子上载荷较为平均，这表明存在一个语言智力因子、一个非语言智力因子。 使用正交旋转将人为地强制两个因子不相关。如果想允许两个因子相关该怎么办呢？此时可以使用斜交转轴法，比如promax #install.packages(&quot;GPArotation&quot;) library(GPArotation) fa.promax &lt;- fa(correlations, nfactors=2, rotate=&quot;promax&quot;, fm=&quot;pa&quot;) fa.promax ## Factor Analysis using method = pa ## Call: fa(r = correlations, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 h2 u2 com ## general 0.37 0.48 0.57 0.432 1.9 ## picture -0.03 0.63 0.38 0.623 1.0 ## blocks -0.10 0.97 0.83 0.166 1.0 ## maze 0.00 0.45 0.20 0.798 1.0 ## reading 1.00 -0.09 0.91 0.089 1.0 ## vocab 0.84 -0.01 0.69 0.313 1.0 ## ## PA1 PA2 ## SS loadings 1.83 1.75 ## Proportion Var 0.30 0.29 ## Cumulative Var 0.30 0.60 ## Proportion Explained 0.51 0.49 ## Cumulative Proportion 0.51 1.00 ## ## With factor correlations of ## PA1 PA2 ## PA1 1.00 0.55 ## PA2 0.55 1.00 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 2 factors are sufficient. ## ## The degrees of freedom for the null model are 15 and the objective function was 2.5 ## The degrees of freedom for the model are 4 and the objective function was 0.07 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.06 ## ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.97 0.94 ## Multiple R square of scores with factors 0.93 0.88 ## Minimum correlation of possible factor scores 0.86 0.77 结果解读 4 根据以上结果，你可以看出正交旋转和斜交旋转的不同之处。 对于正交旋转，因子分析的重点在于因子结构矩阵（变量与因子的相关系数）。 而对于斜交旋转，因子分析会考虑3个矩阵：因子结构矩阵、因子模式矩阵和因子关联矩阵。 因子模式矩阵即标准化的回归系数矩阵。它列出了因子预测变量的权重。因子关联矩阵即因子相关系数矩阵。 在结果中，PA1和PA2栏中的值组成了因子模式矩阵。 它们是标准化的回归系数，而不是相关系数。 注意，矩阵的列仍用来对因子进行命名（虽然此处存在一些争论）。 你同样可以得到一个语言因子和一个非语言因子。 因子关联矩阵显示两个因子的相关系数为0.57,相关性很大。如果因子间的关联性很低，你可能需要重新使用正交旋转来简化问题。 因子结构矩阵（或称因子载荷阵没有被列出来，但你可以使用公式 F=P*Phi 很轻松地得到它，其中，F是因子载荷阵， P为因子模式矩阵， Phi为因子关联矩阵 下面的函数即可进行该乘 法运算 fsm &lt;- function(oblique) { if (class(oblique)[2]==&quot;fa&quot; &amp; is.null(oblique$Phi)) { warning(&quot;Object doesn&#39;t look like oblique EFA&quot;) } else { P &lt;- unclass(oblique$loading) F &lt;- P %*% oblique$Phi colnames(F) &lt;- c(&quot;PA1&quot;, &quot;PA2&quot;) return(F) } } fsm(fa.promax) ## PA1 PA2 ## general 0.64 0.69 ## picture 0.32 0.61 ## blocks 0.43 0.91 ## maze 0.25 0.45 ## reading 0.95 0.46 ## vocab 0.83 0.45 现在你可以看到变量与因子间的相关系数。将它们与正交旋转所得因子载荷阵相比，你会发现该载荷阵列的噪音比较大，这是因为之前你允许潜在因子相关。 虽然斜交方法更为复杂，但模型将更符合真实数据 使用 factor.plot()或fa.diagram() 函数，你可以绘制正交或者斜交结果的图形 factor.plot(fa.promax, labels=rownames(fa.promax$loadings)) 数据集ability.cov中心理学测验的两因子图形:词汇和阅读在第一个因 子(PA1) 上载荷较大，而积木图案、画图和迷宫在第二个因子(PA2)上载 荷较大。普通智力测验在两个因子上较为平均。 fa.diagram(fa.promax, simple=FALSE) 数据集ability.cov 中心理学测验的两因子斜交旋转结果图 16.4 因子分析应用案例 library(psych) example17_3 &lt;- read.table (&quot;data/example17_3.csv&quot;, header=TRUE, sep=&quot;,&quot;) head(example17_3) ## x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 1 4.3 389 99 1.23 25 93 3.6 98 62 ## 2 3.4 271 88 0.85 24 94 2.4 98 73 ## 3 4.4 385 104 1.21 27 93 4.0 98 77 ## 4 4.2 377 99 1.19 27 94 2.9 99 63 ## 5 4.3 378 102 1.19 28 93 2.0 100 80 ## 6 4.1 349 98 1.10 27 91 4.4 99 63 fa.parallel(example17_3, fa=&quot;fa&quot;, n.iter=100, main=&quot;Screen plots with parallel analysis&quot;) ## Parallel analysis suggests that the number of factors = 3 and the number of components = NA abline(0,0) 结果解读1 由绘制的图形可知，0以上有4个节点，故我们可以提取4个因子 fa &lt;- fa(example17_3, nfactors=4, rotate=&quot;none&quot;, fm=&quot;ml&quot;, score=TRUE) fa ## Factor Analysis using method = ml ## Call: fa(r = example17_3, nfactors = 4, rotate = &quot;none&quot;, scores = TRUE, ## fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML3 ML1 ML2 ML4 h2 u2 com ## x1 0.61 0.78 0.11 -0.01 1.00 0.005 1.9 ## x2 -0.40 0.31 0.34 -0.59 0.72 0.276 3.1 ## x3 -0.30 0.56 0.25 0.49 0.71 0.289 2.9 ## x4 -0.55 0.75 0.35 0.01 1.00 0.005 2.3 ## x5 0.67 -0.13 0.16 0.26 0.57 0.435 1.5 ## x6 0.15 -0.39 0.91 0.00 1.00 0.005 1.4 ## x7 0.14 -0.07 -0.47 0.10 0.26 0.743 1.3 ## x8 0.45 0.11 -0.10 0.36 0.36 0.642 2.2 ## x9 -0.56 -0.12 0.05 -0.46 0.55 0.455 2.1 ## ## ML3 ML1 ML2 ML4 ## SS loadings 1.94 1.79 1.40 1.02 ## Proportion Var 0.22 0.20 0.16 0.11 ## Cumulative Var 0.22 0.41 0.57 0.68 ## Proportion Explained 0.32 0.29 0.23 0.17 ## Cumulative Proportion 0.32 0.61 0.83 1.00 ## ## Mean item complexity = 2.1 ## Test of the hypothesis that 4 factors are sufficient. ## ## The degrees of freedom for the null model are 36 and the objective function was 3.8 with Chi Square of 119 ## The degrees of freedom for the model are 6 and the objective function was 0.24 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.09 ## ## The harmonic number of observations is 36 with the empirical chi square 3.4 with prob &lt; 0.75 ## The total number of observations was 36 with Likelihood Chi Square = 6.8 with prob &lt; 0.34 ## ## Tucker Lewis Index of factoring reliability = 0.93 ## RMSEA index = 0.055 and the 90 % confidence intervals are 0 0.23 ## BIC = -15 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## ML3 ML1 ML2 ML4 ## Correlation of (regression) scores with factors 1.00 1.00 1.00 0.87 ## Multiple R square of scores with factors 0.99 1.00 0.99 0.75 ## Minimum correlation of possible factor scores 0.99 0.99 0.99 0.50 结果解读2 我们可以看到，四个因子可以解释68%的方差，但是每个公共因子反应了哪些变量仍未可知，所以我们旋转一下因子。 fa2 &lt;- fa(example17_3, nfactors=4, rotate=&quot;varimax&quot;, fm=&quot;ml&quot;, score=TRUE) fa2 ## Factor Analysis using method = ml ## Call: fa(r = example17_3, nfactors = 4, rotate = &quot;varimax&quot;, scores = TRUE, ## fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML3 ML1 ML2 ML4 h2 u2 com ## x1 -0.31 0.23 -0.03 0.92 1.00 0.005 1.4 ## x2 0.75 0.16 0.24 0.27 0.72 0.276 1.6 ## x3 -0.10 0.83 0.03 0.07 0.71 0.289 1.0 ## x4 0.46 0.84 0.09 0.26 1.00 0.005 1.8 ## x5 -0.64 -0.23 0.24 0.21 0.57 0.435 1.8 ## x6 -0.09 -0.09 0.98 -0.10 1.00 0.005 1.1 ## x7 -0.20 -0.18 -0.42 -0.06 0.26 0.743 1.9 ## x8 -0.56 0.02 -0.10 0.18 0.36 0.642 1.3 ## x9 0.70 -0.04 0.04 -0.21 0.55 0.455 1.2 ## ## ML3 ML1 ML2 ML4 ## SS loadings 2.15 1.58 1.29 1.12 ## Proportion Var 0.24 0.18 0.14 0.12 ## Cumulative Var 0.24 0.41 0.56 0.68 ## Proportion Explained 0.35 0.26 0.21 0.18 ## Cumulative Proportion 0.35 0.61 0.82 1.00 ## ## Mean item complexity = 1.4 ## Test of the hypothesis that 4 factors are sufficient. ## ## The degrees of freedom for the null model are 36 and the objective function was 3.8 with Chi Square of 119 ## The degrees of freedom for the model are 6 and the objective function was 0.24 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.09 ## ## The harmonic number of observations is 36 with the empirical chi square 3.4 with prob &lt; 0.75 ## The total number of observations was 36 with Likelihood Chi Square = 6.8 with prob &lt; 0.34 ## ## Tucker Lewis Index of factoring reliability = 0.93 ## RMSEA index = 0.055 and the 90 % confidence intervals are 0 0.23 ## BIC = -15 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## ML3 ML1 ML2 ML4 ## Correlation of (regression) scores with factors 0.93 0.96 1.00 0.98 ## Multiple R square of scores with factors 0.86 0.92 0.99 0.96 ## Minimum correlation of possible factor scores 0.72 0.85 0.99 0.91 fa2$weights ## ML3 ML1 ML2 ML4 ## x1 -0.200 -0.218 0.0666 1.0659 ## x2 0.409 -0.280 0.0337 0.2113 ## x3 -0.314 0.239 -0.0220 -0.1682 ## x4 0.361 0.985 0.1090 -0.1145 ## x5 -0.118 0.074 -0.0049 -0.0532 ## x6 -0.139 -0.025 0.9924 -0.0068 ## x7 -0.026 0.016 -0.0047 -0.0130 ## x8 -0.108 0.074 -0.0082 -0.0527 ## x9 0.196 -0.134 0.0140 0.0959 fa2$scores ## ML3 ML1 ML2 ML4 ## [1,] -0.726 1.757 0.114 0.0553 ## [2,] -1.016 -0.382 0.491 -2.1394 ## [3,] -0.790 1.663 -0.324 0.1558 ## [4,] -1.013 1.724 0.534 -0.4866 ## [5,] -0.834 1.530 0.109 0.0149 ## [6,] -0.971 1.274 -1.777 -0.5899 ## [7,] -0.524 0.862 -1.644 0.9588 ## [8,] -2.042 -3.068 -1.407 0.6679 ## [9,] -0.605 -0.588 0.359 -0.1406 ## [10,] -0.497 -0.602 -1.417 0.9201 ## [11,] -0.455 0.113 0.182 -0.0614 ## [12,] -0.149 0.270 0.081 -0.0901 ## [13,] -0.083 -0.669 0.336 0.2029 ## [14,] -1.300 -1.730 0.726 -1.6410 ## [15,] -0.835 -0.026 1.769 -0.0039 ## [16,] -0.091 0.108 1.260 0.2167 ## [17,] -0.772 0.064 0.835 -0.0438 ## [18,] -0.651 -0.141 0.381 2.0978 ## [19,] -0.421 -0.416 1.627 2.8772 ## [20,] -0.143 -0.994 -0.937 -0.0121 ## [21,] 0.351 -0.146 -1.084 0.2842 ## [22,] -0.217 -0.070 1.159 -0.4644 ## [23,] 0.376 0.176 -0.268 0.1573 ## [24,] 0.340 0.526 0.107 0.9631 ## [25,] 0.613 -0.987 0.635 -2.6145 ## [26,] 0.349 -0.781 -1.001 -0.1831 ## [27,] 1.451 0.592 1.666 0.5406 ## [28,] 1.317 0.208 0.318 -0.2635 ## [29,] 0.969 0.013 -1.832 0.2150 ## [30,] 0.775 -0.098 -0.936 0.3854 ## [31,] 1.125 -0.046 0.557 0.2577 ## [32,] 1.004 -0.843 0.503 0.0765 ## [33,] 1.225 -0.349 0.949 -0.0431 ## [34,] 1.208 0.115 -0.475 -1.0382 ## [35,] 1.244 0.131 -0.398 -0.6425 ## [36,] 1.785 0.809 -1.198 -0.5892 factor.plot(fa2, labels=rownames(fa$loadings)) fa.diagram(fa2, simple=FALSE) "],["聚类分析.html", "Chapter 17 聚类分析 17.1 概述 17.2 K均值算法 17.3 围绕中心点的划分", " Chapter 17 聚类分析 17.1 概述 17.1.1 聚类的定义 聚类分析是一种数据降维技术，旨在揭露一个数据集中观测值的子集。它可以把大量的观测值归约为若干个类。这里的类被定义为若干个观测值组成的群组，群组内观测值的相似度比群间相似度高。这不是一个精确的定义，从而导致了各种聚类方法的出现。 17.1.2 两种聚类分析 最常用的两种聚类方法是层次聚类（hierarchical agglomerative clustering）和划分聚类（partitioning clustering）。在层次聚类中，每一个观测值自成一类，这些类每次两两合并，直到所有的类被聚成一类为止。在划分聚类中，首先指定类的个数K，然后观测值被随机分成K类，再重新形成聚合的类。 对于层次聚类来说，最常用的算法是单联动（single linkage）、全联动（complete linkage ）、平均联动（average linkage）、质心（centroid）和Ward方法。 对于划分聚类来说，最常用的算法是K均值（K-means）和围绕中心点的划分（PAM）。每个聚类方法都有它的优点和缺点，我们将在本章讨论。 17.1.3 聚类分析的一般步骤 选择合适的变量 第一步是选择你感觉可能对识别和理解数据中不同观测值分组有重要影 响的变量。例如，在一项抑郁症研究中，你可能会评估以下一个或多个方面：心理学症状，身体症状，发病年龄，发病次数、持续时间和发作时间，住院次数，自理能力，社会和工作经历，当前的年龄，性别，种族，社会经济地位，婚姻状况，家族病史以及对以前治疗的反应。高级的聚类方法也不能弥补聚类变量选不好的问题。   缩放数据 如果我们在分析中选择的变量变化范围很大，那么该变量对结果的影响也是最大的。这往往是不可取的，分析师往往在分析之前缩放数据。最常用的方法是将每个变量标准化为均值为0和标准差为1的变量。其他的替代方法包括每个变量被其最大值相除或该变量减去它的平均值并除以变量的平均绝对偏差。这三种方法能用下面的代码来解释: df1 &lt;- apply(mydata, 2, function(x){(x-mean(x))/sd(x)}) df2 &lt;- apply(mydata, 2, function(x){x/max(x)}) df3 &lt;- apply(mydata, 2, function(x){(x – mean(x))/mad(x)}) 在本章中，你可以使用scale()函数来将变量标准化到均值为0和标准差为1的变量。这和第一个代码片段（df1）等价。   寻找异常值 许多聚类方法对于异常值是十分敏感的，它能扭曲我们得到的聚类方案。你可以通过outliers包中的函数来筛选（和删除）异常单变量离群点。 mvoutlier包中包含了能识别多元变量的离群点的函数。一个替代的方法是使用对异常值稳健的聚类方法，围绕中心点的划分可以很好地解释这种方法。   计算距离 尽管不同的聚类算法差异很大，但是它们通常需要计算被聚类的实体之间的距离。两个观测值之间最常用的距离量度是欧几里得距离，其他可选的量度包括曼哈顿距离、兰氏距离、非对称二元距离、最大距离和闵可夫斯基距离（可使用?dist查看详细信息）。在这一章中，计算距离时默认使用欧几里得距离。   选择聚类算法 接下来选择对数据聚类的方法，层次聚类对于小样本来说很实用（如150个观测值或更少），而且这种情况下嵌套聚类更实用。划分的方法能处理更大的数据量，但是需要事先确定聚类的个数。一旦选定了层次方法或划分方法，就必须选择一个特定的聚类算法。这里再次强调每个算法都有优点和缺点   获得一种或多种聚类方法。这一步可以使用步骤(5)选择的方法。   确定类的数目 为了得到最终的聚类方案，你必须确定类的数目。对此研究者们也提出了很多相应的解决方法。常用方法是尝试不同的类数（比如2～K）并比较解的质量。在NbClust包中的NbClust()函数提供了30个不同的指标来帮助你进行选择（也表明这个问题有多么难解）。本章将多次使用这个包。   获得最终聚类方案: 一旦类的个数确定下来，就可以提取出子群，形成最终的聚类方案。   结果可视化:可视化可以帮助你判定聚类方案的意义和用处。层次聚类的结果通常表示为一个树状图。划分的结果通常利用可视化双变量聚类图来表示。   解读类 一旦聚类方案确定，你必须解释（或许命名）这个类。一个类中的观测值有何相似之处？不同的类之间的观测值有何不同？这一步通常通过获得类中每个变量的汇总统计来完成。对于连续数据，每一类中变量的均值和中位数会被计算出来。对于混合数据（数据中包含分类变量），结果中将返回各类的众数或类别分布。   验证结果 验证聚类方案相当于问：“这种划分并不是因为数据集或聚类方法的某种特性，而是确实给出了一个某种程度上有实际意义的结果吗？”如果采用不同的聚类方法或不同的样本，是否会产生相同的类？fpc、clv和clValid包包含了评估聚类解的稳定性的函数。 17.1.4 计算距离 聚类分析的第一步都是度量样本单元间的距离、相异性或相似性。两个观测值之间的欧几里得距离定义为： \\[ d_{ij}=\\sqrt{\\sum\\limits_{p=1}^{p}(x_{ip}-x_{jp})^2} \\] 这里i和j代表第i和第j个观测值，p是变量的个数。 观测值之间的距离越大，异质性越大。观测值和它自己之间的距离是0。 par(ask=TRUE) opar &lt;- par(no.readonly=FALSE) library(flexclust) # Calculating Distances data(nutrient, package=&quot;flexclust&quot;) head(nutrient, 2) ## energy protein fat calcium iron ## BEEF BRAISED 340 20 28 9 2.6 ## HAMBURGER 245 21 17 9 2.7 d &lt;- dist(nutrient) as.matrix(d)[1:4,1:4] ## BEEF BRAISED HAMBURGER BEEF ROAST BEEF STEAK ## BEEF BRAISED 0 96 81 35 ## HAMBURGER 96 0 176 131 ## BEEF ROAST 81 176 0 46 ## BEEF STEAK 35 131 46 0 17.1.5 层次聚类分析 在层次聚类中，起初每一个实例或观测值属于一类。聚类就是每一次把两类聚成新的一类，直到所有的类聚成单个类为止，算法如下： ➢ (1) 定义每个观测值（行或单元）为一类； ➢ (2) 计算每类和其他各类的距离； ➢ (3) 把距离最短的两类合并成一类，这样类的个数就减少一个； ➢ (4) 重复步骤(2)和步骤(3)，直到包含所有观测值的类合并成单个的类为止。 营养数据的平均联动聚类 data(nutrient, package=&quot;flexclust&quot;) row.names(nutrient) &lt;- tolower(row.names(nutrient)) nutrient.scaled &lt;- scale(nutrient) #标准化数据集 d &lt;- dist(nutrient.scaled) fit.average &lt;- hclust(d, method=&quot;average&quot;) plot(fit.average, hang=-1, cex=.8, main=&quot;Average Linkage Clustering&quot;) 17.1.6 选择聚类的个数 # Listing 16.2 - Selecting the number of clusters library(NbClust) nc &lt;- NbClust(nutrient.scaled, distance=&quot;euclidean&quot;, min.nc=2, max.nc=15, method=&quot;average&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 4 proposed 2 as the best number of clusters ## * 4 proposed 3 as the best number of clusters ## * 2 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 9 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## * 2 proposed 13 as the best number of clusters ## * 1 proposed 14 as the best number of clusters ## * 4 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* par(opar) table(nc$Best.n[1,]) ## ## 0 1 2 3 4 5 9 10 13 14 15 ## 2 1 4 4 2 4 1 1 2 1 4 barplot(table(nc$Best.n[1,]), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main=&quot;Number of Clusters Chosen by 26 Criteria&quot;) 17.1.7 获取最终的聚类方案 # Listing 16.3 - Obtaining the final cluster solution clusters &lt;- cutree(fit.average, k=5) # cutree()函数用来把树状图分成五类 table(clusters) ## clusters ## 1 2 3 4 5 ## 7 16 1 2 1 aggregate(nutrient, by=list(cluster=clusters), median) ## cluster energy protein fat calcium iron ## 1 1 340 19 29 9 2.5 ## 2 2 170 20 8 13 1.4 ## 3 3 160 26 5 14 5.9 ## 4 4 58 9 1 78 5.7 ## 5 5 180 22 9 367 2.5 # aggregate()函数用来获取每类的中位数 aggregate(as.data.frame(nutrient.scaled), by=list(cluster=clusters), median) ## cluster energy protein fat calcium iron ## 1 1 1.31 0.00 1.38 -0.45 0.081 ## 2 2 -0.37 0.24 -0.49 -0.40 -0.637 ## 3 3 -0.47 1.65 -0.75 -0.38 2.408 ## 4 4 -1.48 -2.35 -1.11 0.44 2.271 ## 5 5 -0.27 0.71 -0.40 4.14 0.081 plot(fit.average, hang=-1, cex=.8, main=&quot;Average Linkage Clustering\\n5 Cluster Solution&quot;) #树状图被重新绘制 rect.hclust(fit.average, k=5) #rect.hclust()函数用来叠加五类的解决方案 17.2 K均值算法 ➢ 最常见的划分方法是K均值聚类分析。从概念上讲，K均值算法如下： 选择K个中心点（随机选择K行）； 把每个数据点分配到离它最近的中心点； 重新计算每类中的点到该类中心点距离的平均值（也就说，得到长度为p的均值向量，这里的p是变量的个数）； 分配每个数据到它最近的中心点； 重复步骤(3)和步骤(4)直到所有的观测值不再被分配或是达到最大的迭代次数（R把10次作为默认迭代次数）。 # Plot function for within groups sum of squares by number of clusters wssplot &lt;- function(data, nc=15, seed=1234){ wss &lt;- (nrow(data)-1)*sum(apply(data,2,var)) for (i in 2:nc){ set.seed(seed) wss[i] &lt;- sum(kmeans(data, centers=i)$withinss)} plot(1:nc, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;, ylab=&quot;Within groups sum of squares&quot;)} # Listing 16.4 - K-means clustering of wine data library(rattle) data(wine, package=&quot;rattle&quot;) head(wine) ## Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids ## 1 1 14 1.7 2.4 16 127 2.8 3.1 0.28 ## 2 1 13 1.8 2.1 11 100 2.6 2.8 0.26 ## 3 1 13 2.4 2.7 19 101 2.8 3.2 0.30 ## 4 1 14 1.9 2.5 17 113 3.8 3.5 0.24 ## 5 1 13 2.6 2.9 21 118 2.8 2.7 0.39 ## 6 1 14 1.8 2.4 15 112 3.3 3.4 0.34 ## Proanthocyanins Color Hue Dilution Proline ## 1 2.3 5.6 1.04 3.9 1065 ## 2 1.3 4.4 1.05 3.4 1050 ## 3 2.8 5.7 1.03 3.2 1185 ## 4 2.2 7.8 0.86 3.4 1480 ## 5 1.8 4.3 1.04 2.9 735 ## 6 2.0 6.8 1.05 2.8 1450 df &lt;- scale(wine[-1]) # 标准化数据 wssplot(df) library(NbClust) set.seed(1234) #设置种子数 nc &lt;- NbClust(df, min.nc=2, max.nc=15, method=&quot;kmeans&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 2 proposed 2 as the best number of clusters ## * 19 proposed 3 as the best number of clusters ## * 1 proposed 14 as the best number of clusters ## * 1 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* par(opar) table(nc$Best.n[1,]) # 决定聚类个数 ## ## 0 1 2 3 14 15 ## 2 1 2 19 1 1 barplot(table(nc$Best.n[1,]), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main=&quot;Number of Clusters Chosen by 26 Criteria&quot;) set.seed(1234) fit.km &lt;- kmeans(df, 3, nstart=25) # 进行K均值聚类分析 fit.km$size ## [1] 62 65 51 fit.km$centers ## Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids ## 1 0.83 -0.30 0.36 -0.61 0.576 0.883 0.975 -0.561 ## 2 -0.92 -0.39 -0.49 0.17 -0.490 -0.076 0.021 -0.033 ## 3 0.16 0.87 0.19 0.52 -0.075 -0.977 -1.212 0.724 ## Proanthocyanins Color Hue Dilution Proline ## 1 0.579 0.17 0.47 0.78 1.12 ## 2 0.058 -0.90 0.46 0.27 -0.75 ## 3 -0.778 0.94 -1.16 -1.29 -0.41 aggregate(wine[-1], by=list(cluster=fit.km$cluster), mean) ## cluster Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids ## 1 1 14 2.0 2.5 17 108 2.8 3.00 ## 2 2 12 1.9 2.2 20 93 2.2 2.05 ## 3 3 13 3.3 2.4 21 99 1.7 0.82 ## Nonflavanoids Proanthocyanins Color Hue Dilution Proline ## 1 0.29 1.9 5.5 1.07 3.2 1100 ## 2 0.36 1.6 3.0 1.06 2.8 510 ## 3 0.45 1.1 7.2 0.69 1.7 619 # evaluate clustering ct.km &lt;- table(wine$Type, fit.km$cluster) ct.km ## ## 1 2 3 ## 1 59 0 0 ## 2 3 65 3 ## 3 0 0 48 library(flexclust) randIndex(ct.km) ## ARI ## 0.9 17.3 围绕中心点的划分 ➢ 因为K均值聚类方法是基于均值的，所以它对异常值是敏感的。一个更稳健的方法是围绕中心点的划分（PAM）。与其用质心（变量均值向量）表示类，不如用一个最有代表性的观测值来表示（称为中心点）。K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此，PAM可以容纳混合数据类型，并且不仅限于连续变量 17.3.1 PAM算法步骤 随机选择K个观测值（每个都称为中心点）； 计算观测值到各个中心的距离/相异性； 把每个观测值分配到最近的中心点； 计算每个中心点到每个观测值的距离的总和（总成本）； 选择一个该类中不是中心的点，并和中心点互换； 重新把每个点分配到距它最近的中心点； 再次计算总成本； 如果总成本比步骤(4)计算的总成本少，把新的点作为中心点； 重复步骤(5)～(8)直到中心点不再改变。 library(cluster) set.seed(1234) fit.pam &lt;- pam(wine[-1], k=3, stand=TRUE) fit.pam$medoids ## Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids ## [1,] 13 1.8 2.4 20 100 2.7 2.98 0.26 ## [2,] 12 1.7 2.1 19 80 1.6 2.03 0.37 ## [3,] 13 3.9 2.5 23 102 1.8 0.75 0.43 ## Proanthocyanins Color Hue Dilution Proline ## [1,] 1.9 5.1 1.0 3.5 920 ## [2,] 1.6 3.4 1.0 3.2 510 ## [3,] 1.4 7.3 0.7 1.6 750 clusplot(fit.pam, main=&quot;Bivariate Cluster Plot&quot;) # evaluate clustering ct.pam &lt;- table(wine$Type, fit.pam$clustering) ct.pam ## ## 1 2 3 ## 1 59 0 0 ## 2 16 53 2 ## 3 0 1 47 library(flexclust) randIndex(ct.pam) ## ARI ## 0.7 ## Avoiding non-existent clusters library(fMultivar) set.seed(1234) df &lt;- rnorm2d(1000, rho=.5) df &lt;- as.data.frame(df) plot(df, main=&quot;Bivariate Normal Distribution with rho=0.5&quot;) wssplot(df) library(NbClust) nc &lt;- NbClust(df, min.nc=2, max.nc=15, method=&quot;kmeans&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 8 proposed 2 as the best number of clusters ## * 4 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 1 proposed 5 as the best number of clusters ## * 4 proposed 9 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## * 1 proposed 13 as the best number of clusters ## * 3 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* par(opar) barplot(table(nc$Best.n[1,]), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main =&quot;Number of Clusters Chosen by 26 Criteria&quot;) library(ggplot2) library(cluster) fit &lt;- pam(df, k=2) df$clustering &lt;- factor(fit$clustering) ggplot(data=df, aes(x=V1, y=V2, color=clustering, shape=clustering)) + geom_point() + ggtitle(&quot;Clustering of Bivariate Normal Data&quot;) plot(nc$All.index[,4], type=&quot;o&quot;, ylab=&quot;CCC&quot;, xlab=&quot;Number of clusters&quot;, col=&quot;blue&quot;) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
